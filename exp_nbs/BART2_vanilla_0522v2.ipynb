{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BART2_vanilla_0522v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX60u8dY-bIi",
        "outputId": "bf75ba2b-6172-49ff-bb21-4179aa173204"
      },
      "source": [
        "# Divine beast bless no bug here! \n",
        "#         ┌─┐    ┌─┐\n",
        "#      ┌─┘ ┴───┘ ┴──┐\n",
        "#      │                   │\n",
        "#      │       ───       │\n",
        "#      │  ─┬┘     └┬─  │\n",
        "#      │                   │\n",
        "#      │       ─┴─       │\n",
        "#      │                   │\n",
        "#      └─┐         ┌───┘\n",
        "#          │         │\n",
        "#          │         │\n",
        "#          │         │\n",
        "#          │         └──────────────┐\n",
        "#          │                                  │\n",
        "#          │                                  ├─┐\n",
        "#          │                                  ┌─┘\n",
        "#          │                                  │\n",
        "#          └─┐  ┐  ┌──────┬──┐  ┌──┘\n",
        "#            │  ─┤ ─┤         │  ─┤ ─┤\n",
        "#            └──┴──┘         └──┴──┘\n",
        "\n",
        "!pip install transformers\n",
        "!pip install boto3\n",
        "!pip install jiwer==2.2.0\n",
        "!pip install -U nltk\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd ..  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 5.3MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 35.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 37.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/20/4294e37c3c6936c905f1e9da958c776d7fee54a4512bdb7706d69c8720e6/boto3-1.17.84-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.3MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.8MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.84\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/22/72c81d754bbcb128cba2ad88670c3c320e4594e6ddd8cca6512c3967108c/botocore-1.20.84-py2.py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 7.8MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.84->boto3) (2.8.1)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/cd/1e2ec680ec7b09846dc6e605f5a7709dfb9d7128e51a026e7154e18a234e/urllib3-1.26.5-py2.py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.84->boto3) (1.15.0)\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.17.84 botocore-1.20.84 jmespath-0.10.0 s3transfer-0.4.2 urllib3-1.26.5\n",
            "Collecting jiwer==2.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/cc/fb9d3132cba1f6d393b7d5a9398d9d4c8fc033bc54668cf87e9b197a6d7a/jiwer-2.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jiwer==2.2.0) (1.19.5)\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/dc/97f2b63ef0fa1fd78dcb7195aca577804f6b2b51e712516cc0e902a9a201/python-Levenshtein-0.12.2.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein->jiwer==2.2.0) (56.1.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149800 sha256=6d7d6231644b7b096a1d494c03c305280fe0d44f06618f5eba706a2817444676\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/26/73/4b48503bac73f01cf18e52cd250947049a7f339e940c5df8fc\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein, jiwer\n",
            "Successfully installed jiwer-2.2.0 python-Levenshtein-0.12.2\n",
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n",
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8048, done.\u001b[K\n",
            "remote: Counting objects: 100% (135/135), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 8048 (delta 65), reused 93 (delta 41), pack-reused 7913\u001b[K\n",
            "Receiving objects: 100% (8048/8048), 14.11 MiB | 21.76 MiB/s, done.\n",
            "Resolving deltas: 100% (5464/5464), done.\n",
            "/content/apex\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-wvpzdh8q\n",
            "Created temporary directory: /tmp/pip-req-tracker-r11fxy6w\n",
            "Created requirements tracker '/tmp/pip-req-tracker-r11fxy6w'\n",
            "Created temporary directory: /tmp/pip-install-itgj6h2r\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-jsz8mihv\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-r11fxy6w'\n",
            "    Running setup.py (path:/tmp/pip-req-build-jsz8mihv/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "    No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "    Warning: Torch did not find available GPUs on this system.\n",
            "     If your intention is to cross-compile, this is not an error.\n",
            "    By default, Apex will cross-compile for Pascal (compute capabilities 6.0, 6.1, 6.2),\n",
            "    Volta (compute capability 7.0), Turing (compute capability 7.5),\n",
            "    and, if the CUDA version is >= 11.0, Ampere (compute capability 8.0).\n",
            "    If you wish to cross-compile for a single specific architecture,\n",
            "    export TORCH_CUDA_ARCH_LIST=\"compute capability\" before running setup.py.\n",
            "\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.8.1+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-jsz8mihv/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-jsz8mihv/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-jsz8mihv/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-jsz8mihv/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-jsz8mihv/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n",
            "    writing manifest file '/tmp/pip-req-build-jsz8mihv/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-jsz8mihv/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-jsz8mihv has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-r11fxy6w'\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-pllhjpwm\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-pllhjpwm\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-jsz8mihv/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-jsz8mihv/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-pllhjpwm --python-tag cp37\n",
            "  No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "  Warning: Torch did not find available GPUs on this system.\n",
            "   If your intention is to cross-compile, this is not an error.\n",
            "  By default, Apex will cross-compile for Pascal (compute capabilities 6.0, 6.1, 6.2),\n",
            "  Volta (compute capability 7.0), Turing (compute capability 7.5),\n",
            "  and, if the CUDA version is >= 11.0, Ampere (compute capability 8.0).\n",
            "  If you wish to cross-compile for a single specific architecture,\n",
            "  export TORCH_CUDA_ARCH_LIST=\"compute capability\" before running setup.py.\n",
            "\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.8.1+cu101\n",
            "\n",
            "\n",
            "  /tmp/pip-req-build-jsz8mihv/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/__init__.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/weight_norm.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/reparameterization.py -> build/lib/apex/reparameterization\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/pyprof\n",
            "  copying apex/pyprof/__init__.py -> build/lib/apex/pyprof\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/__init__.py -> build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/nvmarker.py -> build/lib/apex/pyprof/nvtx\n",
            "  creating build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/optim.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/output.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pooling.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/base.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/randomSample.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/blas.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/reduction.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/misc.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/activation.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__main__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/data.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/prof.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/recurrentCell.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pointwise.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__init__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/normalization.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/dropout.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/utility.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/embedding.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/linear.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/loss.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/usage.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/conv.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/convert.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/softmax.py -> build/lib/apex/pyprof/prof\n",
            "  creating build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/nvvp.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/kernel.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/db.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__main__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/parse.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__init__.py -> build/lib/apex/pyprof/parse\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-pllhjpwm/apex-0.1-cp37-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v2.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v3.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/pyprof/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/nvmarker.py'\n",
            "  adding 'apex/pyprof/parse/__init__.py'\n",
            "  adding 'apex/pyprof/parse/__main__.py'\n",
            "  adding 'apex/pyprof/parse/db.py'\n",
            "  adding 'apex/pyprof/parse/kernel.py'\n",
            "  adding 'apex/pyprof/parse/nvvp.py'\n",
            "  adding 'apex/pyprof/parse/parse.py'\n",
            "  adding 'apex/pyprof/prof/__init__.py'\n",
            "  adding 'apex/pyprof/prof/__main__.py'\n",
            "  adding 'apex/pyprof/prof/activation.py'\n",
            "  adding 'apex/pyprof/prof/base.py'\n",
            "  adding 'apex/pyprof/prof/blas.py'\n",
            "  adding 'apex/pyprof/prof/conv.py'\n",
            "  adding 'apex/pyprof/prof/convert.py'\n",
            "  adding 'apex/pyprof/prof/data.py'\n",
            "  adding 'apex/pyprof/prof/dropout.py'\n",
            "  adding 'apex/pyprof/prof/embedding.py'\n",
            "  adding 'apex/pyprof/prof/index_slice_join_mutate.py'\n",
            "  adding 'apex/pyprof/prof/linear.py'\n",
            "  adding 'apex/pyprof/prof/loss.py'\n",
            "  adding 'apex/pyprof/prof/misc.py'\n",
            "  adding 'apex/pyprof/prof/normalization.py'\n",
            "  adding 'apex/pyprof/prof/optim.py'\n",
            "  adding 'apex/pyprof/prof/output.py'\n",
            "  adding 'apex/pyprof/prof/pointwise.py'\n",
            "  adding 'apex/pyprof/prof/pooling.py'\n",
            "  adding 'apex/pyprof/prof/prof.py'\n",
            "  adding 'apex/pyprof/prof/randomSample.py'\n",
            "  adding 'apex/pyprof/prof/recurrentCell.py'\n",
            "  adding 'apex/pyprof/prof/reduction.py'\n",
            "  adding 'apex/pyprof/prof/softmax.py'\n",
            "  adding 'apex/pyprof/prof/usage.py'\n",
            "  adding 'apex/pyprof/prof/utility.py'\n",
            "  adding 'apex/reparameterization/__init__.py'\n",
            "  adding 'apex/reparameterization/reparameterization.py'\n",
            "  adding 'apex/reparameterization/weight_norm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp37-none-any.whl size=204691 sha256=e529603594f2d153afc94f5729d52050e4f35a923e6e83d639093bfa0bc4becf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wvpzdh8q/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "  Removing source in /tmp/pip-req-build-jsz8mihv\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-r11fxy6w'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsmxZM0-x7fF",
        "outputId": "837aac39-8e60-4693-a5e6-61fd9d03c3a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGx7BB_dqPbS"
      },
      "source": [
        "# %%writefile setup.sh\n",
        "# export CUDA_HOME=/usr/local/cuda-10.1\n",
        "# git clone https://github.com/NVIDIA/apex\n",
        "# pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcaz1gMYbmt1"
      },
      "source": [
        "# !sh setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3gLMKblgV2b",
        "outputId": "d3068ed1-8973-43f7-e030-d97d0374aa73"
      },
      "source": [
        "# data preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import json\n",
        "import h5py\n",
        "from collections import OrderedDict\n",
        "\n",
        "# control\n",
        "import argparse\n",
        "import logging\n",
        "from tqdm import trange\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# transformers\n",
        "from transformers import GPT2Config\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import BartForCausalLM, BartTokenizer,BartConfig"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr3GLmeRgslI"
      },
      "source": [
        "# the original dataframe\n",
        "df_ori = pd.read_csv(\"./gdrive/MyDrive/COMP0087/full_dataset.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wajqYvcFedhK"
      },
      "source": [
        "np.random.seed(7)\n",
        "train_len = 500000\n",
        "test_len = 200\n",
        "subset_idx = np.random.choice(range(len(df_ori)),train_len+test_len,replace=False)\n",
        "train_idx = subset_idx[0:train_len]\n",
        "test_idx = subset_idx[train_len:]\n",
        "df = df_ori.loc[subset_idx ,:]\n",
        "# df_test = df_ori.loc[subset_idx[-100:],:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxIQiIoQobMb"
      },
      "source": [
        "# data preprocessing\n",
        "remove1 = df.loc[df.title.map(lambda x: len(x)<4 )]\n",
        "remove2 = df.loc[df.ingredients.map(lambda x: len(x)<2)]\n",
        "remove3 = df.loc[df.directions.map(lambda x: len(x) < 2 or len(''.join(x)) < 30)]\n",
        "remove4 = df.loc[df.directions.map(lambda x: re.search('(step|mix all)', ''.join(str(x)), re.IGNORECASE)!=None)]\n",
        "len(remove3)+len(remove2)+len(remove1)+len(remove4)\n",
        "df.drop(df[df.title.map(lambda x: len(x)<4)].index, inplace=True)\n",
        "df.drop(df[df.ingredients.map(lambda x: len(x)<2)].index, inplace=True)\n",
        "df.drop(df[df.directions.map(lambda x: len(x) < 2 or len(''.join(x)) < 30)].index, inplace=True)\n",
        "df.drop(df[df.directions.map(lambda x: re.search('(step|mix all)', ''.join(str(x)), re.IGNORECASE)!=None)].index, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vogi-NsDgd1z"
      },
      "source": [
        "df.drop(df[df.title.map(lambda x: len(x)<4)].index, inplace=True)\n",
        "df.drop(df[df.ingredients.map(lambda x: len(x)<2)].index, inplace=True)\n",
        "df.drop(df[df.directions.map(lambda x: len(x) < 2 or len(''.join(x)) < 30)].index, inplace=True)\n",
        "df.drop(df[df.directions.map(lambda x: re.search('(step|mix all)', ''.join(str(x)), re.IGNORECASE)!=None)].index, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h0MbRQken9a",
        "outputId": "29e6c58c-3f2f-4b2c-e00e-9cb48f6c8b66"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "471023"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyIpD-Apgh7s"
      },
      "source": [
        "df.reset_index(inplace=True)\n",
        "train, test = train_test_split(df, test_size=0.05) #use 5% for test set\n",
        "# we only want first 10000 and 100 for train/test\n",
        "# this has an error: train[75400:75600]\n",
        "# train = train[50000:400000]\n",
        "test = test[0:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuytage9gjzH"
      },
      "source": [
        "def df_to_plaintext_file(input_df, output_file, train = True):\n",
        "    print(\"Writing to\", output_file)\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        for index, row in input_df.iterrows():\n",
        "            if index%100000==0:\n",
        "                print(index)\n",
        "                print(res)\n",
        "            if type(row.NER)!=str:\n",
        "                continue\n",
        "            title = row.title\n",
        "            directions = json.loads(row.directions)\n",
        "            if len(directions) <= 1 and train:\n",
        "              continue\n",
        "            # print(len(directions))\n",
        "            ingredients = json.loads(row.ingredients)\n",
        "            ner = json.loads(row.NER)\n",
        "            # print(ner)\n",
        "            res = \"<RECIPE_START> <INPUT_START> \" + \" <NEXT_INPUT> \".join(ner) + \" <INPUT_END> <INGR_START> \" + \\\n",
        "              \" <NEXT_INGR> \".join(ingredients) + \" <INGR_END> <INSTR_START> \" + \\\n",
        "              \" <NEXT_INSTR> \".join(directions) + \" <INSTR_END> <TITLE_START> \" + title + \" <TITLE_END> <RECIPE_END>\"\n",
        "            # print(res)\n",
        "            f.write(\"{}\\n\".format(res))\n",
        "\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mpW4paENyVv",
        "outputId": "8f33008d-c4b1-4d6d-db23-d83c15930cbf"
      },
      "source": [
        "df_to_plaintext_file(train, 'unsupervised_train.txt')\n",
        "df_to_plaintext_file(test, 'unsupervised_test.txt',train = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing to unsupervised_train.txt\n",
            "100000\n",
            "<RECIPE_START> <INPUT_START> walnuts <NEXT_INPUT> ramen noodles <NEXT_INPUT> butter <NEXT_INPUT> broccoli <NEXT_INPUT> romaine lettuce <NEXT_INPUT> green onions <NEXT_INPUT> vegetable oil <NEXT_INPUT> sugar <NEXT_INPUT> wine vinegar <NEXT_INPUT> soy sauce <NEXT_INPUT> salt <NEXT_INPUT> pepper <INPUT_END> <INGR_START> 1 cup walnuts, chopped <NEXT_INGR> 1 (3 ounce) package ramen noodles, uncooked,broken up (Discard flavor packet) <NEXT_INGR> 4 tablespoons butter <NEXT_INGR> 1 bunch broccoli, coarsely chopped <NEXT_INGR> 1 head romaine lettuce, washed,torn into pieces <NEXT_INGR> 4 green onions, chopped <NEXT_INGR> 1/2 cup vegetable oil <NEXT_INGR> 1/2 cup sugar <NEXT_INGR> 1/4 cup wine vinegar <NEXT_INGR> 2 teaspoons soy sauce <NEXT_INGR> 1/2 teaspoon salt <NEXT_INGR> 1 teaspoon pepper <INGR_END> <INSTR_START> Make the Sweet and Sour dressing and set aside (blend together the last 6 ingredients). <NEXT_INSTR> Brown walnuts and noodles in butter; cool on paper towels. <NEXT_INSTR> Combine noodles and walnuts with broccoli, romaine and onions. <NEXT_INSTR> Pour sweet and sour dressing over salad and toss to coat well. <INSTR_END> <TITLE_START> Crunchy Romaine Toss <TITLE_END> <RECIPE_END>\n",
            "400000\n",
            "<RECIPE_START> <INPUT_START> pork chops <NEXT_INPUT> long grain rice <NEXT_INPUT> onion <NEXT_INPUT> instant chicken bouillon <NEXT_INPUT> apple <INPUT_END> <INGR_START> 4 pork chops <NEXT_INGR> 2/3 c. long grain rice <NEXT_INGR> 4 Tbsp. chopped onion <NEXT_INGR> 2 tsp. instant chicken bouillon <NEXT_INGR> 1/2 c. chopped apple <INGR_END> <INSTR_START> Brown pork chops in a small amount of shortening in skillet. Remove pork chops, setting aside. <NEXT_INSTR> Add rice and onion. <NEXT_INSTR> Cook until golden brown, stirring constantly. <NEXT_INSTR> Add 2 cups of water and bouillon and bring to a boil. <NEXT_INSTR> Stir in apple. <NEXT_INSTR> Turn into casserole, arranging pork chops on top. <NEXT_INSTR> Bake, covered, at 350° for 30 minutes. <NEXT_INSTR> Uncover; bake for 20 minutes longer or until pork chops are tender. <NEXT_INSTR> Yields 4 servings, about 440 calories each. <INSTR_END> <TITLE_START> Pork Chop Supper <TITLE_END> <RECIPE_END>\n",
            "200000\n",
            "<RECIPE_START> <INPUT_START> yellow cake <NEXT_INPUT> eggs <NEXT_INPUT> margarine <NEXT_INPUT> water <INPUT_END> <INGR_START> 1 (18 oz.) pkg. yellow cake mix <NEXT_INGR> 2 eggs <NEXT_INGR> 1 stick margarine, softened <NEXT_INGR> 4 Tbsp. water <INGR_END> <INSTR_START> Mix well. <NEXT_INSTR> Spread dough in bottom of greased 9 x 13-inch pan. <INSTR_END> <TITLE_START> Gooey Cheesecake <TITLE_END> <RECIPE_END>\n",
            "0\n",
            "<RECIPE_START> <INPUT_START> tomatoes <NEXT_INPUT> onions <NEXT_INPUT> sweet green peppers <NEXT_INPUT> peppers <NEXT_INPUT> cinnamon <NEXT_INPUT> cloves <NEXT_INPUT> sugar <NEXT_INPUT> ginger <NEXT_INPUT> mustard <INPUT_END> <INGR_START> 1/2 bushel tomatoes <NEXT_INGR> 3 lb. onions, ground <NEXT_INGR> 1 doz. sweet green peppers <NEXT_INGR> 3 or 4 hot peppers <NEXT_INGR> 1 Tbsp. cinnamon <NEXT_INGR> 1 Tbsp. cloves <NEXT_INGR> 4 c. sugar <NEXT_INGR> 1 Tbsp. ginger <NEXT_INGR> 1 Tbsp. dry mustard <INGR_END> <INSTR_START> Scald and peel tomatoes and cut in pieces. <NEXT_INSTR> After boiling tomatoes for 2 hours, add onions and peppers and add cinnamon, ginger, cloves, dry mustard and sugar. <NEXT_INSTR> Boil for 1/2 hour and then add <NEXT_INSTR> 1/2 pint vinegar. <NEXT_INSTR> Boil 1/2 hour more, put into hot canning jars and seal. <INSTR_END> <TITLE_START> Hot Chile Sauce <TITLE_END> <RECIPE_END>\n",
            "300000\n",
            "<RECIPE_START> <INPUT_START> shortening <NEXT_INPUT> sugar <NEXT_INPUT> eggs <NEXT_INPUT> mashed ripe bananas <NEXT_INPUT> cake flour <NEXT_INPUT> soda <NEXT_INPUT> salt <INPUT_END> <INGR_START> 1/2 c. shortening <NEXT_INGR> 1 c. sugar <NEXT_INGR> 2 eggs <NEXT_INGR> 3/4 c. mashed ripe bananas <NEXT_INGR> 1 1/4 c. sifted cake flour <NEXT_INGR> 3/4 tsp. soda <NEXT_INGR> 1/2 tsp. salt <INGR_END> <INSTR_START> Cream the shortening and sugar until light. <NEXT_INSTR> Add eggs, one at a time, beating well after each. <NEXT_INSTR> Stir in bananas. <NEXT_INSTR> Mix together dry ingredients. <NEXT_INSTR> Add to banana mixture. <NEXT_INSTR> Mix until well blended. Pour into a greased 9 x 9 x 2-inch pan. <NEXT_INSTR> Bake at 350° for 30 to 35 minutes. <NEXT_INSTR> Cut into squares. <INSTR_END> <TITLE_START> Michele Swank'S Banana Bread <TITLE_END> <RECIPE_END>\n",
            "Writing to unsupervised_test.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "UAlHaUmBLgEi",
        "outputId": "45c25d5a-c741-4fbb-cd5b-c9521e53a1d5"
      },
      "source": [
        "\"\"\"\n",
        "def dataset_cleaning(input_df, tokenizer, train = True):\n",
        "  train_size = 350000\n",
        "  test_size = 100\n",
        "  hf = h5py.File(\"unsupervised.h5\", \"w\")\n",
        "  tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "    \n",
        "  end_token_id = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
        "  ing_token_id = tokenizer.convert_tokens_to_ids([\"<INPUT_END>\"])[0]\n",
        "  directions_size = 512\n",
        "  # only generating training samples this way\n",
        "  for filename in [\"train\"]:\n",
        "    out_np = []\n",
        "    num = 0\n",
        "    rows = 0\n",
        "    last=[]\n",
        "    for index, row in input_df.iterrows():\n",
        "      if index%100000==0:\n",
        "        print(index)\n",
        "        print(res)\n",
        "      if type(row.NER)!=str:\n",
        "        continue\n",
        "      title = row.title\n",
        "      directions = json.loads(row.directions)\n",
        "      if len(directions) <= 1 and train:\n",
        "        continue\n",
        "\n",
        "      ingredients = json.loads(row.ingredients)\n",
        "      ner = json.loads(row.NER)\n",
        "\n",
        "      res = \"<RECIPE_START> <INPUT_START> \" + \" <NEXT_INPUT> \".join(ner) + \" <INPUT_END> <INGR_START> \" + \\\n",
        "              \" <NEXT_INGR> \".join(ingredients) + \" <INGR_END> <INSTR_START> \" + \\\n",
        "              \" <NEXT_INSTR> \".join(directions) + \" <INSTR_END> <TITLE_START> \" + title + \" <TITLE_END> <RECIPE_END>\"\n",
        "      # print(res)\n",
        "      num+=1\n",
        "      if num%10000 == 0:\n",
        "        print(\"Read \"+str(num)+\" Written: \"+str(rows))\n",
        "      text_tokens = tokenizer(res)['input_ids']\n",
        "      if ing_token_id in text_tokens:\n",
        "        ing_idx = text_tokens.index(ing_token_id)\n",
        "        text_tokens = text_tokens[3:ing_idx]+text_tokens\n",
        "        # print(text_tokens)\n",
        "      else:\n",
        "        print(text_tokens)\n",
        "        continue\n",
        "        \n",
        "      # error in one recipe\n",
        "      if len(text_tokens) > directions_size or (50273 not in text_tokens): \n",
        "        # print(\"Recipe won't fit the model\")\n",
        "        continue\n",
        "\n",
        "      # text_tokens_ids = tokenizer.convert_tokens_to_ids(text_tokens)\n",
        "      text_tokens_ids = text_tokens\n",
        "      # print(text_tokens_ids[0:5])\n",
        "\n",
        "      while len(text_tokens_ids) < directions_size:\n",
        "        text_tokens_ids.append(end_token_id)\n",
        "        out_np.append(text_tokens_ids)\n",
        "        rows+=1\n",
        "\n",
        "\n",
        "      if rows == train_size and filename == 'train':\n",
        "        print(\"training sample enough:\",train_size)\n",
        "        break\n",
        "      if rows == test_size and filename == 'test':\n",
        "        print(\"testing sample enough\",test_size)\n",
        "        break\n",
        "\n",
        "    out_mat = np.matrix(out_np)\n",
        "    print(out_mat.shape)\n",
        "    # print(out_mat)\n",
        "    hf.create_dataset(filename, data=out_mat)\n",
        "  hf.close()\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "special_tokens = {\n",
        "        \"additional_special_tokens\": [\n",
        "              \"<TITLE_START>\"\n",
        "              \"<TITLE_END>\",\n",
        "              \"<INSTR_START>\",\n",
        "              \"<NEXT_INSTR>\",\n",
        "              \"<INSTR_END>\",\n",
        "              \"<INGR_START>\",\n",
        "              \"<NEXT_INGR>\",\n",
        "              \"<INGR_END>\",\n",
        "              \"<RECIPE_START>\",\n",
        "              \"<RECIPE_END>\",\n",
        "              \"<INPUT_START>\",\n",
        "              \"<INPUT_END>\",\n",
        "              \"<NEXT_INPUT>\"\n",
        "              ]\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "dataset_cleaning(train, tokenizer, train = True)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef dataset_cleaning(input_df, tokenizer, train = True):\\n  train_size = 350000\\n  test_size = 100\\n  hf = h5py.File(\"unsupervised.h5\", \"w\")\\n  tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\\n    \\n  end_token_id = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\\n  ing_token_id = tokenizer.convert_tokens_to_ids([\"<INPUT_END>\"])[0]\\n  directions_size = 512\\n  # only generating training samples this way\\n  for filename in [\"train\"]:\\n    out_np = []\\n    num = 0\\n    rows = 0\\n    last=[]\\n    for index, row in input_df.iterrows():\\n      if index%100000==0:\\n        print(index)\\n        print(res)\\n      if type(row.NER)!=str:\\n        continue\\n      title = row.title\\n      directions = json.loads(row.directions)\\n      if len(directions) <= 1 and train:\\n        continue\\n\\n      ingredients = json.loads(row.ingredients)\\n      ner = json.loads(row.NER)\\n\\n      res = \"<RECIPE_START> <INPUT_START> \" + \" <NEXT_INPUT> \".join(ner) + \" <INPUT_END> <INGR_START> \" +               \" <NEXT_INGR> \".join(ingredients) + \" <INGR_END> <INSTR_START> \" +               \" <NEXT_INSTR> \".join(directions) + \" <INSTR_END> <TITLE_START> \" + title + \" <TITLE_END> <RECIPE_END>\"\\n      # print(res)\\n      num+=1\\n      if num%10000 == 0:\\n        print(\"Read \"+str(num)+\" Written: \"+str(rows))\\n      text_tokens = tokenizer(res)[\\'input_ids\\']\\n      if ing_token_id in text_tokens:\\n        ing_idx = text_tokens.index(ing_token_id)\\n        text_tokens = text_tokens[3:ing_idx]+text_tokens\\n        # print(text_tokens)\\n      else:\\n        print(text_tokens)\\n        continue\\n        \\n      # error in one recipe\\n      if len(text_tokens) > directions_size or (50273 not in text_tokens): \\n        # print(\"Recipe won\\'t fit the model\")\\n        continue\\n\\n      # text_tokens_ids = tokenizer.convert_tokens_to_ids(text_tokens)\\n      text_tokens_ids = text_tokens\\n      # print(text_tokens_ids[0:5])\\n\\n      while len(text_tokens_ids) < directions_size:\\n        text_tokens_ids.append(end_token_id)\\n        out_np.append(text_tokens_ids)\\n        rows+=1\\n\\n\\n      if rows == train_size and filename == \\'train\\':\\n        print(\"training sample enough:\",train_size)\\n        break\\n      if rows == test_size and filename == \\'test\\':\\n        print(\"testing sample enough\",test_size)\\n        break\\n\\n    out_mat = np.matrix(out_np)\\n    print(out_mat.shape)\\n    # print(out_mat)\\n    hf.create_dataset(filename, data=out_mat)\\n  hf.close()\\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\\nspecial_tokens = {\\n        \"additional_special_tokens\": [\\n              \"<TITLE_START>\"\\n              \"<TITLE_END>\",\\n              \"<INSTR_START>\",\\n              \"<NEXT_INSTR>\",\\n              \"<INSTR_END>\",\\n              \"<INGR_START>\",\\n              \"<NEXT_INGR>\",\\n              \"<INGR_END>\",\\n              \"<RECIPE_START>\",\\n              \"<RECIPE_END>\",\\n              \"<INPUT_START>\",\\n              \"<INPUT_END>\",\\n              \"<NEXT_INPUT>\"\\n              ]\\n}\\ntokenizer.add_special_tokens(special_tokens)\\ndataset_cleaning(train, tokenizer, train = True)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wqzp-seoHsw",
        "outputId": "3a1e56aa-f3d4-4325-e9cb-08e8f3c0879e"
      },
      "source": [
        "from collections import OrderedDict\n",
        "train_size = 350000\n",
        "test_size = 100\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "special_tokens = {\n",
        "    \"additional_special_tokens\": [\n",
        "        \"<TITLE_START>\",\n",
        "        \"<TITLE_END>\",\n",
        "        \"<INSTR_START>\",\n",
        "        \"<NEXT_INSTR>\",\n",
        "        \"<INSTR_END>\",\n",
        "        \"<INGR_START>\",\n",
        "        \"<NEXT_INGR>\",\n",
        "        \"<INGR_END>\",\n",
        "        \"<RECIPE_START>\",\n",
        "        \"<RECIPE_END>\",\n",
        "        \"<INPUT_START>\",\n",
        "        \"<INPUT_END>\",\n",
        "        \"<NEXT_INPUT>\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "end_token_id = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
        "ing_token_id = tokenizer.convert_tokens_to_ids([\"<INPUT_END>\"])[0]\n",
        "\n",
        "next_input_token_id = tokenizer.convert_tokens_to_ids([\"<NEXT_INPUT>\"])[0]\n",
        "# print(next_input_token_id)\n",
        "\n",
        "directions_size = 512\n",
        "hf = h5py.File(\"unsupervised.h5\", \"w\")\n",
        "for filename in [\"test\", \"train\"]:\n",
        "    out_np = []\n",
        "    data = open(\"unsupervised_\"+filename+\".txt\", \"r\")\n",
        "    num = 0\n",
        "    rows = 0\n",
        "    last=[]\n",
        "    for line in data:\n",
        "        num+=1\n",
        "        if num%10000 == 0:\n",
        "            print(\"Process: \"+str(num)+\" Valid: \"+str(rows))\n",
        "        text_tokens = tokenizer(line)['input_ids']\n",
        "        \n",
        "        if ing_token_id in text_tokens:\n",
        "          ing_idx = text_tokens.index(ing_token_id)\n",
        "          temp_list = list(OrderedDict.fromkeys(text_tokens[3:ing_idx]))\n",
        "\n",
        "          # if <NEXT_INPUT> not in list, do not need to process\n",
        "          if next_input_token_id in temp_list:\n",
        "            temp_list.remove(next_input_token_id)\n",
        "          else:\n",
        "            continue\n",
        "          text_tokens = temp_list + text_tokens\n",
        "        else:\n",
        "          continue\n",
        "        \n",
        "        # error in one recipe\n",
        "        if len(text_tokens) > directions_size or (50273 not in text_tokens): \n",
        "            continue\n",
        "            \n",
        "        text_tokens_ids = text_tokens\n",
        "        while len(text_tokens_ids) < directions_size :\n",
        "          text_tokens_ids.append(end_token_id)\n",
        "        out_np.append(text_tokens_ids)\n",
        "        rows+=1\n",
        "\n",
        "\n",
        "        if rows == train_size and filename == 'train':\n",
        "          print(\"training sample enough:\",train_size)\n",
        "          break\n",
        "        if rows == test_size and filename == 'test':\n",
        "          print(\"testing sample enough\",test_size)\n",
        "          break\n",
        "    out_mat = np.matrix(out_np)\n",
        "    print(out_mat.shape)\n",
        "    hf.create_dataset(filename, data=out_mat)\n",
        "hf.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(97, 512)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1733 > 1024). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Process: 10000 Valid: 9240\n",
            "Process: 20000 Valid: 18547\n",
            "Process: 30000 Valid: 27785\n",
            "Process: 40000 Valid: 37030\n",
            "Process: 50000 Valid: 46263\n",
            "Process: 60000 Valid: 55534\n",
            "Process: 70000 Valid: 64803\n",
            "Process: 80000 Valid: 73978\n",
            "Process: 90000 Valid: 83137\n",
            "Process: 100000 Valid: 92294\n",
            "Process: 110000 Valid: 101544\n",
            "Process: 120000 Valid: 110782\n",
            "Process: 130000 Valid: 120007\n",
            "Process: 140000 Valid: 129283\n",
            "Process: 150000 Valid: 138526\n",
            "Process: 160000 Valid: 147809\n",
            "Process: 170000 Valid: 157085\n",
            "Process: 180000 Valid: 166332\n",
            "Process: 190000 Valid: 175437\n",
            "Process: 200000 Valid: 184654\n",
            "Process: 210000 Valid: 193902\n",
            "Process: 220000 Valid: 203175\n",
            "Process: 230000 Valid: 212417\n",
            "Process: 240000 Valid: 221642\n",
            "Process: 250000 Valid: 230877\n",
            "Process: 260000 Valid: 240189\n",
            "Process: 270000 Valid: 249442\n",
            "Process: 280000 Valid: 258728\n",
            "Process: 290000 Valid: 267978\n",
            "Process: 300000 Valid: 277307\n",
            "Process: 310000 Valid: 286541\n",
            "Process: 320000 Valid: 295815\n",
            "Process: 330000 Valid: 305096\n",
            "Process: 340000 Valid: 314286\n",
            "Process: 350000 Valid: 323496\n",
            "Process: 360000 Valid: 332686\n",
            "Process: 370000 Valid: 341922\n",
            "training sample enough: 350000\n",
            "(350000, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMvXzbLcIIC-"
      },
      "source": [
        "#raw_text = 'tomato,egg'\n",
        "#prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "# tokenizer.encode(prepared_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnVQ59V4FdiF"
      },
      "source": [
        "# fine-tune with BART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz7e2PoybaiE"
      },
      "source": [
        "# fine-tuning model with bart\n",
        "# logging info\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import random\n",
        "import gc\n",
        "import boto3\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5MKxGg8bhj4"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm, trange\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import torch\n",
        "\n",
        "from transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BartForCausalLM, BartTokenizer,BartConfig,BartForConditionalGeneration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm4SBl8hbqj3"
      },
      "source": [
        "# data loading\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path='train', block_size=512):\n",
        "        cached_features_file = \"unsupervised.h5\"\n",
        "\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        with h5py.File(cached_features_file, 'r') as f:\n",
        "            if file_path=='test':\n",
        "                self.examples = f[file_path][:] #this is a dev set, 10% of a test set\n",
        "            else:\n",
        "                self.examples = f[file_path][:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item])\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, evaluate=False):\n",
        "    dataset = TextDataset(tokenizer, file_path=\"test\" if evaluate else \"train\", block_size=args.block_size)\n",
        "    print(dataset[0:5])\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6grSGLbPb4Cm"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "## Required parameters\n",
        "parser.add_argument(\"--train_data_file\", default=\"unsupervised.h5\", type=str, required=False,\n",
        "                        help=\"The input training data file (a text file).\")\n",
        "parser.add_argument(\"--output_dir\", default=\"./gdrive/MyDrive/COMP0087/model_checkpoint_0526\", type=str, required=False,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "## Other parameters\n",
        "parser.add_argument(\"--model_type\", default=\"facebook/bart-base\" ,type=str,   #\"facebook/bart-base\"\n",
        "                        help=\"The model architecture to be fine-tuned.\")\n",
        "parser.add_argument(\"--model_name_or_path\", default=\"facebook/bart-base\", type=str,\n",
        "                        help=\"The model checkpoint for weights initialization.\")\n",
        "\n",
        "parser.add_argument(\"--eval_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "\n",
        "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
        "parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
        "parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
        "                        help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\n",
        "parser.add_argument(\"--block_size\", default=-1, type=int,\n",
        "                        help=\"Optional input sequence length after tokenization.\"\n",
        "                             \"The training dataset will be truncated in block of this size for training.\"\n",
        "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
        "parser.add_argument(\"--do_train\", action='store_true',\n",
        "                        help=\"Whether to run training.\")\n",
        "parser.add_argument(\"--do_eval\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
        "                        help=\"Run evaluation during training at each logging step.\")\n",
        "parser.add_argument(\"--do_lower_case\", action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "\n",
        "parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for training.\")\n",
        "parser.add_argument(\"--per_gpu_eval_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
        "parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "parser.add_argument(\"--learning_rate\", default=2e-4, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "                        help=\"Weight deay if we apply some.\")\n",
        "parser.add_argument(\"--adam_epsilon\", default=1e-7, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                        help=\"Max gradient norm.\")\n",
        "parser.add_argument(\"--num_train_epochs\", default=1, type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
        "                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
        "parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
        "                        help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "parser.add_argument('--logging_steps', type=int, default=50,\n",
        "                        help=\"Log every X updates steps.\")\n",
        "parser.add_argument('--save_steps', type=int, default=50,\n",
        "                        help=\"Save checkpoint every X updates steps.\")\n",
        "parser.add_argument(\"--eval_all_checkpoints\", action='store_true',\n",
        "                        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\n",
        "parser.add_argument(\"--no_cuda\", action='store_true',\n",
        "                        help=\"Avoid using CUDA when available\")\n",
        "parser.add_argument('--overwrite_output_dir', action='store_true',\n",
        "                        help=\"Overwrite the content of the output directory\")\n",
        "parser.add_argument('--overwrite_cache', action='store_true',\n",
        "                        help=\"Overwrite the cached training and evaluation sets\")\n",
        "parser.add_argument(\"--aws_bucket\", default=\"\", type=str,\n",
        "                        help=\"Whether to upload to specified bucket.\")\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "def setup_args_for_model(args):\n",
        "  args.model_type =\"facebook/bart-base\" # \"facebook/bart-base\"\n",
        "  args.model_type = \"\"\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8yhJigCwuTM"
      },
      "source": [
        "import random\n",
        "\n",
        "def shuffle_instruction(this_batch,ins_element_id,recipe_end_id):\n",
        "  end_index = (this_batch == recipe_end_id).nonzero()[0]\n",
        "  # ins_element_id = tokenizer.convert_tokens_to_ids([\"<NEXT_INSTR>\"])[0]\n",
        "  ing_index = (this_batch == ins_element_id).nonzero()\n",
        "  ing_index = ing_index[ing_index<end_index]\n",
        "  if len(ing_index) > 1:\n",
        "    split_result = torch.tensor_split(this_batch, ing_index.squeeze())\n",
        "    split_list = list(split_result[1:-1])\n",
        "    random.shuffle(split_list)\n",
        "    shuffle_batch = torch.cat((split_result[0],torch.cat(split_list),split_result[-1]))\n",
        "    return shuffle_batch\n",
        "  else: \n",
        "    return this_batch\n",
        "\n",
        "\n",
        "def ins_token_idf(this_batch,ins_element_id,recipe_end_id):\n",
        "  \"\"\"\n",
        "  identify the <NEXT_INSTR> token in a batch, this is necessary for CDM module\n",
        "  :param this_batch: The batch to be processed\n",
        "  :param ins_element_id: The token of <NEXT_INSTR>\n",
        "  :param recipe_end_id: The token of <RECIPE_END>\n",
        "  :return: the identified position of <NEXT_INSTR>\n",
        "  \"\"\"\n",
        "  if (this_batch == recipe_end_id).nonzero().size()[0] == 0:\n",
        "    return None\n",
        "  end_index = (this_batch == recipe_end_id).nonzero()[0]\n",
        "  ing_index = (this_batch == ins_element_id).nonzero()\n",
        "  ing_index = ing_index[ing_index<end_index]\n",
        "  if len(ing_index) > 0:\n",
        "    return ing_index\n",
        "  else: \n",
        "    return None\n",
        "\n",
        "def split_ing_dirs(this_batch):\n",
        "  token_list = tokenizer.convert_tokens_to_ids([\"<INSTR_START>\",\n",
        "                                                \"<INGR_START>\",\n",
        "                                                \"<NEXT_INSTR>\",\n",
        "                                                \"<RECIPE_END>\",\n",
        "                                                \"<INPUT_END>\",\n",
        "                                                \"<RECIPE_START>\"])\n",
        "  ins_start_id = token_list[0]\n",
        "  ing_start_id = token_list[1]\n",
        "  ins_element_id = token_list[2]\n",
        "  recipe_end_id = token_list[3]\n",
        "  input_end_id = token_list[4]\n",
        "  recipe_start_id = token_list[5]\n",
        "\n",
        "  sample = this_batch.clone()\n",
        "  # ingredients size\n",
        "  ing_size = 48\n",
        "  ins_size = 512\n",
        "  # the start index of ingredients\n",
        "  # ing_index = (batch[i] == ing_start_id).nonzero()[0]\n",
        "  # the start index of instruction\n",
        "            \n",
        "  # move the non-zero elements of ingredients vector to left\n",
        "  ins_index = (sample == recipe_start_id).nonzero()[0]-1\n",
        "  # print(ins_index)\n",
        "  ingredients = torch.zeros(ing_size)\n",
        "  # ingredients = torch.full_like(ingredients,input_end_id)\n",
        "  # print(this_batch)\n",
        "  if ins_index > ing_size:\n",
        "    print(\"size overflow\")\n",
        "    ingredients[0:ing_size] = this_batch[0:ing_size]\n",
        "  else:\n",
        "    ingredients[0:ins_index] = this_batch[0:ins_index]\n",
        "  sample[0:ins_index-1] = 0\n",
        "  nz = sample.nonzero().squeeze()\n",
        "  # move the non-zero elements of instructions vector to left\n",
        "  directions = torch.zeros(sample.numel() - nz.numel())\n",
        "  directions = torch.full_like(directions,recipe_end_id)\n",
        "  # directions = this_batch[-1]\n",
        "  directions = torch.cat((sample[nz], directions))\n",
        "  directions[0] = 0\n",
        "  # print(ins_subvector.shape)\n",
        "  del sample\n",
        "  return ingredients, directions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71QjYNG0wggI"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# from transformer import AlbertTokenizer, AlbertForMultipleChoice\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig, BartForCausalLM\n",
        "from torch.nn import CrossEntropyLoss, MSELoss, GRU\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "class bartWithGRU(nn.Module):\n",
        "    def __init__(self, tokenizer, token_size=512):\n",
        "        super(bartWithGRU, self).__init__()\n",
        "\n",
        "        # device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        self.bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "        self.bart.resize_token_embeddings(len(tokenizer))\n",
        "        # self.lm_head = nn.Linear(self.bart.config.d_model, self.bart.config.vocab_size, bias=False)\n",
        "\n",
        "        # add an module for consistency checking\n",
        "        self.gru = torch.nn.GRU(self.bart.config.d_model, 32 , 1 ,batch_first = False)\n",
        "        self.gru_head = nn.Linear(32, 1)\n",
        "        self.linear_activation = nn.Sigmoid()\n",
        "        self.cuda()\n",
        "\n",
        "    def forward(self,encoder_batch, batch, inst_pos):\n",
        "        # inst_pos: the position of token <NEXT_INSTR>, as a index list.\n",
        "        # print(batch.shape)\n",
        "        \n",
        "        outputs = self.bart(input_ids = encoder_batch,\n",
        "                            decoder_input_ids = batch,\n",
        "                            output_hidden_states=True)\n",
        "        last_hidden = outputs[\"decoder_hidden_states\"][-1]\n",
        "        # gru_input = torch.zeros(batch.size()[0],max_length,self.bart.config.d_model)\n",
        "        tokens_list = []\n",
        "        for i in range(batch.size()[0]):\n",
        "          batch_hidden = last_hidden[inst_pos[i][:,0],inst_pos[i][:,1],:]\n",
        "          tokens_list.append(batch_hidden)\n",
        "        gru_input = pad_sequence(tokens_list)\n",
        "        gru_output,_ = self.gru(gru_input)\n",
        "        \n",
        "\n",
        "        # print(ins_subvector.shape)\n",
        "        # consist_output = self.consist_logit(ins_subvector.half().to(args.device))\n",
        "        consist_output = self.gru_head(gru_output[-1])\n",
        "        # print(consist_output.shape)\n",
        "        consist_output = self.linear_activation(consist_output)\n",
        "\n",
        "        logits = outputs[\"logits\"]\n",
        "\n",
        "        return logits, consist_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg5IblFRBET8"
      },
      "source": [
        "def save_model(args,model,tokenizer,model_class,tokenizer_class):\n",
        "  # Create output directory if neede\n",
        "  if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)\n",
        "\n",
        "  logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "  # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "  # They can then be reloaded using `from_pretrained()`\n",
        "  model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "  model_to_save.save_pretrained(args.output_dir)\n",
        "  tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "  # Good practice: save your training arguments together with the trained model\n",
        "  torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n",
        "\n",
        "  # Load a trained model and vocabulary that you have fine-tuned\n",
        "  model = model_class.from_pretrained(args.output_dir)\n",
        "  tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
        "  model.to(args.device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcMyNMy_3gUO"
      },
      "source": [
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "\n",
        "def train_seq(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n",
        "                                                num_training_steps=t_total)\n",
        "\n",
        "    try:\n",
        "        from apex import amp\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "\n",
        "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\")\n",
        "\n",
        "    # Train!\n",
        "    global_step = 0\n",
        "    epoch = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "\n",
        "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=True)\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=True)\n",
        "        \n",
        "\n",
        "        epoch +=1\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            batch_drop = 0\n",
        "            token_list = tokenizer.convert_tokens_to_ids([\"<INSTR_START>\",\n",
        "                                                          \"<INGR_START>\",\n",
        "                                                          \"<NEXT_INSTR>\",\n",
        "                                                          \"<RECIPE_END>\"])\n",
        "            ins_start_id = token_list[0]\n",
        "            ing_start_id = token_list[1]\n",
        "            ins_element_id = token_list[2]\n",
        "            recipe_end_id = token_list[3]\n",
        "\n",
        "            # random_shuffle = torch.randint(2, (batch.size()[0], 1),device = args.device)\n",
        "            random_shuffle = torch.randint(1, (batch.size()[0], 1),device = args.device)\n",
        " \n",
        "            tokens_list = []\n",
        "            max_length = 0\n",
        "\n",
        "            encoder_batch = torch.empty(0,48)\n",
        "            decoder_batch = torch.empty(0,512)\n",
        "            for batch_no in range(len(batch)):\n",
        "              if (batch[batch_no] == ins_start_id).nonzero().size()[0] == 0:\n",
        "                print(tokenizer.decode(batch[batch_no]))\n",
        "                print(\"An error happens, break\")\n",
        "                batch_drop = 1\n",
        "                break\n",
        "              \n",
        "              # randomly shuffle the batch\n",
        "              if random_shuffle[batch_no] == 1:\n",
        "                shuffle_batch = shuffle_instruction(batch[batch_no],\n",
        "                                                      ins_start_id,ing_start_id)\n",
        "              else:\n",
        "                shuffle_batch = batch[batch_no]\n",
        "              # split the encoder & decoder input\n",
        "              encoder_input, decoder_input = split_ing_dirs(shuffle_batch)\n",
        "              \n",
        "              # encoder & decoder input list\n",
        "              encoder_batch = torch.cat((encoder_batch, encoder_input.unsqueeze(0)))\n",
        "              decoder_batch = torch.cat((decoder_batch, decoder_input.unsqueeze(0)))\n",
        "              \n",
        "            if batch_drop == 1:\n",
        "              print(\"An error in this batch, break\")\n",
        "              torch.cuda.empty_cache()\n",
        "              continue\n",
        "\n",
        "            # prepare the batch for GPU training\n",
        "            encoder_batch = encoder_batch.long().to(args.device)\n",
        "            decoder_batch = decoder_batch.long().to(args.device)\n",
        "            inputs, labels = (decoder_batch[:, 0:-1], decoder_batch[:, 1:])\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "\n",
        "            loss_CE = CrossEntropyLoss()\n",
        "            loss_MSE = MSELoss()\n",
        "\n",
        "            # model feed-forward\n",
        "            # logits, consist_output = model(encoder_batch, inputs, tokens_list)\n",
        "            output = model(input_ids = encoder_batch, decoder_input_ids = inputs)\n",
        "            logits = output[\"logits\"]\n",
        "            # loss adjustment\n",
        "            bart_vocab_size = 50278\n",
        "            loss = loss_CE(logits.reshape(-1, bart_vocab_size), labels.reshape(-1))\n",
        "            # loss2 = loss_MSE(consist_output, random_shuffle.float())\n",
        "            # loss = loss1 + loss2\n",
        "            # print(\"loss comp: \", loss, outputs['loss'])\n",
        "            if global_step % 100 == 0:\n",
        "              logger.info(step)\n",
        "              print(\"step\", global_step)\n",
        "              print(\"loss:\",loss.item())\n",
        "            \n",
        "\n",
        "            if global_step % 10000 == 0 and global_step !=0:\n",
        "              model_checkpoint = model\n",
        "              model_class = BartForConditionalGeneration\n",
        "              tokenizer_class = BartTokenizer\n",
        "              save_model(args,model_checkpoint,tokenizer,model_class,tokenizer_class)\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "            # print(\"cp 4\")\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                # print(scaled_loss)\n",
        "                scaled_loss.backward()\n",
        "            # print(\"cp 5\")\n",
        "            tr_loss += loss.item()\n",
        "            if (step) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "            if args.max_steps > 0 and global_step > 2:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "            # del inputs, labels,logits,consist_output , loss1,loss2, loss\n",
        "            del inputs, labels, logits, loss, encoder_batch, decoder_batch, output\n",
        "            torch.cuda.empty_cache()\n",
        "            # break\n",
        "\n",
        "    return global_step, tr_loss / global_step, (batch, encoder_batch, decoder_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRCAG6cGh-oc"
      },
      "source": [
        "def model_init(args,logger, model_class, tokenizer):\n",
        "  if args.eval_data_file is None and args.do_eval:\n",
        "    raise ValueError(\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "                         \"or remove the --do_eval argument.\")\n",
        "  if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
        "    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "  args.n_gpu = torch.cuda.device_count()\n",
        "  args.device = device\n",
        "\n",
        "  # Setup logging\n",
        "  logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                        level = logging.INFO)\n",
        "\n",
        "  # model = bartWithGRU(tokenizer)\n",
        "  model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "  if args.block_size <= 0:\n",
        "    args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "  args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "  model.to(args.device)\n",
        "\n",
        "  logger.info(\"Training/evaluation parameters %s\", args)\n",
        "  return model, logger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OOAth1Vp5ucw",
        "outputId": "96b87f9a-3f6d-4512-cfd8-20f3dab4a6d8"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
        "mybart, logger = model_init(args,logger,BartForConditionalGeneration,tokenizer)\n",
        "global_step, tr_loss, batch = train_seq(args, train_dataset, mybart, tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[15439,  9303,   118,  ..., 50274, 50274, 50274],\n",
            "        [ 2379,  5961,  8929,  ..., 50274, 50274, 50274],\n",
            "        [32588,  9050,    29,  ..., 50274, 50274, 50274],\n",
            "        [  611, 13552, 32588,  ..., 50274, 50274, 50274],\n",
            "        [30132,   620, 13161,  ..., 50274, 50274, 50274]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:02:01 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-07, aws_bucket='', block_size=1022, cache_dir='', config_name='', device=device(type='cuda'), do_eval=False, do_lower_case=False, do_train=False, eval_all_checkpoints=False, eval_data_file=None, evaluate_during_training=False, gradient_accumulation_steps=1, learning_rate=0.0002, logging_steps=50, max_grad_norm=1.0, max_steps=-1, model_name_or_path='facebook/bart-base', model_type='facebook/bart-base', n_gpu=1, no_cuda=False, num_train_epochs=1, output_dir='./gdrive/MyDrive/COMP0087/model_checkpoint_0526', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=8, save_steps=50, tokenizer_name='', train_data_file='unsupervised.h5', warmup_steps=0, weight_decay=0.0)\n",
            "05/31/2021 09:02:02 - INFO - __main__ -   0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "step 0\n",
            "loss: 9.637866020202637\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:125: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:03:06 - INFO - __main__ -   100\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 100\n",
            "loss: 1.371131420135498\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:04:11 - INFO - __main__ -   200\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 200\n",
            "loss: 1.0002011060714722\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:05:16 - INFO - __main__ -   300\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 300\n",
            "loss: 0.6826380491256714\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:06:22 - INFO - __main__ -   400\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 400\n",
            "loss: 0.6156041026115417\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:07:26 - INFO - __main__ -   500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 500\n",
            "loss: 0.7667820453643799\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:08:31 - INFO - __main__ -   600\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 600\n",
            "loss: 1.0622549057006836\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:09:37 - INFO - __main__ -   700\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 700\n",
            "loss: 0.7290205359458923\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:10:43 - INFO - __main__ -   800\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 800\n",
            "loss: 0.7150540947914124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:11:48 - INFO - __main__ -   900\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 900\n",
            "loss: 0.7609317302703857\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:12:53 - INFO - __main__ -   1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 1000\n",
            "loss: 0.711898684501648\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:13:58 - INFO - __main__ -   1100\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 1100\n",
            "loss: 0.8297210931777954\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:15:03 - INFO - __main__ -   1200\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 1200\n",
            "loss: 0.8677281141281128\n",
            "pork bellyCooking oiloninogarlicchicken liversgreen banana chillifish saucecalamans juicesaltred onion<s> <RECIPE_START> <INPUT_START> pork belly <NEXT_INPUT> Cooking oil <NEXT_INPUT> onino <NEXT_INPUT> garlic <NEXT_INPUT> chicken livers <NEXT_INPUT> green banana chilli <NEXT_INPUT> fish sauce <NEXT_INPUT> calamansi juice <NEXT_INPUT> salt <NEXT_INPUT> red onion <INPUT_END> <INGR_START> 750 grams well-layered, skin-on pork belly <NEXT_INGR> 2 pig ears (about 350 grams in total) <NEXT_INGR> Cooking oil, as needed <NEXT_INGR> 1 onino (about 220 grams), chopped <NEXT_INGR> 2 garlic cloves, finely chopped <NEXT_INGR> 200 grams chicken livers, finely chopped <NEXT_INGR> 4-8 red bird's-eye chillies (or more to taste), chopped <NEXT_INGR> 1 green banana chilli, cut into 5mm pieces <NEXT_INGR> About 20ml fish sauce <NEXT_INGR> About 20ml fresh calamansi juice <NEXT_INGR> About 30ml vinegar (distilled while, rice or coconut vinegar) <NEXT_INGR> Fine seas salt and fresh ground black pepper <NEXT_INGR> To serve:\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:16:08 - INFO - __main__ -   1301\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 1300\n",
            "loss: 0.7237846851348877\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:17:14 - INFO - __main__ -   1401\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 1400\n",
            "loss: 0.7157435417175293\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:18:20 - INFO - __main__ -   1501\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 1500\n",
            "loss: 0.6113077402114868\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:19:25 - INFO - __main__ -   1601\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 1600\n",
            "loss: 0.5382362604141235\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:20:30 - INFO - __main__ -   1701\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 1700\n",
            "loss: 0.7761737704277039\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:21:36 - INFO - __main__ -   1801\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 1800\n",
            "loss: 0.5212308168411255\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:22:42 - INFO - __main__ -   1901\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 1900\n",
            "loss: 0.8767653107643127\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:23:47 - INFO - __main__ -   2001\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 2000\n",
            "loss: 0.6793705821037292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:24:53 - INFO - __main__ -   2101\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 2100\n",
            "loss: 0.7326722741127014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:25:59 - INFO - __main__ -   2201\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 2200\n",
            "loss: 0.8405823111534119\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:27:05 - INFO - __main__ -   2301\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 2300\n",
            "loss: 0.652576744556427\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:28:09 - INFO - __main__ -   2401\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 2400\n",
            "loss: 0.8548592329025269\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:29:15 - INFO - __main__ -   2501\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 2500\n",
            "loss: 0.5359354019165039\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:30:21 - INFO - __main__ -   2601\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 2600\n",
            "loss: 0.5160925984382629\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:31:27 - INFO - __main__ -   2701\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 2700\n",
            "loss: 0.7004799246788025\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:32:33 - INFO - __main__ -   2801\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 2800\n",
            "loss: 0.8167957067489624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:33:38 - INFO - __main__ -   2901\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 2900\n",
            "loss: 0.8976501226425171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:34:44 - INFO - __main__ -   3001\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 3000\n",
            "loss: 0.8106663823127747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:35:50 - INFO - __main__ -   3101\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 3100\n",
            "loss: 0.9057751297950745\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:36:54 - INFO - __main__ -   3201\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 3200\n",
            "loss: 0.6706288456916809\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:37:58 - INFO - __main__ -   3301\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 3300\n",
            "loss: 0.9187849760055542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:39:04 - INFO - __main__ -   3401\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 3400\n",
            "loss: 0.5446830987930298\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:40:09 - INFO - __main__ -   3501\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 3500\n",
            "loss: 0.4992559254169464\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:41:14 - INFO - __main__ -   3601\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 3600\n",
            "loss: 0.7289144992828369\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:42:20 - INFO - __main__ -   3701\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 3700\n",
            "loss: 0.6913579702377319\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:43:25 - INFO - __main__ -   3801\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 3800\n",
            "loss: 0.7413060069084167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:44:31 - INFO - __main__ -   3901\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 3900\n",
            "loss: 0.42608797550201416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:45:36 - INFO - __main__ -   4001\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 4000\n",
            "loss: 0.6272333264350891\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:46:42 - INFO - __main__ -   4101\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 4100\n",
            "loss: 0.5595123767852783\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:47:48 - INFO - __main__ -   4201\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 4200\n",
            "loss: 0.7256362438201904\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:48:54 - INFO - __main__ -   4301\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 4300\n",
            "loss: 0.8129202127456665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:49:59 - INFO - __main__ -   4401\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 4400\n",
            "loss: 0.5328766703605652\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:51:05 - INFO - __main__ -   4501\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 4500\n",
            "loss: 0.7625519037246704\n",
            "size overflow\n",
            "TMGreen CurryPrawnsChickenGarlicRootswhite pepperStarchSauceugarOnionsSpring Roll Paperhandful of Coriander<s> <RECIPE_START> <INPUT_START> TMGreen Curry <NEXT_INPUT> Prawns <NEXT_INPUT> Chicken <NEXT_INPUT> Garlic <NEXT_INPUT> Roots <NEXT_INPUT> white pepper <NEXT_INPUT> Starch <NEXT_INPUT> Sauce <NEXT_INPUT> Sugar <NEXT_INPUT> Onions <NEXT_INPUT> Spring Roll Paper <NEXT_INPUT> handful of Coriander <INPUT_END> <INGR_START> 1-2 Tbsp.TMGreen Curry Paste <NEXT_INGR> 100g. Minced Prawns <NEXT_INGR> 100g. Minced Chicken <NEXT_INGR> 3 Cloves Garlic <NEXT_INGR> 2 Coriander Roots <NEXT_INGR> 1tsp white pepper powder <NEXT_INGR> 1tsp. Corn Starch <NEXT_INGR> 1Tbsp TMThai Premium Light Soya Sauce <NEXT_INGR> 1/2Tbsp Sugar <NEXT_INGR> Blanched Spring Onions <NEXT_INGR> Spring Roll Paper (Cut into Circles aprox. 16cm diameter, and reserve some small squares from the scraps) <NEXT_INGR> Oil for Frying <NEXT_INGR> Optional:\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:52:11 - INFO - __main__ -   4602\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "size overflow\n",
            "step 4600\n",
            "loss: 0.8704532980918884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:53:16 - INFO - __main__ -   4702\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 4700\n",
            "loss: 0.7600916028022766\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:54:22 - INFO - __main__ -   4802\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 4800\n",
            "loss: 0.6628653407096863\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:55:28 - INFO - __main__ -   4902\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 4900\n",
            "loss: 0.5990537405014038\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:56:33 - INFO - __main__ -   5002\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 5000\n",
            "loss: 0.9040267467498779\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:57:38 - INFO - __main__ -   5102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 5100\n",
            "loss: 0.5590822100639343\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:58:43 - INFO - __main__ -   5202\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 5200\n",
            "loss: 0.41240400075912476\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 09:59:47 - INFO - __main__ -   5302\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 5300\n",
            "loss: 0.4575049877166748\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:00:53 - INFO - __main__ -   5402\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 5400\n",
            "loss: 0.6522254943847656\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:01:58 - INFO - __main__ -   5502\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 5500\n",
            "loss: 0.5568172931671143\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:03:03 - INFO - __main__ -   5602\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 5600\n",
            "loss: 0.8105930685997009\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:04:09 - INFO - __main__ -   5702\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 5700\n",
            "loss: 0.47512751817703247\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:05:14 - INFO - __main__ -   5802\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 5800\n",
            "loss: 0.9145904779434204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:06:19 - INFO - __main__ -   5902\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 5900\n",
            "loss: 0.5179039239883423\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:07:24 - INFO - __main__ -   6002\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 6000\n",
            "loss: 0.5364900827407837\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:08:29 - INFO - __main__ -   6102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 6100\n",
            "loss: 0.9135105609893799\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:09:35 - INFO - __main__ -   6202\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 6200\n",
            "loss: 0.4147513806819916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:10:40 - INFO - __main__ -   6302\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 6300\n",
            "loss: 0.8946548104286194\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:11:45 - INFO - __main__ -   6402\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 6400\n",
            "loss: 0.38977742195129395\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:12:50 - INFO - __main__ -   6502\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 6500\n",
            "loss: 0.5885995030403137\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:13:56 - INFO - __main__ -   6602\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 6600\n",
            "loss: 0.609269380569458\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:15:02 - INFO - __main__ -   6702\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 6700\n",
            "loss: 0.665931224822998\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:16:08 - INFO - __main__ -   6802\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 6800\n",
            "loss: 0.6470587253570557\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:17:13 - INFO - __main__ -   6902\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 6900\n",
            "loss: 0.6163883209228516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:18:19 - INFO - __main__ -   7002\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 7000\n",
            "loss: 0.5183236598968506\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:19:25 - INFO - __main__ -   7102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 7100\n",
            "loss: 0.6318613290786743\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:20:31 - INFO - __main__ -   7202\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 7200\n",
            "loss: 0.5732081532478333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:21:37 - INFO - __main__ -   7302\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 7300\n",
            "loss: 0.534240186214447\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:22:43 - INFO - __main__ -   7402\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 7400\n",
            "loss: 0.6616339683532715\n",
            "size overflow\n",
            "INGREDIENTSrice noodlescoriander seedsuminfennelberriesoconut oilonionlemongrassgarlicgingerSaltturmericayennelimefish sauce milkshrimpcherry tomatoesslivered scallionsCilantro<s> <RECIPE_START> <INPUT_START> INGREDIENTS <NEXT_INPUT> rice noodles <NEXT_INPUT> coriander seeds <NEXT_INPUT> cumin seeds <NEXT_INPUT> fennel seeds <NEXT_INPUT> berries <NEXT_INPUT> coconut oil <NEXT_INPUT> onion <NEXT_INPUT> lemongrass <NEXT_INPUT> garlic <NEXT_INPUT> ginger <NEXT_INPUT> Salt <NEXT_INPUT> turmeric <NEXT_INPUT> cayenne <NEXT_INPUT> lime <NEXT_INPUT> fish sauce <NEXT_INPUT> coconut milk <NEXT_INPUT> shrimp <NEXT_INPUT> cherry tomatoes <NEXT_INPUT> slivered scallions <NEXT_INPUT> Cilantro <INPUT_END> <INGR_START> INGREDIENTS <NEXT_INGR> 12 ounces rice noodles (vermicelli) <NEXT_INGR> A1/2 teaspoon coriander seeds <NEXT_INGR> A1/2 teaspoon cumin seeds <NEXT_INGR> A1/2 teaspoon fennel seeds <NEXT_INGR> 6 allspice berries <NEXT_INGR> 2 tablespoons coconut oil <NEXT_INGR> 1 medium onion, finely diced, about 2 cups <NEXT_INGR> 2 tablespoons finely chopped lemongrass <NEXT_INGR> A1/2 teaspoon grated garlic <NEXT_INGR> 2 teaspoons grated ginger <NEXT_INGR> Salt and pepper <NEXT_INGR> 1 A1/2 teaspoons turmeric <NEXT_INGR> a\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:23:49 - INFO - __main__ -   7503\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 7500\n",
            "loss: 0.8432539701461792\n",
            "tomato pasteorange juicelimeolive oilpork tenderloingolden pineapplered bell pepper onionVegetable cooking spray<s> <RECIPE_START> <INPUT_START> tomato paste <NEXT_INPUT> orange juice <NEXT_INPUT> lime juice <NEXT_INPUT> olive oil <NEXT_INPUT> pork tenderloin <NEXT_INPUT> golden pineapple <NEXT_INPUT> red bell pepper <NEXT_INPUT> red onion <NEXT_INPUT> Vegetable oil cooking spray <INPUT_END> <INGR_START> 1 tablespoon tomato paste <NEXT_INGR> 1 tablespoon mild or hot chili powder (or achiote paste) <NEXT_INGR> 1/2 cup orange juice <NEXT_INGR> 3 tablespoon fresh lime juice <NEXT_INGR> 1 tablespoon olive oil <NEXT_INGR> 1 pound pork tenderloin, trimmed <NEXT_INGR> 1 small golden pineapple (about 1 1/2 pounds), diced into 1-inch cubes\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:24:54 - INFO - __main__ -   7604\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 7600\n",
            "loss: 0.5813265442848206\n",
            "size overflow\n",
            "/cullsausagebetterwhite fleshed fishshrimpseafood mix winechicken brothtomatoesatoyellow onions onionshallotsgarlicOlive oilChili powderPaprikaCuminlemons<s> <RECIPE_START> <INPUT_START> /culls <NEXT_INPUT> sausage <NEXT_INPUT> better <NEXT_INPUT> white fleshed fish <NEXT_INPUT> shrimp <NEXT_INPUT> seafood mix <NEXT_INPUT> white wine <NEXT_INPUT> chicken broth <NEXT_INPUT> tomatoes <NEXT_INPUT> tomato <NEXT_INPUT> yellow onions <NEXT_INPUT> yellow onion <NEXT_INPUT> shallots <NEXT_INPUT> garlic <NEXT_INPUT> Olive oil <NEXT_INPUT> Chili powder <NEXT_INPUT> Paprika <NEXT_INPUT> Cumin <NEXT_INPUT> lemons <INPUT_END> <INGR_START> 2 lobster bodies/culls <NEXT_INGR> Linguica or Chorizo sausage ~ 1 lbs, casing removed and broken up semi-coarse. <NEXT_INGR> 12 to 14 clams (smaller clams are better, like mahoganies. Let them stand for about 20 minutes in a cold water bath with a tsp. of white vinegar added, so they spit out their sand. ) <NEXT_INGR> 1.5 lbs white fleshed fish (cod or haddock are best, hake marginally less so\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:26:00 - INFO - __main__ -   7705\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 7700\n",
            "loss: 0.618248701095581\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:27:05 - INFO - __main__ -   7805\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 7800\n",
            "loss: 0.475715309381485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:28:11 - INFO - __main__ -   7905\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 7900\n",
            "loss: 0.7645446062088013\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:29:17 - INFO - __main__ -   8005\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 8000\n",
            "loss: 0.8272987008094788\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:30:22 - INFO - __main__ -   8105\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 8100\n",
            "loss: 0.6835487484931946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:31:28 - INFO - __main__ -   8205\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 8200\n",
            "loss: 0.4600045680999756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:32:33 - INFO - __main__ -   8305\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 8300\n",
            "loss: 0.9019973874092102\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:33:39 - INFO - __main__ -   8405\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 8400\n",
            "loss: 0.8640503883361816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:34:44 - INFO - __main__ -   8505\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 8500\n",
            "loss: 0.3495856821537018\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:35:50 - INFO - __main__ -   8605\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 8600\n",
            "loss: 0.6826261281967163\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:36:54 - INFO - __main__ -   8705\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 8700\n",
            "loss: 0.45048224925994873\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:38:00 - INFO - __main__ -   8805\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 8800\n",
            "loss: 0.7122798562049866\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:39:05 - INFO - __main__ -   8905\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 8900\n",
            "loss: 0.6091338396072388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:40:11 - INFO - __main__ -   9005\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 9000\n",
            "loss: 0.6630808711051941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:41:17 - INFO - __main__ -   9105\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 9100\n",
            "loss: 0.5402780771255493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:42:22 - INFO - __main__ -   9205\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 9200\n",
            "loss: 0.837860643863678\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:43:26 - INFO - __main__ -   9305\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 9300\n",
            "loss: 0.7396346926689148\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:44:31 - INFO - __main__ -   9405\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 9400\n",
            "loss: 0.6218572854995728\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:45:36 - INFO - __main__ -   9505\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 9500\n",
            "loss: 0.5297142267227173\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:46:41 - INFO - __main__ -   9605\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 9600\n",
            "loss: 0.535481870174408\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:47:47 - INFO - __main__ -   9705\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 9700\n",
            "loss: 0.6049140095710754\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:48:52 - INFO - __main__ -   9805\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 9800\n",
            "loss: 0.7505099177360535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:49:57 - INFO - __main__ -   9905\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 9900\n",
            "loss: 0.5200182795524597\n",
            "soy saucesesame oillight brown sugarground turkeyvegetableeggsriceables seeds<s> <RECIPE_START> <INPUT_START> soy sauce <NEXT_INPUT> sesame oil <NEXT_INPUT> light brown sugar <NEXT_INPUT> ground turkey <NEXT_INPUT> vegetable oil <NEXT_INPUT> eggs <NEXT_INPUT> rice <NEXT_INPUT> vegetables <NEXT_INPUT> sesame seeds <INPUT_END> <INGR_START> 3/4 cup soy sauce <NEXT_INGR> 6 tablespoons toasted sesame oil <NEXT_INGR> 4 tablespoons light brown sugar <NEXT_INGR> 1 pound ground turkey, beef, or pork <NEXT_INGR> 2 tablespoons vegetable oil <NEXT_INGR> 4 large eggs <NEXT_INGR> 2 cups cooked rice (Chinese takeout works well) <NEXT_INGR> Assorted vegetables (shredded carrot, canned baby corn, spinach, bean sprouts, red onion\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:51:03 - INFO - __main__ -   10006\n",
            "05/31/2021 10:51:03 - INFO - __main__ -   Saving model checkpoint to ./gdrive/MyDrive/COMP0087/model_checkpoint_0526\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 10000\n",
            "loss: 0.7857111692428589\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:52:18 - INFO - __main__ -   10106\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 10100\n",
            "loss: 0.5980057716369629\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:53:24 - INFO - __main__ -   10206\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 10200\n",
            "loss: 0.7552639842033386\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:54:29 - INFO - __main__ -   10306\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 10300\n",
            "loss: 0.6526595950126648\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:55:35 - INFO - __main__ -   10406\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 10400\n",
            "loss: 0.6269242167472839\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:56:41 - INFO - __main__ -   10506\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 10500\n",
            "loss: 0.6616817116737366\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:57:48 - INFO - __main__ -   10606\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 10600\n",
            "loss: 0.697540819644928\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:58:53 - INFO - __main__ -   10706\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 10700\n",
            "loss: 0.6746596097946167\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 10:59:59 - INFO - __main__ -   10806\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 10800\n",
            "loss: 0.5510668754577637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:01:05 - INFO - __main__ -   10906\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 10900\n",
            "loss: 0.6191090941429138\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:02:10 - INFO - __main__ -   11006\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 11000\n",
            "loss: 0.41058534383773804\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:03:16 - INFO - __main__ -   11106\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 11100\n",
            "loss: 0.5934430360794067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:04:21 - INFO - __main__ -   11206\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 11200\n",
            "loss: 0.7635788917541504\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:05:26 - INFO - __main__ -   11306\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 11300\n",
            "loss: 1.064764380455017\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:06:31 - INFO - __main__ -   11406\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 11400\n",
            "loss: 0.523987889289856\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:07:37 - INFO - __main__ -   11506\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 11500\n",
            "loss: 0.5191237926483154\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:08:43 - INFO - __main__ -   11606\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 11600\n",
            "loss: 0.5687115788459778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:09:49 - INFO - __main__ -   11706\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 11700\n",
            "loss: 0.6139957904815674\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:10:56 - INFO - __main__ -   11806\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 11800\n",
            "loss: 0.557357132434845\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:12:02 - INFO - __main__ -   11906\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 11900\n",
            "loss: 0.517829954624176\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:13:08 - INFO - __main__ -   12006\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 12000\n",
            "loss: 0.8218224048614502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:14:13 - INFO - __main__ -   12106\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 12100\n",
            "loss: 0.6342121362686157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:15:16 - INFO - __main__ -   12206\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 12200\n",
            "loss: 0.5863090753555298\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:16:21 - INFO - __main__ -   12306\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 12300\n",
            "loss: 0.5880619287490845\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:17:27 - INFO - __main__ -   12406\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 12400\n",
            "loss: 0.5266156792640686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:18:33 - INFO - __main__ -   12506\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 12500\n",
            "loss: 0.5081561207771301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:19:38 - INFO - __main__ -   12606\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 12600\n",
            "loss: 0.5437636375427246\n",
            "size overflow\n",
            "Green mixchickpeaoniongarlicfresh gingerolive oilvineSalt<s> <RECIPE_START> <INPUT_START> Green mix <NEXT_INPUT> chickpea <NEXT_INPUT> onion <NEXT_INPUT> garlic <NEXT_INPUT> fresh ginger <NEXT_INPUT> olive oil <NEXT_INPUT> vinegar <NEXT_INPUT> Salt <INPUT_END> <INGR_START> Green mix (baby kale +baby spinach+chard) <NEXT_INGR> 120 g chickpea <NEXT_INGR> 1 small onion minced <NEXT_INGR> few garlic pieces minced <NEXT_INGR> fresh ginger minced <NEXT_INGR> 1 Tbs and half olive oil divided <NEXT_INGR> 1 tsp vinegar <NEXT_INGR> Spices\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:20:44 - INFO - __main__ -   12707\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 12700\n",
            "loss: 0.5679988265037537\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:21:49 - INFO - __main__ -   12807\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 12800\n",
            "loss: 0.6275295615196228\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:22:54 - INFO - __main__ -   12907\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 12900\n",
            "loss: 0.4501357674598694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:24:00 - INFO - __main__ -   13007\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 13000\n",
            "loss: 0.6142052412033081\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:25:05 - INFO - __main__ -   13107\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 13100\n",
            "loss: 0.6312041878700256\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:26:11 - INFO - __main__ -   13207\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 13200\n",
            "loss: 0.5267622470855713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:27:17 - INFO - __main__ -   13307\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 13300\n",
            "loss: 0.7327890396118164\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:28:23 - INFO - __main__ -   13407\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 13400\n",
            "loss: 0.6619426608085632\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:29:28 - INFO - __main__ -   13507\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 13500\n",
            "loss: 0.4730161726474762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:30:33 - INFO - __main__ -   13607\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 13600\n",
            "loss: 0.3258533179759979\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:31:39 - INFO - __main__ -   13707\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 13700\n",
            "loss: 0.6650752425193787\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:32:44 - INFO - __main__ -   13807\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 13800\n",
            "loss: 0.41920405626296997\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:33:49 - INFO - __main__ -   13907\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 13900\n",
            "loss: 0.6146359443664551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:34:55 - INFO - __main__ -   14007\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 14000\n",
            "loss: 0.5079565644264221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:36:00 - INFO - __main__ -   14107\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 14100\n",
            "loss: 0.636781632900238\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:37:06 - INFO - __main__ -   14207\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 14200\n",
            "loss: 0.7747756242752075\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:38:11 - INFO - __main__ -   14307\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 14300\n",
            "loss: 0.5231420397758484\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:39:17 - INFO - __main__ -   14407\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 14400\n",
            "loss: 0.5046650171279907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:40:23 - INFO - __main__ -   14507\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 14500\n",
            "loss: 0.6330894231796265\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:41:29 - INFO - __main__ -   14607\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 14600\n",
            "loss: 0.8036892414093018\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:42:34 - INFO - __main__ -   14707\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 14700\n",
            "loss: 0.553772509098053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:43:39 - INFO - __main__ -   14807\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 14800\n",
            "loss: 0.6723473072052002\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:44:44 - INFO - __main__ -   14907\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 14900\n",
            "loss: 0.3943721354007721\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:45:49 - INFO - __main__ -   15007\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 15000\n",
            "loss: 0.699558436870575\n",
            "chickenpork trottersgingeronionsabalonesaltChinese hammushroomsconoygoosestockArrowroot powder for thickeningbroccoli florets<s> <RECIPE_START> <INPUT_START> chicken <NEXT_INPUT> pork trotters <NEXT_INPUT> ginger <NEXT_INPUT> onions <NEXT_INPUT> abalone <NEXT_INPUT> salt <NEXT_INPUT> Chinese ham <NEXT_INPUT> mushrooms <NEXT_INPUT> conpoy <NEXT_INPUT> goose <NEXT_INPUT> stock <NEXT_INPUT> Arrowroot powder for thickening <NEXT_INPUT> broccoli florets <INPUT_END> <INGR_START> For the superior stock:\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:46:55 - INFO - __main__ -   15108\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 15100\n",
            "loss: 0.5332088470458984\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:48:01 - INFO - __main__ -   15208\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 15200\n",
            "loss: 0.5185982584953308\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:49:06 - INFO - __main__ -   15308\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 15300\n",
            "loss: 0.5881897211074829\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:50:11 - INFO - __main__ -   15408\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 15400\n",
            "loss: 0.6653026938438416\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:51:16 - INFO - __main__ -   15508\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 15500\n",
            "loss: 0.7490266561508179\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:52:22 - INFO - __main__ -   15608\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 15600\n",
            "loss: 0.6638048887252808\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:53:28 - INFO - __main__ -   15708\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 15700\n",
            "loss: 0.5002020001411438\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:54:34 - INFO - __main__ -   15808\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 15800\n",
            "loss: 0.6303401589393616\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:55:40 - INFO - __main__ -   15908\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 15900\n",
            "loss: 0.6453584432601929\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:56:45 - INFO - __main__ -   16008\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 16000\n",
            "loss: 0.605675995349884\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:57:51 - INFO - __main__ -   16108\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 16100\n",
            "loss: 0.48300740122795105\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 11:58:56 - INFO - __main__ -   16208\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 16200\n",
            "loss: 0.4898037612438202\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:00:01 - INFO - __main__ -   16308\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 16300\n",
            "loss: 0.7206087708473206\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:01:06 - INFO - __main__ -   16408\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 16400\n",
            "loss: 0.4614626169204712\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:02:11 - INFO - __main__ -   16508\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 16500\n",
            "loss: 0.7383950352668762\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:03:16 - INFO - __main__ -   16608\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 16600\n",
            "loss: 0.4565589427947998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:04:22 - INFO - __main__ -   16708\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 16700\n",
            "loss: 0.5216774344444275\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:05:27 - INFO - __main__ -   16808\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 16800\n",
            "loss: 0.6338500380516052\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:06:32 - INFO - __main__ -   16908\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 16900\n",
            "loss: 0.48384764790534973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:07:37 - INFO - __main__ -   17008\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 17000\n",
            "loss: 0.6689929366111755\n",
            "Ingredientsturkeyflourolive oil<s> <RECIPE_START> <INPUT_START> Ingredients <NEXT_INPUT> turkey <NEXT_INPUT> flour <NEXT_INPUT> olive oil <INPUT_END> <INGR_START> Ingredients <NEXT_INGR> 5.5kg/12lb oven ready organic turkey (thoroughly defrost if frozen)\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:08:42 - INFO - __main__ -   17109\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 17100\n",
            "loss: 0.40005505084991455\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:09:48 - INFO - __main__ -   17209\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 17200\n",
            "loss: 0.5376588702201843\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:10:54 - INFO - __main__ -   17309\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 17300\n",
            "loss: 0.770438015460968\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:11:59 - INFO - __main__ -   17409\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 17400\n",
            "loss: 0.6581230759620667\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:13:04 - INFO - __main__ -   17509\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 17500\n",
            "loss: 0.46841105818748474\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:14:10 - INFO - __main__ -   17609\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 17600\n",
            "loss: 0.5389888882637024\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:15:16 - INFO - __main__ -   17709\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 17700\n",
            "loss: 0.7731364965438843\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:16:21 - INFO - __main__ -   17809\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 17800\n",
            "loss: 0.4050605297088623\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:17:27 - INFO - __main__ -   17909\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 17900\n",
            "loss: 0.5570734739303589\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:18:32 - INFO - __main__ -   18009\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 18000\n",
            "loss: 0.42497649788856506\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:19:37 - INFO - __main__ -   18109\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 18100\n",
            "loss: 0.5671005249023438\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:20:43 - INFO - __main__ -   18209\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 18200\n",
            "loss: 0.4135579764842987\n",
            "ButterMushrooms.Onionfresh SpinachMozzarella cheeseRicottaEggsRomanoNutmegaltBasil SauceLasagna noodles<s> <RECIPE_START> <INPUT_START> Butter <NEXT_INPUT> Mushrooms <NEXT_INPUT>.Onion <NEXT_INPUT> fresh Spinach <NEXT_INPUT> Mozzarella cheese <NEXT_INPUT> Ricotta cheese <NEXT_INPUT>.Eggs <NEXT_INPUT> Romano cheese <NEXT_INPUT> Nutmeg <NEXT_INPUT> salt <NEXT_INPUT> Basil Sauce <NEXT_INPUT> Lasagna noodles <INPUT_END> <INGR_START> 3 Tablespoons\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:21:48 - INFO - __main__ -   18310\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 18300\n",
            "loss: 0.5512685775756836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:22:53 - INFO - __main__ -   18410\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 18400\n",
            "loss: 0.5694832801818848\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:23:59 - INFO - __main__ -   18510\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 18500\n",
            "loss: 0.6502681970596313\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:25:04 - INFO - __main__ -   18610\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 18600\n",
            "loss: 0.574193000793457\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:26:10 - INFO - __main__ -   18710\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 18700\n",
            "loss: 0.5670457482337952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:27:16 - INFO - __main__ -   18810\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 18800\n",
            "loss: 0.5971828103065491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:28:22 - INFO - __main__ -   18910\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 18900\n",
            "loss: 0.3168073296546936\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:29:27 - INFO - __main__ -   19010\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 19000\n",
            "loss: 0.5278951525688171\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:30:31 - INFO - __main__ -   19110\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 19100\n",
            "loss: 0.47438153624534607\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:31:38 - INFO - __main__ -   19210\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 19200\n",
            "loss: 0.5802218317985535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:32:43 - INFO - __main__ -   19310\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 19300\n",
            "loss: 0.6265994310379028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:33:48 - INFO - __main__ -   19410\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 19400\n",
            "loss: 0.5079110264778137\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:34:53 - INFO - __main__ -   19510\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 19500\n",
            "loss: 0.351580411195755\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:35:59 - INFO - __main__ -   19610\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 19600\n",
            "loss: 0.4791581332683563\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:37:04 - INFO - __main__ -   19710\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 19700\n",
            "loss: 0.6619683504104614\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:38:10 - INFO - __main__ -   19810\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 19800\n",
            "loss: 0.5427178740501404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:39:15 - INFO - __main__ -   19910\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 19900\n",
            "loss: 0.43759191036224365\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:40:21 - INFO - __main__ -   20010\n",
            "05/31/2021 12:40:21 - INFO - __main__ -   Saving model checkpoint to ./gdrive/MyDrive/COMP0087/model_checkpoint_0526\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 20000\n",
            "loss: 0.78375643491745\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:41:35 - INFO - __main__ -   20110\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 20100\n",
            "loss: 0.5928820371627808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:42:40 - INFO - __main__ -   20210\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 20200\n",
            "loss: 0.5157882571220398\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:43:46 - INFO - __main__ -   20310\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 20300\n",
            "loss: 0.49489372968673706\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:44:52 - INFO - __main__ -   20410\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 20400\n",
            "loss: 0.49081799387931824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:45:57 - INFO - __main__ -   20510\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 20500\n",
            "loss: 0.4895240366458893\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:47:03 - INFO - __main__ -   20610\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 20600\n",
            "loss: 0.791059672832489\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:48:08 - INFO - __main__ -   20710\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "size overflow\n",
            "step 20700\n",
            "loss: 0.6913101673126221\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:49:14 - INFO - __main__ -   20810\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 20800\n",
            "loss: 0.5989742875099182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:50:19 - INFO - __main__ -   20910\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 20900\n",
            "loss: 0.4529837369918823\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:51:24 - INFO - __main__ -   21010\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 21000\n",
            "loss: 0.5211907625198364\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:52:30 - INFO - __main__ -   21110\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 21100\n",
            "loss: 0.6019251346588135\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:53:34 - INFO - __main__ -   21210\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 21200\n",
            "loss: 0.642694890499115\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:54:39 - INFO - __main__ -   21310\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 21300\n",
            "loss: 0.6212145090103149\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:55:44 - INFO - __main__ -   21410\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 21400\n",
            "loss: 0.7750172019004822\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:56:49 - INFO - __main__ -   21510\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 21500\n",
            "loss: 0.6803484559059143\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:57:55 - INFO - __main__ -   21610\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 21600\n",
            "loss: 0.5615465641021729\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 12:59:01 - INFO - __main__ -   21710\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 21700\n",
            "loss: 0.7915619611740112\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:00:07 - INFO - __main__ -   21810\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 21800\n",
            "loss: 0.6430068016052246\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:01:13 - INFO - __main__ -   21910\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 21900\n",
            "loss: 0.6993038654327393\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:02:18 - INFO - __main__ -   22010\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 22000\n",
            "loss: 0.4468952417373657\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:03:24 - INFO - __main__ -   22110\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 22100\n",
            "loss: 0.880639910697937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:04:29 - INFO - __main__ -   22210\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 22200\n",
            "loss: 0.5771738886833191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:05:35 - INFO - __main__ -   22310\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 22300\n",
            "loss: 0.5327115654945374\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:06:40 - INFO - __main__ -   22410\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 22400\n",
            "loss: 0.665681779384613\n",
            "size overflow\n",
            "Ingredientscoriander seedsuminfennelbrown sugarkosher saltsalmonextra-virgin olive oil<s> <RECIPE_START> <INPUT_START> Ingredients <NEXT_INPUT> coriander seeds <NEXT_INPUT> cumin seeds <NEXT_INPUT> fennel seeds <NEXT_INPUT> brown sugar <NEXT_INPUT> kosher salt <NEXT_INPUT> salmon <NEXT_INPUT> extra-virgin olive oil <INPUT_END> <INGR_START> Ingredients: <NEXT_INGR> 1 tsp. coriander seeds <NEXT_INGR> 1 tsp. cumin seeds <NEXT_INGR> 1/2 tsp. fennel seeds <NEXT_INGR> 1 tsp. firmly packed light brown sugar <NEXT_INGR> 1 tsp. kosher salt <NEXT_INGR> 2 salmon fillets, each 6 to 8 oz., with skin intact,\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:07:46 - INFO - __main__ -   22511\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 22500\n",
            "loss: 0.554303765296936\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:08:51 - INFO - __main__ -   22611\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 22600\n",
            "loss: 0.4207393229007721\n",
            "Ingredientschicken - butterfliedlemon juicebasting saucecup)Dijionnaisewhite winemap syrupcayenneMix well<s> <RECIPE_START> <INPUT_START> Ingredients <NEXT_INPUT> chicken - butterflied <NEXT_INPUT> lemon juice <NEXT_INPUT> chicken <NEXT_INPUT> basting sauce <NEXT_INPUT> cup)Dijionnaise <NEXT_INPUT> white wine <NEXT_INPUT> maple syrup <NEXT_INPUT> cayenne <NEXT_INPUT> Mix well <INPUT_END> <INGR_START> Ingredients\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:09:57 - INFO - __main__ -   22712\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 22700\n",
            "loss: 0.6810171604156494\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:11:03 - INFO - __main__ -   22812\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 22800\n",
            "loss: 0.6876066327095032\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:12:09 - INFO - __main__ -   22912\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 22900\n",
            "loss: 0.4208383858203888\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:13:13 - INFO - __main__ -   23012\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 23000\n",
            "loss: 0.6468948125839233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:14:19 - INFO - __main__ -   23112\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 23100\n",
            "loss: 0.5219880938529968\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:15:24 - INFO - __main__ -   23212\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 23200\n",
            "loss: 0.5470429062843323\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:16:29 - INFO - __main__ -   23312\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 23300\n",
            "loss: 0.4351060390472412\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:17:34 - INFO - __main__ -   23412\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 23400\n",
            "loss: 0.5955062508583069\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:18:38 - INFO - __main__ -   23512\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 23500\n",
            "loss: 0.801723301410675\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:19:43 - INFO - __main__ -   23612\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 23600\n",
            "loss: 0.7962437272071838\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:20:49 - INFO - __main__ -   23712\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 23700\n",
            "loss: 0.7774975299835205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:21:55 - INFO - __main__ -   23812\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 23800\n",
            "loss: 0.722878098487854\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:23:00 - INFO - __main__ -   23912\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 23900\n",
            "loss: 0.6181535720825195\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:24:06 - INFO - __main__ -   24012\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "size overflow\n",
            "step 24000\n",
            "loss: 0.8815761208534241\n",
            "size overflow\n",
            "size overflow\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:25:12 - INFO - __main__ -   24112\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 24100\n",
            "loss: 0.3996518552303314\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:26:17 - INFO - __main__ -   24212\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 24200\n",
            "loss: 0.5920577645301819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:27:23 - INFO - __main__ -   24312\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 24300\n",
            "loss: 0.6794125437736511\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:28:28 - INFO - __main__ -   24412\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 24400\n",
            "loss: 0.4694322645664215\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:29:34 - INFO - __main__ -   24512\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 24500\n",
            "loss: 0.4920417070388794\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:30:40 - INFO - __main__ -   24612\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 24600\n",
            "loss: 0.5122520327568054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:31:46 - INFO - __main__ -   24712\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 24700\n",
            "loss: 0.5502093434333801\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:32:52 - INFO - __main__ -   24812\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 24800\n",
            "loss: 0.5172042846679688\n",
            "size overflow\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:33:58 - INFO - __main__ -   24912\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 24900\n",
            "loss: 0.5832816958427429\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:35:04 - INFO - __main__ -   25012\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 25000\n",
            "loss: 0.6089194416999817\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:36:09 - INFO - __main__ -   25112\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 25100\n",
            "loss: 0.4573003649711609\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:37:14 - INFO - __main__ -   25212\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "size overflow\n",
            "step 25200\n",
            "loss: 0.45622557401657104\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:38:20 - INFO - __main__ -   25312\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 25300\n",
            "loss: 0.3993646800518036\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:39:26 - INFO - __main__ -   25412\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 25400\n",
            "loss: 0.7697012424468994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:40:32 - INFO - __main__ -   25512\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 25500\n",
            "loss: 0.48303306102752686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:41:37 - INFO - __main__ -   25612\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 25600\n",
            "loss: 0.4358716309070587\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:42:42 - INFO - __main__ -   25712\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 25700\n",
            "loss: 0.5712738633155823\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:43:48 - INFO - __main__ -   25812\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 25800\n",
            "loss: 0.4009513854980469\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:44:54 - INFO - __main__ -   25912\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 25900\n",
            "loss: 0.727364182472229\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:45:58 - INFO - __main__ -   26012\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 26000\n",
            "loss: 0.3642948567867279\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:47:04 - INFO - __main__ -   26112\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 26100\n",
            "loss: 0.6577354073524475\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:48:10 - INFO - __main__ -   26212\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 26200\n",
            "loss: 0.535423219203949\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:49:14 - INFO - __main__ -   26312\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 26300\n",
            "loss: 0.6130877137184143\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:50:21 - INFO - __main__ -   26412\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 26400\n",
            "loss: 0.8821168541908264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:51:26 - INFO - __main__ -   26512\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 26500\n",
            "loss: 0.6137123107910156\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:52:32 - INFO - __main__ -   26612\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 26600\n",
            "loss: 0.541074275970459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:53:38 - INFO - __main__ -   26712\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 26700\n",
            "loss: 0.5764346122741699\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:54:43 - INFO - __main__ -   26812\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 26800\n",
            "loss: 0.47672373056411743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:55:49 - INFO - __main__ -   26912\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 26900\n",
            "loss: 0.35862842202186584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:56:55 - INFO - __main__ -   27012\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 27000\n",
            "loss: 0.6957908868789673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:58:01 - INFO - __main__ -   27112\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 27100\n",
            "loss: 0.46000662446022034\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 13:59:07 - INFO - __main__ -   27212\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 27200\n",
            "loss: 0.7941513657569885\n",
            "size overflow\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:00:13 - INFO - __main__ -   27312\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 27300\n",
            "loss: 0.668743908405304\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:01:19 - INFO - __main__ -   27412\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 27400\n",
            "loss: 0.29655057191848755\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:02:24 - INFO - __main__ -   27512\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 27500\n",
            "loss: 0.3973635137081146\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:03:27 - INFO - __main__ -   27612\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 27600\n",
            "loss: 0.49674150347709656\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:04:33 - INFO - __main__ -   27712\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 27700\n",
            "loss: 0.49775370955467224\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:05:38 - INFO - __main__ -   27812\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 27800\n",
            "loss: 0.4507959485054016\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:06:43 - INFO - __main__ -   27912\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 27900\n",
            "loss: 0.6915285587310791\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:07:48 - INFO - __main__ -   28012\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 28000\n",
            "loss: 0.4636543393135071\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:08:54 - INFO - __main__ -   28112\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 28100\n",
            "loss: 0.4465266764163971\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:10:00 - INFO - __main__ -   28212\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 28200\n",
            "loss: 0.6293724179267883\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:11:06 - INFO - __main__ -   28312\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 28300\n",
            "loss: 0.8344714641571045\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:12:11 - INFO - __main__ -   28412\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 28400\n",
            "loss: 0.46335238218307495\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:13:16 - INFO - __main__ -   28512\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 28500\n",
            "loss: 0.6007403135299683\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:14:22 - INFO - __main__ -   28612\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 28600\n",
            "loss: 0.3063984215259552\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:15:28 - INFO - __main__ -   28712\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 28700\n",
            "loss: 0.5920100212097168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:16:33 - INFO - __main__ -   28812\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 28800\n",
            "loss: 0.6652227640151978\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:17:39 - INFO - __main__ -   28912\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 28900\n",
            "loss: 0.9383276104927063\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:18:45 - INFO - __main__ -   29012\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 29000\n",
            "loss: 0.7477216720581055\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:19:50 - INFO - __main__ -   29112\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 29100\n",
            "loss: 0.462690144777298\n",
            "floursugarbaking powderaltcinnamonnutmegcardamomunsalted butterpumpkin pureemilkCheavy creamowdered sugar<s> <RECIPE_START> <INPUT_START> flour <NEXT_INPUT> sugar <NEXT_INPUT> baking powder <NEXT_INPUT> salt <NEXT_INPUT> cinnamon <NEXT_INPUT> nutmeg <NEXT_INPUT> cardamom <NEXT_INPUT> unsalted butter <NEXT_INPUT> pumpkin puree <NEXT_INPUT> milk <NEXT_INPUT> Cinnamon <NEXT_INPUT> pumpkin puree <NEXT_INPUT> heavy cream <NEXT_INPUT> powdered sugar <NEXT_INPUT> cinnamon <NEXT_INPUT> salt <INPUT_END> <INGR_START> Spiced Pumpkin Scones: <NEXT_INGR> 2A1/2 cups (320 g) all-purpose flour <NEXT_INGR> A1/4 cup (50 g) sugar, plus 1 teaspoon to sprinkle on top of the scones <NEXT_INGR> 1 tablespoon baking powder <NEXT_INGR> A teaspoon salt <NEXT_INGR> A1/2 teaspoon cinnamon <NEXT_INGR> A1/4 teaspoon nutmeg <NEXT_INGR> a\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:20:56 - INFO - __main__ -   29213\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 29200\n",
            "loss: 0.6937569975852966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:22:01 - INFO - __main__ -   29313\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 29300\n",
            "loss: 0.4559898376464844\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:23:06 - INFO - __main__ -   29413\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 29400\n",
            "loss: 0.5499184727668762\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:24:11 - INFO - __main__ -   29513\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 29500\n",
            "loss: 0.7768797874450684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:25:17 - INFO - __main__ -   29613\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 29600\n",
            "loss: 0.4530963897705078\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:26:23 - INFO - __main__ -   29713\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 29700\n",
            "loss: 0.5034492611885071\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:27:28 - INFO - __main__ -   29813\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 29800\n",
            "loss: 0.37067776918411255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:28:33 - INFO - __main__ -   29913\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 29900\n",
            "loss: 0.6319988965988159\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:29:38 - INFO - __main__ -   30013\n",
            "05/31/2021 14:29:38 - INFO - __main__ -   Saving model checkpoint to ./gdrive/MyDrive/COMP0087/model_checkpoint_0526\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 30000\n",
            "loss: 0.5323946475982666\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:30:51 - INFO - __main__ -   30113\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 30100\n",
            "loss: 0.6015163064002991\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:31:56 - INFO - __main__ -   30213\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 30200\n",
            "loss: 0.4344727694988251\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:33:00 - INFO - __main__ -   30313\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 30300\n",
            "loss: 0.6223418116569519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:34:05 - INFO - __main__ -   30413\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 30400\n",
            "loss: 0.6441720724105835\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:35:10 - INFO - __main__ -   30513\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 30500\n",
            "loss: 0.4521418809890747\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:36:15 - INFO - __main__ -   30613\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 30600\n",
            "loss: 0.7201566100120544\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:37:20 - INFO - __main__ -   30713\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 30700\n",
            "loss: 0.41541460156440735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:38:25 - INFO - __main__ -   30813\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 30800\n",
            "loss: 0.47685322165489197\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:39:31 - INFO - __main__ -   30913\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 30900\n",
            "loss: 0.4132797122001648\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:40:37 - INFO - __main__ -   31013\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 31000\n",
            "loss: 0.654907763004303\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:41:43 - INFO - __main__ -   31113\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 31100\n",
            "loss: 0.4367073178291321\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:42:49 - INFO - __main__ -   31213\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 31200\n",
            "loss: 0.6098449230194092\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:43:54 - INFO - __main__ -   31313\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 31300\n",
            "loss: 0.7753977179527283\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:45:00 - INFO - __main__ -   31413\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 31400\n",
            "loss: 0.565274715423584\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:46:06 - INFO - __main__ -   31513\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 31500\n",
            "loss: 0.37625792622566223\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:47:12 - INFO - __main__ -   31613\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 31600\n",
            "loss: 0.8706580400466919\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:48:17 - INFO - __main__ -   31713\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 31700\n",
            "loss: 0.4851242005825043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:49:22 - INFO - __main__ -   31813\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 31800\n",
            "loss: 0.566358208656311\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:50:28 - INFO - __main__ -   31913\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 31900\n",
            "loss: 0.44603046774864197\n",
            "size overflow\n",
            "sweet potatoescornblack beansbutteryellow onionpepperslight cream cheese soursaltcilantroshredded<s> <RECIPE_START> <INPUT_START> sweet potatoes <NEXT_INPUT> corn <NEXT_INPUT> black beans <NEXT_INPUT> butter <NEXT_INPUT> yellow onion <NEXT_INPUT> peppers <NEXT_INPUT> light cream cheese <NEXT_INPUT> light sour cream <NEXT_INPUT> salt <NEXT_INPUT> cilantro <NEXT_INPUT> shredded cheese <INPUT_END> <INGR_START> 3 medium sweet potatoes <NEXT_INGR> 1 can corn, rinsed and patted dry <NEXT_INGR> 1 can black beans, rinsed and drained <NEXT_INGR> 1 tablespoon butter <NEXT_INGR> 1/2 yellow onion, chopped <NEXT_INGR> 2-4 INDIVIDUAL chipotle peppers in adobo sauce, minced or pureed (not the whole can!) <NEXT_INGR> 1 ounce light cream cheese <NEXT_INGR> 1/4 cup light sour cream <NEXT_INGR> 1 teaspoon salt (+ more to taste) <NEXT_INGR> 1/2 cup cilantro, roughly chopped <NEXT_INGR> 6 tablespoons shredded cheese (Pepperjack, Cheddar, Colby Jack\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:51:36 - INFO - __main__ -   32014\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 32000\n",
            "loss: 0.48418542742729187\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:52:42 - INFO - __main__ -   32114\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 32100\n",
            "loss: 0.4691307246685028\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:53:48 - INFO - __main__ -   32214\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 32200\n",
            "loss: 0.7104600667953491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:54:53 - INFO - __main__ -   32314\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 32300\n",
            "loss: 0.4915127754211426\n",
            "size overflow\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:55:59 - INFO - __main__ -   32414\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 32400\n",
            "loss: 0.4690662920475006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:57:05 - INFO - __main__ -   32514\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 32500\n",
            "loss: 0.6020536422729492\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:58:11 - INFO - __main__ -   32614\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 32600\n",
            "loss: 0.5807685256004333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 14:59:17 - INFO - __main__ -   32714\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 32700\n",
            "loss: 0.6646521091461182\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:00:23 - INFO - __main__ -   32814\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 32800\n",
            "loss: 0.6360026597976685\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:01:28 - INFO - __main__ -   32914\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 32900\n",
            "loss: 0.4494001865386963\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:02:34 - INFO - __main__ -   33014\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 33000\n",
            "loss: 0.5655779838562012\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:03:40 - INFO - __main__ -   33114\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 33100\n",
            "loss: 0.44146615266799927\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:04:47 - INFO - __main__ -   33214\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 33200\n",
            "loss: 0.40522971749305725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:05:52 - INFO - __main__ -   33314\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 33300\n",
            "loss: 0.4990622401237488\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:06:58 - INFO - __main__ -   33414\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 33400\n",
            "loss: 0.7181434035301208\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:08:04 - INFO - __main__ -   33514\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 33500\n",
            "loss: 0.4894809424877167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:09:08 - INFO - __main__ -   33614\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 33600\n",
            "loss: 0.4567408561706543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:10:14 - INFO - __main__ -   33714\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 33700\n",
            "loss: 0.38205283880233765\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:11:20 - INFO - __main__ -   33814\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 33800\n",
            "loss: 0.5774858593940735\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:12:26 - INFO - __main__ -   33914\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 33900\n",
            "loss: 0.6239466071128845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:13:32 - INFO - __main__ -   34014\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 34000\n",
            "loss: 0.4770817160606384\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:14:38 - INFO - __main__ -   34114\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 34100\n",
            "loss: 0.9529105424880981\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:15:43 - INFO - __main__ -   34214\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 34200\n",
            "loss: 0.6044279932975769\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:16:48 - INFO - __main__ -   34314\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 34300\n",
            "loss: 0.40906473994255066\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:17:54 - INFO - __main__ -   34414\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 34400\n",
            "loss: 0.3340328335762024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:18:59 - INFO - __main__ -   34514\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 34500\n",
            "loss: 0.5631040334701538\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:20:05 - INFO - __main__ -   34614\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 34600\n",
            "loss: 0.5754680037498474\n",
            "size overflow\n",
            "size overflow\n",
            "sweet potatoesgranulated sugarbrownmilkeggsunsalted butterground cinnamonvanillaosher saltpecansflouralt<s> <RECIPE_START> <INPUT_START> sweet potatoes <NEXT_INPUT> granulated sugar <NEXT_INPUT> brown sugar <NEXT_INPUT> milk <NEXT_INPUT> eggs <NEXT_INPUT> unsalted butter <NEXT_INPUT> ground cinnamon <NEXT_INPUT> vanilla <NEXT_INPUT> kosher salt <NEXT_INPUT> pecans <NEXT_INPUT> flour <NEXT_INPUT> brown sugar <NEXT_INPUT> unsalted butter <NEXT_INPUT> ground cinnamon <NEXT_INPUT> salt <INPUT_END> <INGR_START> (Serves 10-12) <NEXT_INGR> 2 lbs. sweet potatoes, 5-6 medium sweet potatoes <NEXT_INGR> 2/3 cup granulated sugar <NEXT_INGR> 1/2 cup brown sugar <NEXT_INGR> 1/4 cup evaporated milk (low-fat optional) <NEXT_INGR> 2 eggs <NEXT_INGR> </s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:21:11 - INFO - __main__ -   34715\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 34700\n",
            "loss: 0.6589650511741638\n",
            "chicken breastsolive oilgarlicred peppermarinara sauceParmesan cheesemozzarellacroutons<s> <RECIPE_START> <INPUT_START> chicken breasts <NEXT_INPUT> olive oil <NEXT_INPUT> garlic <NEXT_INPUT> red pepper <NEXT_INPUT> marinara sauce <NEXT_INPUT> Parmesan cheese <NEXT_INPUT> mozzarella cheese <NEXT_INPUT> croutons <INPUT_END> <INGR_START> chicken breasts (however many you want/need) <NEXT_INGR> olive oil (just one dollop) <NEXT_INGR> minced garlic (a smattering\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:22:17 - INFO - __main__ -   34816\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 34800\n",
            "loss: 0.4962981939315796\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:23:23 - INFO - __main__ -   34916\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 34900\n",
            "loss: 0.6406587362289429\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:24:29 - INFO - __main__ -   35016\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 35000\n",
            "loss: 0.6398648619651794\n",
            "shorteningbuttergranulated sugarOCreameggsvanilla the eggsflourbaking powderalt<s> <RECIPE_START> <INPUT_START> shortening <NEXT_INPUT> butter <NEXT_INPUT> granulated sugar <NEXT_INPUT> O <NEXT_INPUT> Cream <NEXT_INPUT> eggs <NEXT_INPUT> vanilla <NEXT_INPUT> Cream the eggs <NEXT_INPUT> flour <NEXT_INPUT> baking powder <NEXT_INPUT> salt <INPUT_END> <INGR_START> 1/2 cup shortening <NEXT_INGR> 1/4 cup butter <NEXT_INGR> 1/2 cup granulated sugar <NEXT_INGR> 1 - 3oz. pkg of Jell-O ( I used raspberry, orange, lemon and lime flavors\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:25:35 - INFO - __main__ -   35117\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 35100\n",
            "loss: 0.35649898648262024\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:26:42 - INFO - __main__ -   35217\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 35200\n",
            "loss: 0.36180439591407776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:27:48 - INFO - __main__ -   35317\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 35300\n",
            "loss: 0.6153317093849182\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:28:54 - INFO - __main__ -   35417\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 35400\n",
            "loss: 0.5377979278564453\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:30:00 - INFO - __main__ -   35517\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 35500\n",
            "loss: 0.5043755173683167\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:31:06 - INFO - __main__ -   35617\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 35600\n",
            "loss: 0.34999942779541016\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:32:13 - INFO - __main__ -   35717\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 35700\n",
            "loss: 0.49912092089653015\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:33:18 - INFO - __main__ -   35817\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 35800\n",
            "loss: 0.49644893407821655\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:34:24 - INFO - __main__ -   35917\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "size overflow\n",
            "step 35900\n",
            "loss: 0.5350974798202515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:35:30 - INFO - __main__ -   36017\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 36000\n",
            "loss: 0.7133157253265381\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:36:35 - INFO - __main__ -   36117\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 36100\n",
            "loss: 0.46562471985816956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:37:41 - INFO - __main__ -   36217\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 36200\n",
            "loss: 0.6005870699882507\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:38:47 - INFO - __main__ -   36317\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 36300\n",
            "loss: 0.3716821074485779\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:39:53 - INFO - __main__ -   36417\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 36400\n",
            "loss: 0.36646735668182373\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:40:59 - INFO - __main__ -   36517\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 36500\n",
            "loss: 0.8552607297897339\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:42:05 - INFO - __main__ -   36617\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 36600\n",
            "loss: 0.40320977568626404\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:43:11 - INFO - __main__ -   36717\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 36700\n",
            "loss: 0.40893715620040894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:44:16 - INFO - __main__ -   36817\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 36800\n",
            "loss: 0.6541656255722046\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:45:22 - INFO - __main__ -   36917\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 36900\n",
            "loss: 0.3853752017021179\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:46:27 - INFO - __main__ -   37017\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 37000\n",
            "loss: 0.4524328112602234\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:47:33 - INFO - __main__ -   37117\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 37100\n",
            "loss: 0.4540431499481201\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:48:39 - INFO - __main__ -   37217\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 37200\n",
            "loss: 0.7150264978408813\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:49:44 - INFO - __main__ -   37317\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "size overflow\n",
            "step 37300\n",
            "loss: 0.5367235541343689\n",
            "Chicken Breastsfresh green beansred potatoeslemon juiceolive oiloreganosaltpepperonion powdergarlic<s> <RECIPE_START> <INPUT_START> Chicken Breasts <NEXT_INPUT> fresh green beans <NEXT_INPUT> red potatoes <NEXT_INPUT> lemon juice <NEXT_INPUT> olive oil <NEXT_INPUT> oregano <NEXT_INPUT> salt <NEXT_INPUT> pepper <NEXT_INPUT> onion powder <NEXT_INPUT> garlic <INPUT_END> <INGR_START> 1.5- 2lbs Boneless Skinless Chicken Breasts <NEXT_INGR> A1/2 lb. fresh green beans, trimmed (about 2.5 cups) <NEXT_INGR> 1.25 lb. diced red potatoes (about 4 cups) <NEXT_INGR> a\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:50:50 - INFO - __main__ -   37418\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 37400\n",
            "loss: 0.46754127740859985\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:51:56 - INFO - __main__ -   37518\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 37500\n",
            "loss: 0.4379802346229553\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:53:01 - INFO - __main__ -   37618\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 37600\n",
            "loss: 0.3359871506690979\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:54:08 - INFO - __main__ -   37718\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 37700\n",
            "loss: 0.6146595478057861\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:55:14 - INFO - __main__ -   37818\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 37800\n",
            "loss: 0.4794028699398041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:56:20 - INFO - __main__ -   37918\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 37900\n",
            "loss: 0.37905797362327576\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:57:26 - INFO - __main__ -   38018\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 38000\n",
            "loss: 0.5111709833145142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:58:31 - INFO - __main__ -   38118\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 38100\n",
            "loss: 0.5621689558029175\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 15:59:37 - INFO - __main__ -   38218\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 38200\n",
            "loss: 0.5759650468826294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:00:43 - INFO - __main__ -   38318\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 38300\n",
            "loss: 0.7254220247268677\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:01:50 - INFO - __main__ -   38418\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 38400\n",
            "loss: 0.6167506575584412\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:02:56 - INFO - __main__ -   38518\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 38500\n",
            "loss: 0.6149173378944397\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:04:01 - INFO - __main__ -   38618\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 38600\n",
            "loss: 0.48762574791908264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:05:06 - INFO - __main__ -   38718\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 38700\n",
            "loss: 0.4437303841114044\n",
            "chickpeassaltbaking sodavanillanut butteralmond milkbrown sugaroatsocolate chips<s> <RECIPE_START> <INPUT_START> chickpeas <NEXT_INPUT> salt <NEXT_INPUT> baking soda <NEXT_INPUT> vanilla <NEXT_INPUT> nut butter <NEXT_INPUT> almond milk <NEXT_INPUT> brown sugar <NEXT_INPUT> oats <NEXT_INPUT> chocolate chips <INPUT_END> <INGR_START> 1 (15 oz) can of chickpeas, drained and rinsed <NEXT_INGR> 1/8 tsp salt <NEXT_INGR> 1/8 tsp baking soda <NEXT_INGR> 2 tsp pure vanilla extract <NEXT_INGR> 1/4 cup nut butter (I use natural peanut butter) <NEXT_INGR> 1/4 cup almond milk (or soy, or rice\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:06:12 - INFO - __main__ -   38819\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "size overflow\n",
            "step 38800\n",
            "loss: 0.7431514859199524\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:07:17 - INFO - __main__ -   38919\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 38900\n",
            "loss: 0.3530384600162506\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:08:22 - INFO - __main__ -   39019\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 39000\n",
            "loss: 0.5042753219604492\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:09:28 - INFO - __main__ -   39119\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 39100\n",
            "loss: 0.32361775636672974\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:10:34 - INFO - __main__ -   39219\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 39200\n",
            "loss: 0.3986857533454895\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:11:40 - INFO - __main__ -   39319\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 39300\n",
            "loss: 0.6185387372970581\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:12:46 - INFO - __main__ -   39419\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 39400\n",
            "loss: 0.581285297870636\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:13:53 - INFO - __main__ -   39519\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 39500\n",
            "loss: 0.6853084564208984\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:14:59 - INFO - __main__ -   39619\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 39600\n",
            "loss: 0.6552368402481079\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:16:04 - INFO - __main__ -   39719\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 39700\n",
            "loss: 0.494873970746994\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:17:10 - INFO - __main__ -   39819\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "size overflow\n",
            "step 39800\n",
            "loss: 0.5766085982322693\n",
            "Green Beanspotatoescream cheesemilkbuttersaltpeppe<s> <RECIPE_START> <INPUT_START> Green Beans <NEXT_INPUT> potatoes <NEXT_INPUT> cream cheese <NEXT_INPUT> milk <NEXT_INPUT> butter <NEXT_INPUT> salt <NEXT_INPUT> peppe <INPUT_END> <INGR_START> 1 Pound Green Beans (fresh or frozen) <NEXT_INGR> 6-8 small new potatoes <NEXT_INGR> 4 ounces cream cheese (1/2 of a brick) <NEXT_INGR> A1/2 cup milk <NEXT_INGR> A1/4 cup butter (1/2 of a stick) <NEXT_INGR> A1/4 teaspoon salt <NEXT_INGR> a\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:18:14 - INFO - __main__ -   39920\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 39900\n",
            "loss: 0.45595771074295044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:19:20 - INFO - __main__ -   40020\n",
            "05/31/2021 16:19:20 - INFO - __main__ -   Saving model checkpoint to ./gdrive/MyDrive/COMP0087/model_checkpoint_0526\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 40000\n",
            "loss: 0.6326603293418884\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:20:33 - INFO - __main__ -   40120\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 40100\n",
            "loss: 0.43292224407196045\n",
            "BEANSJamon serranoTBLyellow oniongarlicwhite beansextra-virgin olive oiltomatoessugarred chiliesaffron threads winealtshrimpflatleaf parsleylemon<s> <RECIPE_START> <INPUT_START> BEANS <NEXT_INPUT> Jamon serrano <NEXT_INPUT> TBL <NEXT_INPUT> yellow onion <NEXT_INPUT> garlic <NEXT_INPUT> white beans <NEXT_INPUT> yellow onion <NEXT_INPUT> garlic <NEXT_INPUT> extra-virgin olive oil <NEXT_INPUT> tomatoes <NEXT_INPUT> sugar <NEXT_INPUT> red chilies <NEXT_INPUT> saffron threads <NEXT_INPUT> white wine <NEXT_INPUT> salt <NEXT_INPUT> shrimp <NEXT_INPUT> flat-leaf parsley <NEXT_INPUT> lemon <INPUT_END> <INGR_START> FOR THE BEANS: <NEXT_INGR> 1/4 cup diced Jamon serrano (dry-cured Spanish ham) or Italian prosciutto <NEXT_INGR> 2 TBL extra-virgin olive oil <NEXT_INGR> 1 medium yellow onion, chopped <NEXT_INGR> 4 garlic cloves, chopped <NEXT_INGR> 1 cup dried white beans, soaked overnight and drained (TIP: For fresher dried beans buy them at food purveyors where they have a large turnover\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:21:39 - INFO - __main__ -   40221\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 40200\n",
            "loss: 0.678227961063385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:22:45 - INFO - __main__ -   40321\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 40300\n",
            "loss: 0.4371790587902069\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:23:50 - INFO - __main__ -   40421\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 40400\n",
            "loss: 0.5324379801750183\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:24:55 - INFO - __main__ -   40521\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 40500\n",
            "loss: 0.5434094667434692\n",
            "No-Boil Lasagnacontainersmozzarella cheeseParmesan Cheeseeggsspaghetti sauceMushroomground beeflas noodles<s> <RECIPE_START> <INPUT_START> No-Boil Lasagna <NEXT_INPUT> containers <NEXT_INPUT> mozzarella cheese <NEXT_INPUT> Parmesan Cheese <NEXT_INPUT> eggs <NEXT_INPUT> spaghetti sauce <NEXT_INPUT> Mushroom <NEXT_INPUT> ground beef <NEXT_INPUT> lasagna noodles <INPUT_END> <INGR_START> No-Boil Lasagna <NEXT_INGR> 2 15-ounce containers of Ricotta Cheese (Cottage Cheese) <NEXT_INGR> 2 cups mozzarella cheese, shredded (Swiss Cheese) <NEXT_INGR> 1/2 cup Parmesan Cheese Grated <NEXT_INGR> 2 eggs (No eggs) <NEXT_INGR> 2- 2-pound jars meaty spaghetti sauce <NEXT_INGR> 2\tjars Classico Spaghetti Sauce with Mushroom & Ripe Olives <NEXT_INGR> Add to spaghetti sauce\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:26:01 - INFO - __main__ -   40622\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 40600\n",
            "loss: 0.6062660813331604\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:27:07 - INFO - __main__ -   40722\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 40700\n",
            "loss: 0.3739793002605438\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:28:13 - INFO - __main__ -   40822\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 40800\n",
            "loss: 0.37782880663871765\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:29:19 - INFO - __main__ -   40922\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 40900\n",
            "loss: 0.43290480971336365\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:30:25 - INFO - __main__ -   41022\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 41000\n",
            "loss: 0.5213696956634521\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:31:30 - INFO - __main__ -   41122\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 41100\n",
            "loss: 0.38516008853912354\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:32:36 - INFO - __main__ -   41222\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 41200\n",
            "loss: 0.5803726315498352\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:33:41 - INFO - __main__ -   41322\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 41300\n",
            "loss: 0.7455587387084961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:34:46 - INFO - __main__ -   41422\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 41400\n",
            "loss: 0.4713149666786194\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:35:51 - INFO - __main__ -   41522\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 41500\n",
            "loss: 0.7873806357383728\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:36:56 - INFO - __main__ -   41622\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 41600\n",
            "loss: 0.4879131317138672\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:38:02 - INFO - __main__ -   41722\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 41700\n",
            "loss: 0.7106889486312866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:39:08 - INFO - __main__ -   41822\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 41800\n",
            "loss: 0.5237404108047485\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:40:14 - INFO - __main__ -   41922\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 41900\n",
            "loss: 0.5924328565597534\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:41:19 - INFO - __main__ -   42022\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 42000\n",
            "loss: 0.3344362676143646\n",
            "Green sauceIngredientsfresh basilextra-virgin olive oilgarliccapersanchovyred pepper wine vinegarmustardkosherFreshly ground<s> <RECIPE_START> <INPUT_START> Green sauce <NEXT_INPUT> Ingredients <NEXT_INPUT> fresh basil <NEXT_INPUT> extra-virgin olive oil <NEXT_INPUT> garlic <NEXT_INPUT> capers <NEXT_INPUT> anchovy <NEXT_INPUT> red pepper <NEXT_INPUT> red wine vinegar <NEXT_INPUT> mustard <NEXT_INPUT> kosher <NEXT_INPUT> Freshly ground pepper <INPUT_END> <INGR_START> Green sauce <NEXT_INGR> Ingredients:\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:42:25 - INFO - __main__ -   42123\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 42100\n",
            "loss: 0.5036484599113464\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:43:31 - INFO - __main__ -   42223\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 42200\n",
            "loss: 0.6386656761169434\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:44:37 - INFO - __main__ -   42323\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 42300\n",
            "loss: 0.5559412240982056\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:45:42 - INFO - __main__ -   42423\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 42400\n",
            "loss: 0.5935866236686707\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:46:47 - INFO - __main__ -   42523\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 42500\n",
            "loss: 0.5578925013542175\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:47:53 - INFO - __main__ -   42623\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 42600\n",
            "loss: 0.7192521691322327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:48:58 - INFO - __main__ -   42723\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 42700\n",
            "loss: 0.37174391746520996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:50:04 - INFO - __main__ -   42823\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 42800\n",
            "loss: 0.5979136228561401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:51:09 - INFO - __main__ -   42923\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 42900\n",
            "loss: 0.48555058240890503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:52:15 - INFO - __main__ -   43023\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 43000\n",
            "loss: 0.5201611518859863\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:53:19 - INFO - __main__ -   43123\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "size overflow\n",
            "step 43100\n",
            "loss: 0.561063289642334\n",
            "size overflow\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:54:24 - INFO - __main__ -   43223\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 43200\n",
            "loss: 0.27778294682502747\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:55:29 - INFO - __main__ -   43323\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 43300\n",
            "loss: 0.3244798481464386\n",
            "size overflow\n",
            "size overflow\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:56:35 - INFO - __main__ -   43423\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 43400\n",
            "loss: 0.4672005772590637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:57:40 - INFO - __main__ -   43523\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 43500\n",
            "loss: 0.43858563899993896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:58:46 - INFO - __main__ -   43623\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 43600\n",
            "loss: 0.5569567680358887\n",
            "size overflow\n",
            "chicken thighpotatoescarrotredzucchinibutton mushroomsilliesgreen currylight coconut milksnow peasfrozen babycashewsBrown riceCoriander Sprigs<s> <RECIPE_START> <INPUT_START> chicken thigh <NEXT_INPUT> potatoes <NEXT_INPUT> carrot <NEXT_INPUT> red <NEXT_INPUT> zucchini <NEXT_INPUT> button mushrooms <NEXT_INPUT> chillies <NEXT_INPUT> green curry <NEXT_INPUT> light coconut milk <NEXT_INPUT> snow peas <NEXT_INPUT> frozen baby peas <NEXT_INPUT> cashews <NEXT_INPUT> Brown rice <NEXT_INPUT> Coriander Sprigs <INPUT_END> <INGR_START> 1 kg chicken thigh fillets, cut into chunky pieces <NEXT_INGR> 6 medium potatoes, peeled and quartered <NEXT_INGR> 1 large carrot, scrubbed and sliced <NEXT_INGR> 1 red capsicum, sliced <NEXT_INGR> 1 large zucchini, sliced <NEXT_INGR> 1 cup button mushrooms <NEXT_INGR> 2-3 chillies, seeded and thinly sliced <NEXT_INGR> 1/3 cup Thai green curry paste, <NEXT_INGR> 2 x 400 ml cans light coconut milk <NEXT_INGR> 11/2 cups snow peas, topped and tailed <NEXT_INGR> 1 cup frozen baby peas <NEXT_INGR> 1/3 cup raw cashews <NEXT_INGR> To serve\n",
            "</s> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END> <RECIPE_END>\n",
            "An error happens, break\n",
            "An error in this batch, break\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 16:59:51 - INFO - __main__ -   43724\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 43700\n",
            "loss: 0.5292638540267944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-0a577ff67546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_cache_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmybart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBartForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmybart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-ee0f0f0ed83d>\u001b[0m in \u001b[0;36mtrain_seq\u001b[0;34m(args, train_dataset, model, tokenizer)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;31m# break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'encoder_batch' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXzoNYpwlYz0"
      },
      "source": [
        "## evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpKZFikiiyaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b69d5ee-4225-4ae7-da7f-2c3c39dfd778"
      },
      "source": [
        "model = mybart\n",
        "model_class = BartForConditionalGeneration\n",
        "tokenizer_class = BartTokenizer\n",
        "save_model(args,model,tokenizer,model_class,tokenizer_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/31/2021 18:13:16 - INFO - __main__ -   Saving model checkpoint to ./gdrive/MyDrive/COMP0087/model_checkpoint_0526\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXpDix2yd3lB"
      },
      "source": [
        "# generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX9UqNILeAvK"
      },
      "source": [
        "#model loading\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"mbien/recipenlg\")\n",
        "#model = AutoModelWithLMHead.from_pretrained(\"mbien/recipenlg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPi6Xwt5GrvO",
        "outputId": "89af604d-4d8c-47b2-b123-c3255df9b9a5"
      },
      "source": [
        "raw_text = 'tomato,egg'\n",
        "prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "tokenizer.encode(prepared_input)[0:-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 50273, 50275, 33063, 3938, 50277, 38299]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofrfJDXw_iqf"
      },
      "source": [
        "def set_seed(args):\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(model, length, raw_text, tokenizer, num_samples=1, temperature=1, top_k=0, top_p=0.0, device='cpu'):\n",
        "    end_token = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "\n",
        "    context_tokens = tokenizer.encode(prepared_input)[0:-1]\n",
        "    raw_text_tokens = tokenizer.encode(raw_text)[0:-1]\n",
        "\n",
        "    context = torch.tensor(context_tokens, dtype=torch.long, device=device)\n",
        "    raw_text_tokens = torch.tensor(raw_text_tokens, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    raw_text_tokens = raw_text_tokens.unsqueeze(0).repeat(num_samples, 1)\n",
        "    # print(raw_text_tokens)\n",
        "    # print(context)\n",
        "    \n",
        "    # genereated token\n",
        "    start_token =  tokenizer.convert_tokens_to_ids([\"<RECIPE_START>\"])[0]\n",
        "    # generated = torch.tensor(start_token, dtype=torch.long, device=device).reshape(1).unsqueeze(0)\n",
        "    generated = context\n",
        "    # print(generated.shape)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            # inputs = {'input_ids': generated}\n",
        "            outputs = model(input_ids = raw_text_tokens, decoder_input_ids = generated)\n",
        "            # outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
        "            # print(outputs)\n",
        "            next_token_logits = outputs[\"logits\"][0, -1, :] / temperature\n",
        "            \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
        "            #print(next_token)\n",
        "            if next_token.item() == end_token:\n",
        "                break\n",
        "    return generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6VNmuOzd4vW",
        "outputId": "22b19788-5320-4ee8-90af-96261bcc3197"
      },
      "source": [
        "# raw_text = args.prompt if args.prompt else input(\"Comma-separated ingredients, semicolon to close the list >>> \")\n",
        "raw_text = 'tomato, potato, beef;'\n",
        "prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "context_tokens = tokenizer.encode(prepared_input)[0:-1]\n",
        "\n",
        "out = sample_sequence(\n",
        "            model=mybart,\n",
        "            raw_text=raw_text,\n",
        "            tokenizer=tokenizer,\n",
        "            length=512,\n",
        "            device = args.device\n",
        "        )\n",
        "out = out[0, len(context_tokens):].tolist()\n",
        "text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n",
        "if \"<RECIPE_END>\" not in text:\n",
        "  print(text)\n",
        "  print(\"Failed to generate, recipe's too long\")\n",
        "  full_text = prepared_input + text\n",
        "  print(full_text)\n",
        "else:\n",
        "  full_text = prepared_input + text\n",
        "  print(prepared_input)\n",
        "  print(full_text)\n",
        "  markdown = re.sub(\"<RECIPE_(START|END)>\", \"\", full_text)\n",
        "  recipe_n_title = markdown.split(\"<TITLE_START>\")\n",
        "  title = \"# \" + recipe_n_title[1].replace(\"<TITLE_END>\", \"\") + \" #\\n\"\n",
        "  markdown = recipe_n_title[0].replace(\"<INPUT_START>\", \"## Input ingredients ##\\n`\").replace(\"<INPUT_END>\", \"`\\n\")\n",
        "  markdown = markdown.replace(\"<NEXT_INPUT>\", \"`\\n`\").replace(\"<INGR_START>\", \"## Ingredients ##\\n* \").replace(\"<NEXT_INGR>\", \"\\n* \").replace(\"<INGR_END>\", \"\\n\")\n",
        "  markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n1) \").replace(\"<NEXT_INSTR>\", \"\\n1) \").replace(\"<INSTR_END>\", \"\\n\")\n",
        "  markdown = re.sub(\"$ +#\", \"#\", markdown)\n",
        "  markdown = re.sub(\"( +`|` +)\", \"`\", markdown)\n",
        "  print(title+markdown)#\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " <RECIPE_START> <INPUT_START> tomato <NEXT_INPUT>  potato <NEXT_INPUT>  beef <INPUT_END>\n",
            " <RECIPE_START> <INPUT_START> tomato <NEXT_INPUT>  potato <NEXT_INPUT>  beef <INPUT_END><INGR_START> 1 veck's can hen <NEXT_INGR> tomato <NEXT_INGR> 1/2 potato, vegetarian <NEXT_INGR>, beef; beef;salt to taste, garlic, mustard and shape seasoning to taste <INGR_END> <INSTR_START> Prepare a burger with oil and flour about a stehi <NEXT_INGR> salt to taste, tomato, potato, rotato. <NEXT_INSTR> men g kitchen-tops. <NEXT_INSTR> Add the tomato and beef;salt to taste. <NEXT_INSTR> Heat finely in 310F bacon grease just until it's just hot. <NEXT_INSTR> Pour the tomato mixture on and let cool. <NEXT_INSTR> It's good until you eat the rolls :). <NEXT_INSTR> Pat flt holes on the toasted rye with a spoon and cover with the bottom of the can of bread to rest. <NEXT_INSTR> Heat the oil in a large frying pan and add the beef; sprinkle it over the top, and put the pan into the oven. <NEXT_INSTR> You might have to spoon some tomato and potato mixture onto the top of the tomato. <NEXT_INSTR> This technique makes the chunk of mix the tomato and potato top none. <INSTR_END> <TITLE_START> Donat Bone Cheet (For Tomatoes!) <TITLE_END> <RECIPE_END>\n",
            "#  Donat Bone Cheet (For Tomatoes!)   #\n",
            "  ## Input ingredients ##\n",
            "`tomato`\n",
            "`potato`\n",
            "`beef`\n",
            "## Ingredients ##\n",
            "*  1 veck's can hen \n",
            "*  tomato \n",
            "*  1/2 potato, vegetarian \n",
            "* , beef; beef;salt to taste, garlic, mustard and shape seasoning to taste \n",
            " ## Instructions ##\n",
            "1)  Prepare a burger with oil and flour about a stehi \n",
            "*  salt to taste, tomato, potato, rotato. \n",
            "1)  men g kitchen-tops. \n",
            "1)  Add the tomato and beef;salt to taste. \n",
            "1)  Heat finely in 310F bacon grease just until it's just hot. \n",
            "1)  Pour the tomato mixture on and let cool. \n",
            "1)  It's good until you eat the rolls :). \n",
            "1)  Pat flt holes on the toasted rye with a spoon and cover with the bottom of the can of bread to rest. \n",
            "1)  Heat the oil in a large frying pan and add the beef; sprinkle it over the top, and put the pan into the oven. \n",
            "1)  You might have to spoon some tomato and potato mixture onto the top of the tomato. \n",
            "1)  This technique makes the chunk of mix the tomato and potato top none. \n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KoPaCB2vVej"
      },
      "source": [
        "# gold standard\n",
        "#df_eval = gold_st\n",
        "model = mybart\n",
        "model.to(args.device)\n",
        "test_input = test.reset_index()\n",
        "df_eval_ner = test_input[\"NER\"]  #NER\n",
        "df_eval_dir = test_input[\"directions\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY78AVj4vcEU"
      },
      "source": [
        "def get_raw_input(NER):\n",
        "    test_str = NER\n",
        "    test_str = test_str.replace(\"[\",\"\")\n",
        "    test_str = test_str.replace(\"]\",\"\")\n",
        "    test_str = test_str.replace(\"\\\"\",\"\")\n",
        "\n",
        "    return test_str\n",
        "\n",
        "def get_instr(markdown):\n",
        "  markdown = markdown.split(\"\\n\")\n",
        "  if ' ## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index(' ## Instructions ##')\n",
        "    \n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "\n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "    \n",
        "    return output\n",
        "\n",
        "  elif '## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index('## Instructions ##')\n",
        "\n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "        \n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "\n",
        "    return output\n",
        "    \n",
        "  else:\n",
        "    return [\"failed to generate\"]\n",
        "\n",
        "\n",
        "def generate_recipe(raw_text):\n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "    context_tokens = tokenizer.encode(prepared_input)[0:-1]\n",
        "    out = sample_sequence(\n",
        "                model=model,\n",
        "                raw_text=raw_text,\n",
        "                tokenizer=tokenizer,\n",
        "                length=512,\n",
        "                device = args.device\n",
        "            )\n",
        "    out = out[0, len(context_tokens):].tolist()\n",
        "    text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n",
        "    if \"<RECIPE_END>\" not in text:\n",
        "      # print(text)\n",
        "      print(\"Failed to generate, recipe's too long\")\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      return generate_recipe(raw_text)\n",
        "    else:\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      markdown = re.sub(\"<RECIPE_(START|END)>\", \"\", full_text)\n",
        "      recipe_n_title = markdown.split(\"<TITLE_START>\")\n",
        "      if len(recipe_n_title)<=1:\n",
        "        return generate_recipe(raw_text)\n",
        "      title = \"# \" + recipe_n_title[1].replace(\"<TITLE_END>\", \"\") + \" #\\n\"\n",
        "      markdown = recipe_n_title[0].replace(\"<INPUT_START>\", \"## Input ingredients ##\\n`\").replace(\"<INPUT_END>\", \"`\\n\")\n",
        "      markdown = markdown.replace(\"<NEXT_INPUT>\", \"`\\n`\").replace(\"<INGR_START>\", \"## Ingredients ##\\n* \").replace(\"<NEXT_INGR>\", \"\\n* \").replace(\"<INGR_END>\", \"\\n\")\n",
        "      #markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\\\"\").replace(\"<NEXT_INSTR>\", \"\\n\\\"\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\").replace(\"<NEXT_INSTR>\", \"\\n\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = re.sub(\"$ +#\", \"#\", markdown)\n",
        "      markdown = re.sub(\"( +`|` +)\", \"`\", markdown)\n",
        "  \n",
        "      return markdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h84dStu7vegL",
        "outputId": "785396bd-d40a-4b19-a180-9989704e12e2"
      },
      "source": [
        "df_eval_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     [\"In a clad skillet (don't use cast iron becau...\n",
              "1     [\"Cook spaghetti according to package directio...\n",
              "2     [\"Line pan with 1/2 crushed wafers.\", \"Cream p...\n",
              "3     [\"Boil tongue in salt water.\", \"Tongue is done...\n",
              "4     [\"In a large bowl, combine pudding mix and mil...\n",
              "                            ...                        \n",
              "95    [\"Preheat oven to 375 degrees.\", \"Cut the pear...\n",
              "96    [\"In medium saucepan over low heat, melt almon...\n",
              "97    [\"In a 6-qt. stockpot, cook sausage, shallots ...\n",
              "98    [\"Brown ground beef and onion; drain.\", \"Brown...\n",
              "99    [\"Mix pineapple juice and lemonade.\", \"Chill u...\n",
              "Name: directions, Length: 100, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w_xrYOqvgkE",
        "outputId": "69cad284-8fcc-4123-de36-3642f4075507"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "directions =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "\n",
        "  for j in range(replicated_size):\n",
        "    print(\"recipe: \",j)\n",
        "    md = generate_recipe(raw_text)\n",
        "    directions[i].append(get_instr(md))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Generating the recipe:  0\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  1\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  2\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  3\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  4\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  5\n",
            "recipe:  0\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  6\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  7\n",
            "recipe:  0\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  8\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  9\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  10\n",
            "recipe:  0\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  11\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  12\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  13\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  14\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  15\n",
            "recipe:  0\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  16\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  17\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  18\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  19\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  20\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  21\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  22\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  23\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  24\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  25\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  26\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  4\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  5\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  6\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  27\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  28\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  29\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  30\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  31\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  32\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  33\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  34\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  35\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  36\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  37\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  38\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  39\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  40\n",
            "recipe:  0\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  1\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  41\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  42\n",
            "recipe:  0\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  43\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  44\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  45\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  46\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  47\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  48\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  49\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  50\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  51\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  52\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  53\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  54\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  55\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  56\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  57\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  58\n",
            "recipe:  0\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  59\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  60\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  61\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  62\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  63\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  64\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  6\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  65\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  66\n",
            "recipe:  0\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  67\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  68\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  6\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  69\n",
            "recipe:  0\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  1\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  70\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  71\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  72\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  73\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  74\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  75\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  76\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  77\n",
            "recipe:  0\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  78\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  79\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  80\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  81\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  82\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  83\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  84\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  85\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  86\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  87\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  88\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  89\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  90\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  91\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  92\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  93\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  94\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  95\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  96\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  97\n",
            "recipe:  0\n",
            "Failed to generate, recipe's too long\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  98\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n",
            "\n",
            " Generating the recipe:  99\n",
            "recipe:  0\n",
            "recipe:  1\n",
            "recipe:  2\n",
            "recipe:  3\n",
            "recipe:  4\n",
            "recipe:  5\n",
            "recipe:  6\n",
            "recipe:  7\n",
            "recipe:  8\n",
            "recipe:  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY7IJP1tvtDY"
      },
      "source": [
        "#func of cos_sim\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def get_cosine(vec1, vec2):\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "\n",
        "def text_to_vector(text):\n",
        "    word = re.compile(r'\\w+')\n",
        "    words = word.findall(text)\n",
        "    return Counter(words)\n",
        "\n",
        "\n",
        "def get_result(content_a, content_b):\n",
        "    text1 = content_a\n",
        "    text2 = content_b\n",
        "\n",
        "    vector1 = text_to_vector(text1)\n",
        "    vector2 = text_to_vector(text2)\n",
        "\n",
        "    cosine_result = get_cosine(vector1, vector2)\n",
        "    return cosine_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA6mX6-bvzes"
      },
      "source": [
        "#cosine_similarity\n",
        "\n",
        "def COS_SIM(df_eval_dir,directions):\n",
        "  \"\"\"\n",
        "  df_eval_dir := gold_standard recipes\n",
        "  directions  := corresponding recipes \"\"\"\n",
        "\n",
        "  avg = 0\n",
        "\n",
        "  for i in range(len(directions)):\n",
        "    best = 0\n",
        "    for j in range(len(directions[i])):\n",
        "      cos = get_result(\" \".join(eval(df_eval_dir[i])),\n",
        "                      \" \".join(directions[i][j]))\n",
        "      best = max(best, cos)\n",
        "\n",
        "    avg += best\n",
        "\n",
        "  avg = avg/len(directions)\n",
        "\n",
        "  print(\"avg:\", avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8nbUd8qvzqd",
        "outputId": "6540f1c9-b4a0-42fa-a717-0161f2b9fd7b"
      },
      "source": [
        "COS_SIM(df_eval_dir,directions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg: 0.5173583741878693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhmPW8Jvv4Oq"
      },
      "source": [
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "import nltk.translate.gleu_score as gleu\n",
        "import nltk.translate.meteor_score as meteor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39CPJFonv4SC"
      },
      "source": [
        "# Helper Func.\n",
        "def list_to_words(recipe):\n",
        "  words = []\n",
        "  for i in recipe:\n",
        "    words += i.split()\n",
        "\n",
        "  return words\n",
        "\n",
        "# New BLEU/GLEU\n",
        "def bleu_score(recipe, refer):\n",
        "    hyp = list_to_words(eval(recipe))\n",
        "    refs = []\n",
        "    for i in refer:\n",
        "        # print(list_to_words(i))\n",
        "        refs.append(list_to_words(i))\n",
        "\n",
        "    smoothie = SmoothingFunction().method5\n",
        "    score_ref_a = bleu.sentence_bleu(refs, hyp, smoothing_function=smoothie, weights=(1,0,0,0))\n",
        "    return score_ref_a\n",
        "\n",
        "def gleu_score(recipe, refer):\n",
        "    hyp = list_to_words(eval(recipe))\n",
        "    refs = []\n",
        "    for i in refer:\n",
        "        refs.append(list_to_words(i))\n",
        "\n",
        "    score_ref_a = gleu.sentence_gleu(refs, hyp)\n",
        "    return score_ref_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjOFqH23v4Um",
        "outputId": "7ebf5b7c-8c4f-46f9-e958-c0357df446f0"
      },
      "source": [
        "bleu_avg = []\n",
        "gleu_avg = []\n",
        "\n",
        "for i in range(len(directions)):\n",
        "  # print(bleu_score(df_eval_dir[i], directions[i]))\n",
        "  bleu_avg.append(bleu_score(df_eval_dir[i], directions[i]))\n",
        "\n",
        "for i in range(len(directions)):\n",
        "  gleu_avg.append(gleu_score(df_eval_dir[i], directions[i]))\n",
        "print(np.mean(bleu_avg))\n",
        "print(np.mean(gleu_avg))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7411056491081777\n",
            "0.07095443012871357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuOGI5alSYk0",
        "outputId": "8afc81d3-87a2-4e92-cb0c-fb8a3d3279ed"
      },
      "source": [
        "!pip install rouge/requirements.txt\n",
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def list_to_string(start_list):\n",
        "  string=''.join([str(item) for item in start_list])\n",
        "  return string\n",
        "\n",
        "def rouge_cal(df_eval_dir, directions, rouge_type = 'rougeL'):\n",
        "  rough_avg = []\n",
        "  scorer = rouge_scorer.RougeScorer([rouge_type], use_stemmer=True)\n",
        "\n",
        "  for i in range(len(df_eval_dir)):\n",
        "    for j in range(10):\n",
        "      scores= scorer.score(df_eval_dir[i], list_to_string(directions[i][j]))\n",
        "      rough_avg.append(list(scores.values())[0][0])\n",
        "  return np.mean(rough_avg)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'rouge/requirements.txt'\n",
            "Hint: It looks like a path. File 'rouge/requirements.txt' does not exist.\u001b[0m\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (1.0.1)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyoj7kpDSYnD",
        "outputId": "82e457a1-b28d-464e-d815-6a04a2a974be"
      },
      "source": [
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, directions,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, directions,'rougeLsum'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rouge-1: 0.2355160133816919\n",
            "rouge-L: 0.16405261164197613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFxAgVNc0zOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545a8411-9d95-443f-a5c8-64eee2ce5f67"
      },
      "source": [
        "!sudo apt-get install git-lfs\n",
        "!pip install huggingface_hub\n",
        "!transformers-cli login\n",
        "!git config --global user.email \"uclqjia@ucl.ac.uk\"\n",
        "!git config --global user.name \"jky594176\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 1s (2,239 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.7/dist-packages (0.0.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface_hub) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2020.12.5)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.26.5\n",
            "    Uninstalling urllib3-1.26.5:\n",
            "      Successfully uninstalled urllib3-1.26.5\n",
            "Successfully installed urllib3-1.25.11\n",
            "2021-06-01 06:35:19.281918: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        \n",
            "Username: jky594176\n",
            "Password: \n",
            "Login successful\n",
            "Your token: wFlaWwkzYsLYQJZVfUieRigrdDLeDnTTZyiVhPAaIgeOELnnojbdQUAzHcSqpkgfkfKmxjkzOAmXBRTuYACjeYBoMzLFYVQgxRPnCXlpkACwUKifzsfRoihlcNWIGVgn \n",
            "\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRTpj4XEOn1Y"
      },
      "source": [
        "from transformers import BartForConditionalGeneration\n",
        "model_type = [\"recipe_GPT2\",\"recipe_BART1\",\"recipe_BART2\",\"recipe_BART1_NN\",\"recipe_BART1_GRU\",\"recipe_bart2_v3\"]\n",
        "output_dir = \"./gdrive/MyDrive/COMP0087/model_checkpoint_0526\"\n",
        "model = BartForConditionalGeneration.from_pretrained(output_dir)\n",
        "model.push_to_hub(model_type[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_4GgD11O63o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}