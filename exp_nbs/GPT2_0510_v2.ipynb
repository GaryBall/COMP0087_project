{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of GPT2_Retrain.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BuHBMqJkcJyV",
        "r_H7VVRegge6",
        "ixDQiUGmWJt9"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9f95e418c3b49ae8396afffbd4472dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2c498c7413024ea68df1407a9ffa460a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a7c88d625c4b4376b35e1f13e8fd8233",
              "IPY_MODEL_e2a684475a4b42038e1393a3e29e239e"
            ]
          }
        },
        "2c498c7413024ea68df1407a9ffa460a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a7c88d625c4b4376b35e1f13e8fd8233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_890df539ddb344b0b9d42de0bcbcac1f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a537eb855379429ba427da0e25bc6774"
          }
        },
        "e2a684475a4b42038e1393a3e29e239e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6d5f14f8a96144b18fdfc07f1adde101",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.04M/1.04M [00:02&lt;00:00, 396kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_634d2376d3944d19a19fd3fb504f9d90"
          }
        },
        "890df539ddb344b0b9d42de0bcbcac1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a537eb855379429ba427da0e25bc6774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d5f14f8a96144b18fdfc07f1adde101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "634d2376d3944d19a19fd3fb504f9d90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c35684b7774d465f9761bc92f0c6e807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6fd45f38034a44aaa284e5db026d80e1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ae9ca7ab7b0b4b3b85aab653e06110a9",
              "IPY_MODEL_0b5bb0738dfd45fe9e8ec5e16ef8a242"
            ]
          }
        },
        "6fd45f38034a44aaa284e5db026d80e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae9ca7ab7b0b4b3b85aab653e06110a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bb27a9d298754ce38d4f31e669ea5252",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58763ea1f7f74ae09c62d0158733dd7c"
          }
        },
        "0b5bb0738dfd45fe9e8ec5e16ef8a242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5acaf5b070c348279b80774d960ac7bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:01&lt;00:00, 251kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c95056fc4feb4386b7781f39944d4ed7"
          }
        },
        "bb27a9d298754ce38d4f31e669ea5252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58763ea1f7f74ae09c62d0158733dd7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5acaf5b070c348279b80774d960ac7bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c95056fc4feb4386b7781f39944d4ed7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "27b553da80b44fffac64801bba4b3558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9498539ec74d4c4d92c1d1383d1e72ca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_77c4e74ed1874298b71df81695e09170",
              "IPY_MODEL_f4015a24b4bc44b597e4f77ce39107de"
            ]
          }
        },
        "9498539ec74d4c4d92c1d1383d1e72ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77c4e74ed1874298b71df81695e09170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5be974a3fc804e9085cc8c66bb4c8078",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355256,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355256,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_de773b04a3ba4f6fa78bbfba1aef6519"
          }
        },
        "f4015a24b4bc44b597e4f77ce39107de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6f268cbc58404898a379ee8efb0a1378",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 3.02MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a54bc7f5dd5a47b483966d225c9d14c1"
          }
        },
        "5be974a3fc804e9085cc8c66bb4c8078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "de773b04a3ba4f6fa78bbfba1aef6519": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f268cbc58404898a379ee8efb0a1378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a54bc7f5dd5a47b483966d225c9d14c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuHBMqJkcJyV"
      },
      "source": [
        "# SET UP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oidXsqx_S6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67463a77-e968-4307-d1d7-73f04201d19b"
      },
      "source": [
        "# Divine beast bless no bug here! \n",
        "#         ┌─┐    ┌─┐\n",
        "#      ┌─┘ ┴───┘ ┴──┐\n",
        "#      │                   │\n",
        "#      │       ───       │\n",
        "#      │  ─┬┘     └┬─  │\n",
        "#      │                   │\n",
        "#      │       ─┴─       │\n",
        "#      │                   │\n",
        "#      └─┐         ┌───┘\n",
        "#          │         │\n",
        "#          │         │\n",
        "#          │         │\n",
        "#          │         └──────────────┐\n",
        "#          │                                  │\n",
        "#          │                                  ├─┐\n",
        "#          │                                  ┌─┘\n",
        "#          │                                  │\n",
        "#          └─┐  ┐  ┌──────┬──┐  ┌──┘\n",
        "#            │  ─┤ ─┤         │  ─┤ ─┤\n",
        "#            └──┴──┘         └──┴──┘\n",
        "\n",
        "!pip install transformers\n",
        "!pip install boto3\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd ..  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 27.1MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 34.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/fb/695bde21f34b6bd52db30278d6fdc63b9ce54b27d6ef3369444f621c98e2/boto3-1.17.83-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 6.4MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.6MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.83\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/46/8a9db3126e2390cb0a551344a237b5bc5162478b04f56a96cbe1cac5d5a8/botocore-1.20.83-py2.py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 8.2MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/cd/1e2ec680ec7b09846dc6e605f5a7709dfb9d7128e51a026e7154e18a234e/urllib3-1.26.5-py2.py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.83->boto3) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.83->boto3) (1.15.0)\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, urllib3, botocore, s3transfer, boto3\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.17.83 botocore-1.20.83 jmespath-0.10.0 s3transfer-0.4.2 urllib3-1.26.5\n",
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8048, done.\u001b[K\n",
            "remote: Counting objects: 100% (135/135), done.\u001b[K\n",
            "remote: Compressing objects: 100% (102/102), done.\u001b[K\n",
            "remote: Total 8048 (delta 66), reused 65 (delta 28), pack-reused 7913\u001b[K\n",
            "Receiving objects: 100% (8048/8048), 14.11 MiB | 28.95 MiB/s, done.\n",
            "Resolving deltas: 100% (5467/5467), done.\n",
            "/content/apex\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-f1ry6_4c\n",
            "Created temporary directory: /tmp/pip-req-tracker-geela_hd\n",
            "Created requirements tracker '/tmp/pip-req-tracker-geela_hd'\n",
            "Created temporary directory: /tmp/pip-install-zp6lw3z9\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-ty4furyy\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-geela_hd'\n",
            "    Running setup.py (path:/tmp/pip-req-build-ty4furyy/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.8.1+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-ty4furyy/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-ty4furyy/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-ty4furyy/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-ty4furyy/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-ty4furyy/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n",
            "    writing manifest file '/tmp/pip-req-build-ty4furyy/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-ty4furyy/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-ty4furyy has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-geela_hd'\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-5lcc_7sn\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-5lcc_7sn\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-ty4furyy/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-ty4furyy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-5lcc_7sn --python-tag cp37\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.8.1+cu101\n",
            "\n",
            "\n",
            "  /tmp/pip-req-build-ty4furyy/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/pyprof\n",
            "  copying apex/pyprof/__init__.py -> build/lib/apex/pyprof\n",
            "  creating build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/__init__.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/reparameterization.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/weight_norm.py -> build/lib/apex/reparameterization\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/__init__.py -> build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/nvmarker.py -> build/lib/apex/pyprof/nvtx\n",
            "  creating build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/loss.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/dropout.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/softmax.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/usage.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/output.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/conv.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__main__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/base.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/utility.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/embedding.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/recurrentCell.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/prof.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pointwise.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/reduction.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/activation.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__init__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pooling.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/normalization.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/misc.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/linear.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/randomSample.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/data.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/convert.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/blas.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/optim.py -> build/lib/apex/pyprof/prof\n",
            "  creating build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/db.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/kernel.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/nvvp.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__main__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__init__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/parse.py -> build/lib/apex/pyprof/parse\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-5lcc_7sn/apex-0.1-cp37-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v2.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v3.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/pyprof/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/nvmarker.py'\n",
            "  adding 'apex/pyprof/parse/__init__.py'\n",
            "  adding 'apex/pyprof/parse/__main__.py'\n",
            "  adding 'apex/pyprof/parse/db.py'\n",
            "  adding 'apex/pyprof/parse/kernel.py'\n",
            "  adding 'apex/pyprof/parse/nvvp.py'\n",
            "  adding 'apex/pyprof/parse/parse.py'\n",
            "  adding 'apex/pyprof/prof/__init__.py'\n",
            "  adding 'apex/pyprof/prof/__main__.py'\n",
            "  adding 'apex/pyprof/prof/activation.py'\n",
            "  adding 'apex/pyprof/prof/base.py'\n",
            "  adding 'apex/pyprof/prof/blas.py'\n",
            "  adding 'apex/pyprof/prof/conv.py'\n",
            "  adding 'apex/pyprof/prof/convert.py'\n",
            "  adding 'apex/pyprof/prof/data.py'\n",
            "  adding 'apex/pyprof/prof/dropout.py'\n",
            "  adding 'apex/pyprof/prof/embedding.py'\n",
            "  adding 'apex/pyprof/prof/index_slice_join_mutate.py'\n",
            "  adding 'apex/pyprof/prof/linear.py'\n",
            "  adding 'apex/pyprof/prof/loss.py'\n",
            "  adding 'apex/pyprof/prof/misc.py'\n",
            "  adding 'apex/pyprof/prof/normalization.py'\n",
            "  adding 'apex/pyprof/prof/optim.py'\n",
            "  adding 'apex/pyprof/prof/output.py'\n",
            "  adding 'apex/pyprof/prof/pointwise.py'\n",
            "  adding 'apex/pyprof/prof/pooling.py'\n",
            "  adding 'apex/pyprof/prof/prof.py'\n",
            "  adding 'apex/pyprof/prof/randomSample.py'\n",
            "  adding 'apex/pyprof/prof/recurrentCell.py'\n",
            "  adding 'apex/pyprof/prof/reduction.py'\n",
            "  adding 'apex/pyprof/prof/softmax.py'\n",
            "  adding 'apex/pyprof/prof/usage.py'\n",
            "  adding 'apex/pyprof/prof/utility.py'\n",
            "  adding 'apex/reparameterization/__init__.py'\n",
            "  adding 'apex/reparameterization/reparameterization.py'\n",
            "  adding 'apex/reparameterization/weight_norm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp37-none-any.whl size=204691 sha256=7ea8b3250e09314215d2636997c00f76346902d3989fc686f2835671b9a27489\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f1ry6_4c/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "  Removing source in /tmp/pip-req-build-ty4furyy\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-geela_hd'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsmxZM0-x7fF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a55bc5c-2390-43ca-829f-f5d5447c5366"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_K2tj7NcN3S"
      },
      "source": [
        "# DATA Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3gLMKblgV2b",
        "outputId": "01a25ee2-46ce-46a6-a3a6-827af95fa941"
      },
      "source": [
        "# data preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import json\n",
        "import h5py\n",
        "\n",
        "# control\n",
        "import argparse\n",
        "import logging\n",
        "from tqdm import trange\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# transformers\n",
        "from transformers import GPT2Config\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import BartForCausalLM, BartTokenizer,BartConfig"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr3GLmeRgslI"
      },
      "source": [
        "# the original dataframe\n",
        "df_ori = pd.read_csv(\"./gdrive/MyDrive/COMP0087/full_dataset.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxIQiIoQobMb"
      },
      "source": [
        "np.random.seed(10)\n",
        "train_len = 500000\n",
        "test_len = 200\n",
        "subset_idx = np.random.choice(range(len(df_ori)),train_len+test_len,replace=False)\n",
        "train_idx = subset_idx[0:train_len]\n",
        "test_idx = subset_idx[train_len:]\n",
        "df = df_ori.loc[subset_idx ,:]\n",
        "# df_test = df_ori.loc[subset_idx[-100:],:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vogi-NsDgd1z"
      },
      "source": [
        "df.drop(df[df.title.map(lambda x: len(x)<4)].index, inplace=True)\n",
        "df.drop(df[df.ingredients.map(lambda x: len(x)<2)].index, inplace=True)\n",
        "df.drop(df[df.directions.map(lambda x: len(x) < 2 or len(''.join(x)) < 30)].index, inplace=True)\n",
        "df.drop(df[df.directions.map(lambda x: re.search('(step|mix all)', ''.join(str(x)), re.IGNORECASE)!=None)].index, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyIpD-Apgh7s"
      },
      "source": [
        "df.reset_index(inplace=True)\n",
        "train, test = train_test_split(df, test_size=0.05) #use 5% for test set\n",
        "# we only want first 10000 and 100 for train/test\n",
        "# this has an error: train[75400:75600]\n",
        "train = train[50000:400000]\n",
        "test = test[0:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuytage9gjzH"
      },
      "source": [
        "def df_to_plaintext_file(input_df, output_file):\n",
        "    print(\"Writing to\", output_file)\n",
        "    with open(output_file, 'w') as f:\n",
        "        for index, row in input_df.iterrows():\n",
        "            if index%100000==0:\n",
        "                print(index)\n",
        "            if type(row.NER)!=str:\n",
        "                continue\n",
        "            title = row.title\n",
        "            directions = json.loads(row.directions)\n",
        "            ingredients = json.loads(row.ingredients)\n",
        "            ner = json.loads(row.NER)\n",
        "            res = \"<RECIPE_START> <INPUT_START> \" + \" <NEXT_INPUT> \".join(ner) + \" <INPUT_END> <INGR_START> \" + \\\n",
        "              \" <NEXT_INGR> \".join(ingredients) + \" <INGR_END> <INSTR_START> \" + \\\n",
        "              \" <NEXT_INSTR> \".join(directions) + \" <INSTR_END> <TITLE_START> \" + title + \" <TITLE_END> <RECIPE_END>\"\n",
        "            f.write(\"{}\\n\".format(res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mpW4paENyVv",
        "outputId": "caea64ab-51d0-4c5a-872a-dd06ddbe8bad"
      },
      "source": [
        "df_to_plaintext_file(train, 'unsupervised_train.txt')\n",
        "df_to_plaintext_file(test, 'unsupervised_test.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing to unsupervised_train.txt\n",
            "300000\n",
            "0\n",
            "100000\n",
            "400000\n",
            "200000\n",
            "Writing to unsupervised_test.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788,
          "referenced_widgets": [
            "a9f95e418c3b49ae8396afffbd4472dd",
            "2c498c7413024ea68df1407a9ffa460a",
            "a7c88d625c4b4376b35e1f13e8fd8233",
            "e2a684475a4b42038e1393a3e29e239e",
            "890df539ddb344b0b9d42de0bcbcac1f",
            "a537eb855379429ba427da0e25bc6774",
            "6d5f14f8a96144b18fdfc07f1adde101",
            "634d2376d3944d19a19fd3fb504f9d90",
            "c35684b7774d465f9761bc92f0c6e807",
            "6fd45f38034a44aaa284e5db026d80e1",
            "ae9ca7ab7b0b4b3b85aab653e06110a9",
            "0b5bb0738dfd45fe9e8ec5e16ef8a242",
            "bb27a9d298754ce38d4f31e669ea5252",
            "58763ea1f7f74ae09c62d0158733dd7c",
            "5acaf5b070c348279b80774d960ac7bb",
            "c95056fc4feb4386b7781f39944d4ed7",
            "27b553da80b44fffac64801bba4b3558",
            "9498539ec74d4c4d92c1d1383d1e72ca",
            "77c4e74ed1874298b71df81695e09170",
            "f4015a24b4bc44b597e4f77ce39107de",
            "5be974a3fc804e9085cc8c66bb4c8078",
            "de773b04a3ba4f6fa78bbfba1aef6519",
            "6f268cbc58404898a379ee8efb0a1378",
            "a54bc7f5dd5a47b483966d225c9d14c1"
          ]
        },
        "id": "4wqzp-seoHsw",
        "outputId": "138d9c6b-3d2e-4e49-dc7d-ceb3b6556124"
      },
      "source": [
        "#tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "special_tokens = {\n",
        "    \"additional_special_tokens\": [\n",
        "        \"<TITLE_START>\",\n",
        "        \"<TITLE_END>\",\n",
        "        \"<INSTR_START>\",\n",
        "        \"<NEXT_INSTR>\",\n",
        "        \"<INSTR_END>\",\n",
        "        \"<INGR_START>\",\n",
        "        \"<NEXT_INGR>\",\n",
        "        \"<INGR_END>\",\n",
        "        \"<RECIPE_START>\",\n",
        "        \"<RECIPE_END>\",\n",
        "        \"<INPUT_START>\",\n",
        "        \"<INPUT_END>\",\n",
        "        \"<NEXT_INPUT>\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "end_token_id = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
        "\n",
        "hf = h5py.File(\"unsupervised.h5\", \"w\")\n",
        "for filename in [\"test\", \"train\"]:\n",
        "    out_np = []\n",
        "    data = open(\"unsupervised_\"+filename+\".txt\", \"r\")\n",
        "    num = 0\n",
        "    rows = 0\n",
        "    last=[]\n",
        "    for line in data:\n",
        "        num+=1\n",
        "        if num%10000 == 0:\n",
        "            print(\"Read \"+str(num)+\" Written: \"+str(rows))\n",
        "        # print(line)\n",
        "        text_tokens = tokenizer(line)['input_ids']\n",
        "        # print(text_tokens[0:5])\n",
        "        if len(text_tokens) > 1024: #Recipe won't fit the model\n",
        "            continue\n",
        "\n",
        "        # text_tokens_ids = tokenizer.convert_tokens_to_ids(text_tokens)\n",
        "        text_tokens_ids = text_tokens\n",
        "        # print(text_tokens_ids[0:5])\n",
        "\n",
        "        if (len(last) + len(text_tokens_ids)) <= 1024:\n",
        "            last+=text_tokens_ids\n",
        "        else:\n",
        "            while len(last) < 1024:\n",
        "                last.append(end_token_id)\n",
        "            out_np.append(last)\n",
        "            last=text_tokens_ids\n",
        "            rows+=1\n",
        "    out_mat = np.matrix(out_np)\n",
        "    print(out_mat.shape)\n",
        "    # print(out_mat)\n",
        "    hf.create_dataset(filename, data=out_mat)\n",
        "hf.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9f95e418c3b49ae8396afffbd4472dd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c35684b7774d465f9761bc92f0c6e807",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27b553da80b44fffac64801bba4b3558",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(27, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1024). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Read 10000 Written: 2624\n",
            "Read 20000 Written: 5275\n",
            "Read 30000 Written: 7887\n",
            "Read 40000 Written: 10514\n",
            "Read 50000 Written: 13131\n",
            "Read 60000 Written: 15766\n",
            "Read 70000 Written: 18401\n",
            "Read 80000 Written: 21012\n",
            "Read 90000 Written: 23630\n",
            "Read 100000 Written: 26283\n",
            "Read 110000 Written: 28891\n",
            "Read 120000 Written: 31517\n",
            "Read 130000 Written: 34168\n",
            "Read 140000 Written: 36816\n",
            "Read 150000 Written: 39422\n",
            "Read 160000 Written: 42062\n",
            "Read 170000 Written: 44710\n",
            "Read 180000 Written: 47379\n",
            "Read 190000 Written: 49976\n",
            "Read 200000 Written: 52616\n",
            "Read 210000 Written: 55257\n",
            "Read 220000 Written: 57876\n",
            "Read 230000 Written: 60518\n",
            "Read 240000 Written: 63137\n",
            "Read 250000 Written: 65748\n",
            "Read 260000 Written: 68340\n",
            "Read 270000 Written: 70973\n",
            "Read 280000 Written: 73614\n",
            "Read 290000 Written: 76211\n",
            "Read 300000 Written: 78827\n",
            "Read 310000 Written: 81477\n",
            "Read 320000 Written: 84133\n",
            "Read 330000 Written: 86782\n",
            "Read 340000 Written: 89433\n",
            "Read 350000 Written: 92058\n",
            "(93222, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnVQ59V4FdiF"
      },
      "source": [
        "# fine-tune with GPT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz7e2PoybaiE"
      },
      "source": [
        "# fine-tuning model with bart\n",
        "# logging info\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import random\n",
        "import gc\n",
        "import boto3\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5MKxGg8bhj4"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm, trange\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import torch\n",
        "\n",
        "from transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm4SBl8hbqj3"
      },
      "source": [
        "# data loading\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path='train', block_size=512):\n",
        "        cached_features_file = \"unsupervised.h5\"\n",
        "\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        with h5py.File(cached_features_file, 'r') as f:\n",
        "            if file_path=='test':\n",
        "                self.examples = f[file_path][:] #this is a dev set, 10% of a test set\n",
        "            else:\n",
        "                self.examples = f[file_path][:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item])\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, evaluate=False):\n",
        "    dataset = TextDataset(tokenizer, file_path=\"test\" if evaluate else \"train\", block_size=args.block_size)\n",
        "    print(dataset[0:5])\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6grSGLbPb4Cm"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "## Required parameters\n",
        "parser.add_argument(\"--train_data_file\", default=\"unsupervised.h5\", type=str, required=False,\n",
        "                        help=\"The input training data file (a text file).\")\n",
        "parser.add_argument(\"--output_dir\", default=\"./gdrive/MyDrive/COMP0087/model_checkpoint/350k_GPT2\", type=str, required=False,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "## Other parameters\n",
        "parser.add_argument(\"--model_type\", default=\"gpt2\" ,type=str,   #\"facebook/bart-base\"\n",
        "                        help=\"The model architecture to be fine-tuned.\")\n",
        "parser.add_argument(\"--model_name_or_path\", default=\"gpt2\", type=str,\n",
        "                        help=\"The model checkpoint for weights initialization.\")\n",
        "\n",
        "parser.add_argument(\"--eval_data_file\", default=None, type=str,\n",
        "                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
        "\n",
        "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
        "parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
        "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
        "parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
        "                        help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\n",
        "parser.add_argument(\"--block_size\", default=-1, type=int,\n",
        "                        help=\"Optional input sequence length after tokenization.\"\n",
        "                             \"The training dataset will be truncated in block of this size for training.\"\n",
        "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
        "parser.add_argument(\"--do_train\", action='store_true',\n",
        "                        help=\"Whether to run training.\")\n",
        "parser.add_argument(\"--do_eval\", action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
        "                        help=\"Run evaluation during training at each logging step.\")\n",
        "parser.add_argument(\"--do_lower_case\", action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "\n",
        "parser.add_argument(\"--per_gpu_train_batch_size\", default=3, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for training.\")\n",
        "parser.add_argument(\"--per_gpu_eval_batch_size\", default=4, type=int,\n",
        "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
        "parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "                        help=\"Weight deay if we apply some.\")\n",
        "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                        help=\"Max gradient norm.\")\n",
        "parser.add_argument(\"--num_train_epochs\", default=1, type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
        "                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
        "parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
        "                        help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "parser.add_argument('--logging_steps', type=int, default=50,\n",
        "                        help=\"Log every X updates steps.\")\n",
        "parser.add_argument('--save_steps', type=int, default=500000,\n",
        "                        help=\"Save checkpoint every X updates steps.\")\n",
        "parser.add_argument(\"--eval_all_checkpoints\", action='store_true',\n",
        "                        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\n",
        "parser.add_argument(\"--no_cuda\", action='store_true',\n",
        "                        help=\"Avoid using CUDA when available\")\n",
        "parser.add_argument('--overwrite_output_dir', action='store_true',\n",
        "                        help=\"Overwrite the content of the output directory\")\n",
        "parser.add_argument('--overwrite_cache', action='store_true',\n",
        "                        help=\"Overwrite the cached training and evaluation sets\")\n",
        "parser.add_argument(\"--aws_bucket\", default=\"\", type=str,\n",
        "                        help=\"Whether to upload to specified bucket.\")\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "def setup_args_for_model(args):\n",
        "  args.model_type =\"gpt2\" #\"facebook/bart-base\"\n",
        "  args.model_type = \"\"\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MqfhIo9cgYn"
      },
      "source": [
        "def model_init(args,logger, model_class, tokenizer):\n",
        "  if args.eval_data_file is None and args.do_eval:\n",
        "    raise ValueError(\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "                         \"or remove the --do_eval argument.\")\n",
        "  if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
        "    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "  args.n_gpu = torch.cuda.device_count()\n",
        "  args.device = device\n",
        "\n",
        "  # Setup logging\n",
        "  logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                        level = logging.INFO)\n",
        "\n",
        "  model = model_class.from_pretrained(args.model_name_or_path)\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "  if args.block_size <= 0:\n",
        "    args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "  args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "  model.to(args.device)\n",
        "\n",
        "  logger.info(\"Training/evaluation parameters %s\", args)\n",
        "  return model, logger\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWoFI3LciZzc"
      },
      "source": [
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "    try:\n",
        "        from apex import amp\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "    \n",
        "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\")\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\"  Total train batch size = %d\",\n",
        "                   args.train_batch_size * args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "\n",
        "    \n",
        "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=False)\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            logger.info(step)\n",
        "            #inputs, labels = (batch[:,0:-1], batch[:,1:])\n",
        "            inputs, labels = (batch, batch)\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            # print(inputs[,3:10],labels[,3:10])\n",
        "\n",
        "            outputs = model(inputs,labels =labels)\n",
        "            # print(outputs)\n",
        "            loss = outputs['loss'] # model outputs are always tuple in transformers (see doc)\n",
        "            print(loss)\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                # if False:\n",
        "                if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "                    if args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
        "                    if args.aws_bucket:\n",
        "                        tgz = \"checkpoint-{}.tar\".format(global_step)\n",
        "                        tardir(output_dir, tgz)\n",
        "                        shutil.rmtree(output_dir)\n",
        "                        s3 = boto3.resource('s3')\n",
        "                        s3.Object(args.aws_bucket, \"checkpoints-gpt-medium/\"+tgz).upload_file(tgz)\n",
        "                        os.remove(tgz)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > 2:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "            del inputs, labels, outputs, loss\n",
        "            torch.cuda.empty_cache()\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRhiiMT4cwEr"
      },
      "source": [
        "#model, logger = model_init(args,logger,BartForCausalLM,tokenizer)\n",
        "model, logger = model_init(args,logger, GPT2LMHeadModel, tokenizer)\n",
        "#model, logger = model_init(args,logger,BartForConditionalGeneration,BartTokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ42CcQK_iRJ"
      },
      "source": [
        "train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
        "print(len(train_dataset.examples))\n",
        "global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "print(\" global_step = %.5f, average loss = %.5f\" %(global_step, tr_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXzoNYpwlYz0"
      },
      "source": [
        "## evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQL7vPBEDZjK"
      },
      "source": [
        "def save_model(args,model,tokenizer,model_class,tokenizer_class):\n",
        "  # Create output directory if neede\n",
        "  if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)\n",
        "\n",
        "  logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "  # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "  # They can then be reloaded using `from_pretrained()`\n",
        "  model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "  model_to_save.save_pretrained(args.output_dir)\n",
        "  tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "  # Good practice: save your training arguments together with the trained model\n",
        "  torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n",
        "\n",
        "  # Load a trained model and vocabulary that you have fine-tuned\n",
        "  model = model_class.from_pretrained(args.output_dir)\n",
        "  tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
        "  model.to(args.device)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpKZFikiiyaE"
      },
      "source": [
        "#model_class = BartForCausalLM\n",
        "#tokenizer_class = BartTokenizer\n",
        "#save_model(args,model,tokenizer,model_class,tokenizer_class)\n",
        "model_class = GPT2LMHeadModel\n",
        "tokenizer_class = GPT2Tokenizer\n",
        "save_model(args,model,tokenizer,model_class,tokenizer_class)\n",
        "#model_class = BartForConditionalGeneration\n",
        "#tokenizer_class = BartTokenizer\n",
        "#save_model(args,model,tokenizer,model_class,tokenizer_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa-gH4egxBBD"
      },
      "source": [
        "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)\n",
        "\n",
        "    if not os.path.exists(eval_output_dir):\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        batch = batch.to(args.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch, labels=batch)\n",
        "            lm_loss = outputs[0]\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
        "\n",
        "    result = {\n",
        "        \"perplexity\": perplexity\n",
        "    }\n",
        "\n",
        "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvYkcy_EmF_M"
      },
      "source": [
        "# Evaluation\n",
        "results = {}\n",
        "checkpoints = [args.output_dir]\n",
        "if args.eval_all_checkpoints:\n",
        "  checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
        "  logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "for checkpoint in checkpoints:\n",
        "  global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
        "  model_class = GPT2LMHeadModel#BartForConditionalGeneration#GPT2LMHeadModel # BartForCausalLM\n",
        "  model = model_class.from_pretrained(checkpoint)\n",
        "  model.to(args.device)\n",
        "  result = evaluate(args, model, tokenizer, prefix=global_step)\n",
        "  result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
        "  results.update(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXpDix2yd3lB"
      },
      "source": [
        "# generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX9UqNILeAvK"
      },
      "source": [
        "#model loading\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"mbien/recipenlg\")\n",
        "#model = AutoModelWithLMHead.from_pretrained(\"mbien/recipenlg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPi6Xwt5GrvO"
      },
      "source": [
        "#raw_text = 'tomato,egg'\n",
        "#prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "#tokenizer.encode(prepared_input)[0:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8ea0cb0AdLu"
      },
      "source": [
        "#new_model = BartForCausalLM.from_pretrained(\"./gdrive/MyDrive/COMP0087/model_checkpoint\")\n",
        "#new_tokenizer = BartTokenizer.from_pretrained(\"./gdrive/MyDrive/COMP0087/model_checkpoint\")\n",
        "#new_model = GPT2LMHeadModel.from_pretrained(args.output_dir)\n",
        "#new_tokenizer = GPT2LMHeadModel.from_pretrained(args.output_dir)\n",
        "#new_model = BartForConditionalGeneration.from_pretrained(args.output_dir)\n",
        "#new_tokenizer= BartForConditionalGeneration.from_pretrained(args.output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofrfJDXw_iqf"
      },
      "source": [
        "def set_seed(args):\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(model, length, context, tokenizer, num_samples=1, temperature=1, top_k=0, top_p=0.0, device='cpu'):\n",
        "    end_token = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
        "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "    with torch.no_grad():\n",
        "        for _ in trange(length):\n",
        "            inputs = {'input_ids': generated}\n",
        "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
        "            # print(outputs)\n",
        "            next_token_logits = outputs[\"logits\"][0, -1, :] / temperature\n",
        "            \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
        "            #print(next_token)\n",
        "            if next_token.item() == end_token:\n",
        "                break\n",
        "    return generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHbWruT6ba7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580bf124-e202-4458-9bdd-7ef409aa5557"
      },
      "source": [
        "#model = BartForCausalLM.from_pretrained('./gdrive/MyDrive/COMP0087/model_checkpoint/500k')\n",
        "#tokenizer = BartTokenizer.from_pretrained('./gdrive/MyDrive/COMP0087/model_checkpoint/500k')\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"mbien/recipenlg\")\n",
        "#model = AutoModelWithLMHead.from_pretrained(\"mbien/recipenlg\")\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('./gdrive/MyDrive/COMP0087/model_checkpoint/350k_GPT2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('./gdrive/MyDrive/COMP0087/model_checkpoint/350k_GPT2')\n",
        "model.eval()\n",
        "model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50270, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50270, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6VNmuOzd4vW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74d59f5-73cb-40c2-aef3-d5c17bdc6b4a"
      },
      "source": [
        "# raw_text = args.prompt if args.prompt else input(\"Comma-separated ingredients, semicolon to close the list >>> \")\n",
        "raw_text = 'noodle, cake'\n",
        "\n",
        "prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "context_tokens = tokenizer.encode(prepared_input)[0:-1]\n",
        "out = sample_sequence(\n",
        "            model=model,\n",
        "            context=context_tokens,\n",
        "            tokenizer=tokenizer,\n",
        "            length=1000,\n",
        "            device = 'cuda' #args.device #'cpu' #\n",
        "        )\n",
        "out = out[0, len(context_tokens):].tolist()\n",
        "text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n",
        "if \"<RECIPE_END>\" not in text:\n",
        "  print(text)\n",
        "  print(\"Failed to generate, recipe's too long\")\n",
        "  full_text = prepared_input + text\n",
        "  print(full_text)\n",
        "else:\n",
        "  full_text = prepared_input + text\n",
        "  print(full_text)\n",
        "  markdown = re.sub(\"<RECIPE_(START|END)>\", \"\", full_text)\n",
        "  recipe_n_title = markdown.split(\"<TITLE_START>\")\n",
        "  title = \"# \" + recipe_n_title[1].replace(\"<TITLE_END>\", \"\") + \" #\\n\"\n",
        "  markdown = recipe_n_title[0].replace(\"<INPUT_START>\", \"## Input ingredients ##\\n`\").replace(\"<INPUT_END>\", \"`\\n\")\n",
        "  markdown = markdown.replace(\"<NEXT_INPUT>\", \"`\\n`\").replace(\"<INGR_START>\", \"## Ingredients ##\\n* \").replace(\"<NEXT_INGR>\", \"\\n* \").replace(\"<INGR_END>\", \"\\n\")\n",
        "  markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\").replace(\"<NEXT_INSTR>\", \"\\n\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "  #markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\\\"\").replace(\"<NEXT_INSTR>\", \"\\n\\\"\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "  markdown = re.sub(\"$ +#\", \"#\", markdown)\n",
        "  markdown = re.sub(\"( +`|` +)\", \"`\", markdown)\n",
        "  print(title+markdown)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 8/1000 [00:00<00:13, 75.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "  2%|▏         | 16/1000 [00:00<00:13, 74.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "  2%|▏         | 24/1000 [00:00<00:13, 73.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "  3%|▎         | 31/1000 [00:00<00:13, 71.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "  4%|▍         | 39/1000 [00:00<00:13, 71.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "  5%|▍         | 46/1000 [00:00<00:13, 70.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "  5%|▌         | 54/1000 [00:00<00:13, 71.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "  6%|▌         | 62/1000 [00:00<00:13, 71.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "  7%|▋         | 69/1000 [00:00<00:13, 70.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "  8%|▊         | 76/1000 [00:01<00:13, 69.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "  8%|▊         | 84/1000 [00:01<00:13, 69.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "  9%|▉         | 91/1000 [00:01<00:13, 68.35it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|▉         | 98/1000 [00:01<00:13, 67.90it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 105/1000 [00:01<00:13, 68.28it/s]\u001b[A\u001b[A\n",
            "\n",
            " 11%|█▏        | 113/1000 [00:01<00:12, 69.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 12%|█▏        | 120/1000 [00:01<00:12, 68.54it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 127/1000 [00:01<00:12, 67.57it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 134/1000 [00:01<00:13, 64.57it/s]\u001b[A\u001b[A\n",
            "\n",
            " 14%|█▍        | 141/1000 [00:02<00:13, 63.34it/s]\u001b[A\u001b[A\n",
            "\n",
            " 15%|█▍        | 148/1000 [00:02<00:13, 62.34it/s]\u001b[A\u001b[A\n",
            "\n",
            " 16%|█▌        | 155/1000 [00:02<00:13, 61.91it/s]\u001b[A\u001b[A\n",
            "\n",
            " 16%|█▌        | 162/1000 [00:02<00:13, 62.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 17%|█▋        | 169/1000 [00:02<00:13, 61.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 18%|█▊        | 176/1000 [00:02<00:13, 60.67it/s]\u001b[A\u001b[A\n",
            "\n",
            " 18%|█▊        | 183/1000 [00:02<00:13, 60.83it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19%|█▉        | 190/1000 [00:02<00:13, 59.17it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|█▉        | 196/1000 [00:02<00:13, 59.37it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 202/1000 [00:03<00:13, 59.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|██        | 209/1000 [00:03<00:13, 59.79it/s]\u001b[A\u001b[A\n",
            "\n",
            " 22%|██▏       | 215/1000 [00:03<00:13, 59.44it/s]\u001b[A\u001b[A\n",
            "\n",
            " 22%|██▏       | 222/1000 [00:03<00:12, 60.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 23%|██▎       | 229/1000 [00:03<00:12, 60.56it/s]\u001b[A\u001b[A\n",
            "\n",
            " 24%|██▎       | 236/1000 [00:03<00:12, 60.38it/s]\u001b[A\u001b[A\n",
            "\n",
            " 24%|██▍       | 243/1000 [00:03<00:12, 60.42it/s]\u001b[A\u001b[A\n",
            "\n",
            " 25%|██▌       | 250/1000 [00:03<00:12, 60.54it/s]\u001b[A\u001b[A\n",
            "\n",
            " 26%|██▌       | 257/1000 [00:03<00:12, 58.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 26%|██▋       | 263/1000 [00:04<00:13, 56.60it/s]\u001b[A\u001b[A\n",
            "\n",
            " 27%|██▋       | 269/1000 [00:04<00:13, 55.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 275/1000 [00:04<00:13, 54.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 281/1000 [00:04<00:13, 53.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 29%|██▊       | 287/1000 [00:04<00:13, 52.35it/s]\u001b[A\u001b[A\n",
            "\n",
            " 29%|██▉       | 293/1000 [00:04<00:13, 51.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30%|██▉       | 299/1000 [00:04<00:13, 51.37it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30%|███       | 305/1000 [00:04<00:13, 50.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 31%|███       | 311/1000 [00:05<00:13, 50.63it/s]\u001b[A\u001b[A\n",
            "\n",
            " 32%|███▏      | 317/1000 [00:05<00:13, 49.89it/s]\u001b[A\u001b[A\n",
            "\n",
            " 32%|███▏      | 322/1000 [00:05<00:13, 48.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 327/1000 [00:05<00:13, 48.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 332/1000 [00:05<00:14, 47.66it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34%|███▎      | 337/1000 [00:05<00:14, 47.30it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34%|███▍      | 342/1000 [00:05<00:13, 47.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35%|███▍      | 347/1000 [00:05<00:13, 46.87it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35%|███▌      | 352/1000 [00:05<00:14, 46.18it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▌      | 357/1000 [00:06<00:14, 45.48it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▌      | 362/1000 [00:06<00:14, 45.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 37%|███▋      | 367/1000 [00:06<00:14, 44.77it/s]\u001b[A\u001b[A\n",
            "\n",
            " 37%|███▋      | 372/1000 [00:06<00:14, 44.45it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|███▊      | 377/1000 [00:06<00:14, 44.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|███▊      | 382/1000 [00:06<00:14, 43.43it/s]\u001b[A\u001b[A\n",
            "\n",
            " 39%|███▊      | 387/1000 [00:06<00:14, 42.20it/s]\u001b[A\u001b[A\n",
            "\n",
            " 39%|███▉      | 392/1000 [00:06<00:14, 41.23it/s]\u001b[A\u001b[A\n",
            "\n",
            " 40%|███▉      | 397/1000 [00:07<00:14, 40.41it/s]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 402/1000 [00:07<00:14, 40.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41%|████      | 407/1000 [00:07<00:14, 39.65it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41%|████      | 411/1000 [00:07<00:15, 39.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 42%|████▏     | 415/1000 [00:07<00:15, 38.80it/s]\u001b[A\u001b[A\n",
            "\n",
            " 42%|████▏     | 419/1000 [00:07<00:15, 38.42it/s]\u001b[A\u001b[A\n",
            "\n",
            " 42%|████▏     | 423/1000 [00:07<00:15, 38.24it/s]\u001b[A\u001b[A\n",
            "\n",
            " 43%|████▎     | 427/1000 [00:07<00:15, 38.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 43%|████▎     | 431/1000 [00:07<00:15, 37.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▎     | 435/1000 [00:08<00:14, 37.68it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▍     | 439/1000 [00:08<00:14, 37.52it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▍     | 443/1000 [00:08<00:15, 37.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 45%|████▍     | 447/1000 [00:08<00:15, 36.40it/s]\u001b[A\u001b[A\n",
            "\n",
            " 45%|████▌     | 451/1000 [00:08<00:15, 35.92it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46%|████▌     | 455/1000 [00:08<00:15, 35.51it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46%|████▌     | 459/1000 [00:08<00:15, 35.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46%|████▋     | 463/1000 [00:08<00:15, 34.75it/s]\u001b[A\u001b[A\n",
            "\n",
            " 47%|████▋     | 467/1000 [00:08<00:15, 34.41it/s]\u001b[A\u001b[A\n",
            "\n",
            " 47%|████▋     | 471/1000 [00:09<00:15, 34.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 48%|████▊     | 475/1000 [00:09<00:15, 34.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 48%|████▊     | 479/1000 [00:09<00:15, 34.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 48%|████▊     | 483/1000 [00:09<00:15, 33.82it/s]\u001b[A\u001b[A\n",
            "\n",
            " 49%|████▊     | 487/1000 [00:09<00:15, 33.62it/s]\u001b[A\u001b[A\n",
            "\n",
            " 49%|████▉     | 491/1000 [00:09<00:15, 33.60it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50%|████▉     | 495/1000 [00:09<00:15, 33.43it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50%|████▉     | 499/1000 [00:09<00:15, 33.35it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 503/1000 [00:10<00:14, 33.29it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51%|█████     | 507/1000 [00:10<00:14, 33.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51%|█████     | 511/1000 [00:10<00:15, 32.45it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 515/1000 [00:10<00:15, 31.74it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 519/1000 [00:10<00:15, 31.26it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 523/1000 [00:10<00:15, 30.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 53%|█████▎    | 527/1000 [00:10<00:15, 30.53it/s]\u001b[A\u001b[A\n",
            "\n",
            " 53%|█████▎    | 531/1000 [00:10<00:15, 30.43it/s]\u001b[A\u001b[A\n",
            "\n",
            " 54%|█████▎    | 535/1000 [00:11<00:15, 30.28it/s]\u001b[A\u001b[A\n",
            "\n",
            " 54%|█████▍    | 539/1000 [00:11<00:15, 30.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 54%|█████▍    | 543/1000 [00:11<00:15, 30.02it/s]\u001b[A\u001b[A\n",
            "\n",
            " 55%|█████▍    | 547/1000 [00:11<00:15, 29.87it/s]\u001b[A\u001b[A\n",
            "\n",
            " 55%|█████▌    | 550/1000 [00:11<00:15, 29.63it/s]\u001b[A\u001b[A\n",
            "\n",
            " 55%|█████▌    | 553/1000 [00:11<00:15, 29.51it/s]\u001b[A\u001b[A\n",
            "\n",
            " 56%|█████▌    | 556/1000 [00:11<00:15, 29.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 56%|█████▌    | 559/1000 [00:11<00:15, 29.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 56%|█████▌    | 562/1000 [00:11<00:15, 29.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 56%|█████▋    | 565/1000 [00:12<00:15, 28.99it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57%|█████▋    | 568/1000 [00:12<00:14, 28.90it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57%|█████▋    | 571/1000 [00:12<00:14, 28.90it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57%|█████▋    | 574/1000 [00:12<00:15, 28.26it/s]\u001b[A\u001b[A\n",
            "\n",
            " 58%|█████▊    | 577/1000 [00:12<00:15, 27.57it/s]\u001b[A\u001b[A\n",
            "\n",
            " 58%|█████▊    | 580/1000 [00:12<00:15, 27.18it/s]\u001b[A\u001b[A\n",
            "\n",
            " 58%|█████▊    | 583/1000 [00:12<00:15, 26.94it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|█████▊    | 586/1000 [00:12<00:15, 26.73it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|█████▉    | 589/1000 [00:12<00:15, 26.64it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|█████▉    | 592/1000 [00:13<00:15, 26.54it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|█████▉    | 595/1000 [00:13<00:15, 26.41it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|█████▉    | 598/1000 [00:13<00:15, 26.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 601/1000 [00:13<00:15, 26.17it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 604/1000 [00:13<00:15, 26.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 61%|██████    | 607/1000 [00:13<00:15, 25.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 61%|██████    | 610/1000 [00:13<00:15, 25.85it/s]\u001b[A\u001b[A\n",
            "\n",
            " 61%|██████▏   | 613/1000 [00:13<00:15, 25.80it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62%|██████▏   | 616/1000 [00:14<00:14, 25.80it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62%|██████▏   | 619/1000 [00:14<00:14, 25.82it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62%|██████▏   | 622/1000 [00:14<00:14, 25.76it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62%|██████▎   | 625/1000 [00:14<00:14, 25.69it/s]\u001b[A\u001b[A\n",
            "\n",
            " 63%|██████▎   | 628/1000 [00:14<00:14, 25.56it/s]\u001b[A\u001b[A\n",
            "\n",
            " 63%|██████▎   | 631/1000 [00:14<00:14, 25.50it/s]\u001b[A\u001b[A\n",
            "\n",
            " 63%|██████▎   | 634/1000 [00:14<00:14, 25.55it/s]\u001b[A\u001b[A\n",
            "\n",
            " 64%|██████▎   | 637/1000 [00:14<00:14, 25.20it/s]\u001b[A\u001b[A\n",
            "\n",
            " 64%|██████▍   | 640/1000 [00:14<00:14, 24.43it/s]\u001b[A\u001b[A\n",
            "\n",
            " 64%|██████▍   | 643/1000 [00:15<00:14, 23.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 65%|██████▍   | 646/1000 [00:15<00:14, 23.71it/s]\u001b[A\u001b[A\n",
            "\n",
            " 65%|██████▍   | 649/1000 [00:15<00:15, 23.38it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " <RECIPE_START> <INPUT_START> noodle <NEXT_INPUT>  cakepotatoes <NEXT_INPUT> green of onion <NEXT_INPUT> cocktail sauce <NEXT_INPUT> firm tofu <NEXT_INPUT> bitters <NEXT_INPUT> fresh ginger <NEXT_INPUT> salt <NEXT_INPUT> rice wine vinegar <NEXT_INPUT> fish sauce <NEXT_INPUT> unbleached white pepper <NEXT_INPUT> saffron thread <NEXT_INPUT> egg <NEXT_INPUT> Vegetable cooking spray <INPUT_END> <INGR_START> 1/2 cup dijon-style medium-grain rice noodles <NEXT_INGR> 2 large potatoes, peeled and cut in 1/2-inch cubes <NEXT_INGR> 1 cup thinly sliced green of onion <NEXT_INGR> 2 ounces cocktail sauce (such as German Kimchi(R)) <NEXT_INGR> 11 ounces firm tofu, chopped <NEXT_INGR> 2 12 slices firm tofu, halved <NEXT_INGR> 4 strips whole mange (about 12 cups) <NEXT_INGR> 1 tablespoon chopped fresh ginger <NEXT_INGR> 1 14 teaspoons salt <NEXT_INGR> 1 14 cups rice wine vinegar <NEXT_INGR> 1 tablespoon fish sauce <NEXT_INGR> 14 teaspoon unsalted white pepper <NEXT_INGR> 1 pinch pinch saffron thread <NEXT_INGR> 1 egg white, lightly beaten <NEXT_INGR> Vegetable cooking spray <INGR_END> <INSTR_START> Place the noodles in a large ovenproof bowl, place under the broiler, and cover with a damp cloth to keep warm. (About 15 minutes). <NEXT_INSTR> Grease the bottoms of a 6-quart pot sufficient to hold the noodles. <NEXT_INSTR> Toss the rice noodles in the rice vinegar, the prepared cocktail sauce, the cubed tofu, the mange, and sliced ginger. <NEXT_INSTR> To make the dressing, combine the vinegar, fish sauce, white pepper, saffron, and egg white on a plate; sprinkle lightly with egg white. <NEXT_INSTR> Drizzle generously with the dressing over the noodles. <NEXT_INSTR> Bring a medium saucepan of lightly salted water to a boil. <NEXT_INSTR> Add the strawberries, and cook until the tender green leaves begin to wilt but are still red, 13 to 14 minutes. <NEXT_INSTR> Drain, reserving the syrup. <NEXT_INSTR> Cook the mange in a large nonstick skillet over medium-low heat 30 seconds, stirring twice, until it loses its pink color. <NEXT_INSTR> Add the reserved syrup, and cook until it wilts. <NEXT_INSTR> Whisk in the reserved plum syrup. <NEXT_INSTR> Sprinkle the noodles with the sweeter brown sugar syrup. <NEXT_INSTR> Place the hamburger buns on a hot baking sheet, and put the slices on already studded baking sheets over hot water (they should be slightly overlapping toward the edges). <NEXT_INSTR> Bake, uncovered, at 375°-375°, until the meat is cooked through, 5 to 7 minutes. <NEXT_INSTR> Note: The buns should be seasoned with salt and pepper. <NEXT_INSTR> Remove the burgers from the buns, sprinkle the tomato tomatoes with its juice, and return the hot buns to the saucepan. <NEXT_INSTR> Bring the buns up to a boil, and reduce them to about 8 butterful (1 1/2 cups) hamburger buns. <NEXT_INSTR> Divide the vegetable oil in half, the fat from the bottom side of a baking sheet, and the remaining fat from the top 1/2 to 3 tablespoons. <NEXT_INSTR> Heat the dish. <NEXT_INSTR> Place a skillet of lightly salted water atop the burgers; quickly flip dogs over, then cook until dipping meat rolls no longer lift, tossing the buns around constantly, only add the fat and oil to the sauces over the hot dogs. <INSTR_END> <TITLE_START> Fruity Tofu-Asian Hamburgers <TITLE_END> <RECIPE_END>\n",
            "#  Fruity Tofu-Asian Hamburgers   #\n",
            "  ## Input ingredients ##\n",
            "`noodle`\n",
            "`cakepotatoes`\n",
            "`green of onion`\n",
            "`cocktail sauce`\n",
            "`firm tofu`\n",
            "`bitters`\n",
            "`fresh ginger`\n",
            "`salt`\n",
            "`rice wine vinegar`\n",
            "`fish sauce`\n",
            "`unbleached white pepper`\n",
            "`saffron thread`\n",
            "`egg`\n",
            "`Vegetable cooking spray`\n",
            " ## Ingredients ##\n",
            "*  1/2 cup dijon-style medium-grain rice noodles \n",
            "*  2 large potatoes, peeled and cut in 1/2-inch cubes \n",
            "*  1 cup thinly sliced green of onion \n",
            "*  2 ounces cocktail sauce (such as German Kimchi(R)) \n",
            "*  11 ounces firm tofu, chopped \n",
            "*  2 12 slices firm tofu, halved \n",
            "*  4 strips whole mange (about 12 cups) \n",
            "*  1 tablespoon chopped fresh ginger \n",
            "*  1 14 teaspoons salt \n",
            "*  1 14 cups rice wine vinegar \n",
            "*  1 tablespoon fish sauce \n",
            "*  14 teaspoon unsalted white pepper \n",
            "*  1 pinch pinch saffron thread \n",
            "*  1 egg white, lightly beaten \n",
            "*  Vegetable cooking spray \n",
            " ## Instructions ##\n",
            " Place the noodles in a large ovenproof bowl, place under the broiler, and cover with a damp cloth to keep warm. (About 15 minutes). \n",
            " Grease the bottoms of a 6-quart pot sufficient to hold the noodles. \n",
            " Toss the rice noodles in the rice vinegar, the prepared cocktail sauce, the cubed tofu, the mange, and sliced ginger. \n",
            " To make the dressing, combine the vinegar, fish sauce, white pepper, saffron, and egg white on a plate; sprinkle lightly with egg white. \n",
            " Drizzle generously with the dressing over the noodles. \n",
            " Bring a medium saucepan of lightly salted water to a boil. \n",
            " Add the strawberries, and cook until the tender green leaves begin to wilt but are still red, 13 to 14 minutes. \n",
            " Drain, reserving the syrup. \n",
            " Cook the mange in a large nonstick skillet over medium-low heat 30 seconds, stirring twice, until it loses its pink color. \n",
            " Add the reserved syrup, and cook until it wilts. \n",
            " Whisk in the reserved plum syrup. \n",
            " Sprinkle the noodles with the sweeter brown sugar syrup. \n",
            " Place the hamburger buns on a hot baking sheet, and put the slices on already studded baking sheets over hot water (they should be slightly overlapping toward the edges). \n",
            " Bake, uncovered, at 375°-375°, until the meat is cooked through, 5 to 7 minutes. \n",
            " Note: The buns should be seasoned with salt and pepper. \n",
            " Remove the burgers from the buns, sprinkle the tomato tomatoes with its juice, and return the hot buns to the saucepan. \n",
            " Bring the buns up to a boil, and reduce them to about 8 butterful (1 1/2 cups) hamburger buns. \n",
            " Divide the vegetable oil in half, the fat from the bottom side of a baking sheet, and the remaining fat from the top 1/2 to 3 tablespoons. \n",
            " Heat the dish. \n",
            " Place a skillet of lightly salted water atop the burgers; quickly flip dogs over, then cook until dipping meat rolls no longer lift, tossing the buns around constantly, only add the fat and oil to the sauces over the hot dogs. \n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qif-sI6Qml2k"
      },
      "source": [
        "# Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRVTGlAcnPU6"
      },
      "source": [
        "# gold standard\n",
        "#df_eval = gold_st\n",
        "#df_eval.reset_index(drop=True, inplace=True)\n",
        "df_eval = pd.read_csv(\"./gdrive/MyDrive/COMP0087/350k_GPT-2_Score.csv\")\n",
        "df_eval_ner = df_eval[\"NER\"]  #NER\n",
        "df_eval_dir = df_eval[\"directions\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiztKgA6Ks7P"
      },
      "source": [
        "def get_raw_input(NER):\n",
        "    test_str = NER\n",
        "    test_str = test_str.replace(\"[\",\"\")\n",
        "    test_str = test_str.replace(\"]\",\"\")\n",
        "    test_str = test_str.replace(\"\\\"\",\"\")\n",
        "\n",
        "    return test_str\n",
        "\n",
        "def get_instr(markdown):\n",
        "  markdown = markdown.split(\"\\n\")\n",
        "  if ' ## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index(' ## Instructions ##')\n",
        "    \n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "\n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "    \n",
        "    return output\n",
        "\n",
        "  elif '## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index('## Instructions ##')\n",
        "\n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "        \n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "\n",
        "    return output\n",
        "    \n",
        "  else:\n",
        "    return [\"failed to generate\"]\n",
        "\n",
        "\n",
        "def generate_recipe(raw_text):\n",
        "\n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "    context_tokens = tokenizer.encode(prepared_input)[0:-1]\n",
        "    out = sample_sequence(\n",
        "                model=model,\n",
        "                context=context_tokens,\n",
        "                tokenizer=tokenizer,\n",
        "                length=1000,\n",
        "                device = 'cuda' #args.device # 'cpu' #\n",
        "            )\n",
        "    out = out[0, len(context_tokens):].tolist()\n",
        "    text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n",
        "    if \"<RECIPE_END>\" not in text:\n",
        "      print(text)\n",
        "      print(\"Failed to generate, recipe's too long\")\n",
        "      full_text = prepared_input + text\n",
        "      print(full_text)\n",
        "      return generate_recipe(raw_text)\n",
        "    else:\n",
        "      full_text = prepared_input + text\n",
        "      print(full_text)\n",
        "      markdown = re.sub(\"<RECIPE_(START|END)>\", \"\", full_text)\n",
        "      recipe_n_title = markdown.split(\"<TITLE_START>\")\n",
        "      if len(recipe_n_title)<=1:\n",
        "        return generate_recipe(raw_text)\n",
        "      title = \"# \" + recipe_n_title[1].replace(\"<TITLE_END>\", \"\") + \" #\\n\"\n",
        "      markdown = recipe_n_title[0].replace(\"<INPUT_START>\", \"## Input ingredients ##\\n`\").replace(\"<INPUT_END>\", \"`\\n\")\n",
        "      markdown = markdown.replace(\"<NEXT_INPUT>\", \"`\\n`\").replace(\"<INGR_START>\", \"## Ingredients ##\\n* \").replace(\"<NEXT_INGR>\", \"\\n* \").replace(\"<INGR_END>\", \"\\n\")\n",
        "      #markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\\\"\").replace(\"<NEXT_INSTR>\", \"\\n\\\"\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\").replace(\"<NEXT_INSTR>\", \"\\n\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = re.sub(\"$ +#\", \"#\", markdown)\n",
        "      markdown = re.sub(\"( +`|` +)\", \"`\", markdown)\n",
        "  \n",
        "      return markdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIsIqmgWe-b4"
      },
      "source": [
        "## directions v. testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr_7mrA0fFTb"
      },
      "source": [
        "directions =  [[] for i in range(10)]\n",
        "\n",
        "for i in range(10):  #100\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  \n",
        "  for j in range(10):\n",
        "    md = generate_recipe(raw_text)\n",
        "    directions[i].append(get_instr(md))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtHbY6nafi1G"
      },
      "source": [
        "## directions v. full"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2nrE8qaem6L"
      },
      "source": [
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "directions =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "\n",
        "  for j in range(replicated_size):\n",
        "    print(\"recipe: \",j)\n",
        "    md = generate_recipe(raw_text)\n",
        "    directions[i].append(get_instr(md))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "focF7aIUgb_4"
      },
      "source": [
        "## cos_sim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "537dDRYuQo9S"
      },
      "source": [
        "#func of cos_sim\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def get_cosine(vec1, vec2):\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "\n",
        "def text_to_vector(text):\n",
        "    word = re.compile(r'\\w+')\n",
        "    words = word.findall(text)\n",
        "    return Counter(words)\n",
        "\n",
        "\n",
        "def get_result(content_a, content_b):\n",
        "    text1 = content_a\n",
        "    text2 = content_b\n",
        "\n",
        "    vector1 = text_to_vector(text1)\n",
        "    vector2 = text_to_vector(text2)\n",
        "\n",
        "    cosine_result = get_cosine(vector1, vector2)\n",
        "    return cosine_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h2yJtpTXwUg"
      },
      "source": [
        "#cosine_similarity\n",
        "\n",
        "def COS_SIM(df_eval_dir,directions):\n",
        "  \"\"\"\n",
        "  df_eval_dir := gold_standard recipes\n",
        "  directions  := corresponding recipes \"\"\"\n",
        "\n",
        "  avg = 0\n",
        "\n",
        "  for i in range(len(directions)):\n",
        "    best = 0\n",
        "    for j in range(len(directions[i])):\n",
        "      cos = get_result(\" \".join(eval(df_eval_dir[i])),\n",
        "                      \" \".join(directions[i][j]))\n",
        "      best = max(best, cos)\n",
        "\n",
        "    avg += best\n",
        "\n",
        "  avg = avg/len(directions)\n",
        "\n",
        "  print(\"avg:\", avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up2NvAQ_imbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90cc269d-a03f-4603-c72d-94aaa3c3bc76"
      },
      "source": [
        "COS_SIM(df_eval_dir,directions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg: 0.5421947290378641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_H7VVRegge6"
      },
      "source": [
        "## bleu/gleu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZRt1Wb1LVFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea6d54c-27dc-4398-8012-a901be20747f"
      },
      "source": [
        "pip install jiwer==2.2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jiwer==2.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/cc/fb9d3132cba1f6d393b7d5a9398d9d4c8fc033bc54668cf87e9b197a6d7a/jiwer-2.2.0-py3-none-any.whl\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/dc/97f2b63ef0fa1fd78dcb7195aca577804f6b2b51e712516cc0e902a9a201/python-Levenshtein-0.12.2.tar.gz (50kB)\n",
            "\r\u001b[K     |██████▌                         | 10kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20kB 21.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 30kB 25.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 40kB 28.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jiwer==2.2.0) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein->jiwer==2.2.0) (56.1.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149811 sha256=23e7e316b38362b3a5432ca96c49db6b25d28e46b48b9f22a28c3b05c2e2a78a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/26/73/4b48503bac73f01cf18e52cd250947049a7f339e940c5df8fc\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein, jiwer\n",
            "Successfully installed jiwer-2.2.0 python-Levenshtein-0.12.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToQ6JDKvLWmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8eeaff-0a37-4fb1-c0eb-890dfb4d2c9c"
      },
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 23.0MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 28.0MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 26.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 17.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 14.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 15.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 15.3MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 15.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 16.3MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 16.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 16.3MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 16.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 16.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 16.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 16.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 16.3MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 16.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 16.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 16.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 327kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 337kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 624kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 645kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 655kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 665kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 675kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 931kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 942kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 962kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 972kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 983kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 993kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0MB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0MB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3MB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3MB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3MB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3MB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4MB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4MB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4MB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 16.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (8.0.0)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cLhCdUH8u5C"
      },
      "source": [
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "import nltk.translate.gleu_score as gleu\n",
        "import nltk.translate.meteor_score as meteor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDaVQ870jD0p"
      },
      "source": [
        "# Helper Func.\n",
        "def list_to_words(recipe):\n",
        "  words = []\n",
        "  for i in recipe:\n",
        "    words += i.split()\n",
        "\n",
        "  return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJQPZb6EnYr1"
      },
      "source": [
        "# New BLEU/GLEU\n",
        "def bleu_score(recipe, refer):\n",
        "    hyp = list_to_words(eval(recipe))\n",
        "    refs = []\n",
        "    for i in refer:\n",
        "        refs.append(list_to_words(i))\n",
        "\n",
        "    smoothie = SmoothingFunction().method5\n",
        "    score_ref_a = bleu.sentence_bleu(refs, hyp, smoothing_function=smoothie, weights=(1, 0, 0, 0))\n",
        "    #score_ref_a = bleu.sentence_bleu(refs, hyp, smoothing_function=smoothie, weights=(.25, 0.25, 0.25, 0.25))\n",
        "    return score_ref_a\n",
        "\n",
        "def gleu_score(recipe, refer):\n",
        "    hyp = list_to_words(eval(recipe))\n",
        "    refs = []\n",
        "    for i in refer:\n",
        "        refs.append(list_to_words(i))\n",
        "\n",
        "    score_ref_a = gleu.sentence_gleu(refs, hyp)\n",
        "    return score_ref_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpuD3icRn5I5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d02991-9170-4ea2-87e8-228a6b4cb83c"
      },
      "source": [
        "bleu_avg = []\n",
        "gleu_avg = []\n",
        "\n",
        "for i in range(len(directions)):\n",
        "  # print(bleu_score(df_eval_dir[i], directions[i]))\n",
        "  bleu_avg.append(bleu_score(df_eval_dir[i], directions[i]))\n",
        "\n",
        "for i in range(len(directions)):\n",
        "  gleu_avg.append(gleu_score(df_eval_dir[i], directions[i]))\n",
        "print(np.mean(bleu_avg))\n",
        "print(np.mean(gleu_avg))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7739315958814414\n",
            "0.09064334028913239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr6NXkL5s31Z"
      },
      "source": [
        "## Old BLEU/GLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAdhUXeH7gUE"
      },
      "source": [
        "def bleu_score(recipe, refer):\n",
        "    hyp = recipe\n",
        "    refs = refer\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    score_ref_a = bleu.sentence_bleu(refs, hyp, smoothing_function=smoothie)\n",
        "    return score_ref_a\n",
        "\n",
        "\n",
        "def gleu_score(recipe, refer):\n",
        "    hyp = recipe\n",
        "    refs = refer\n",
        "    score_ref_a = gleu.sentence_gleu([refs], hyp)\n",
        "    return score_ref_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq_pAXcl9EcS"
      },
      "source": [
        "ret_bleu = []\n",
        "ret_gleu = []\n",
        "#ret_wer = []\n",
        "\n",
        "for i in range(100):\n",
        "  \n",
        "  for j in range(10):\n",
        "\n",
        "    res_bleu = bleu_score(get_raw_input(df_eval_dir[i]),get_raw_input(directions[i][j]))\n",
        "\n",
        "    res_gleu = gleu_score(get_raw_input(df_eval_dir[i]),get_raw_input(directions[i][j]))\n",
        "\n",
        "    #res_wer = wer_score(get_raw_input(df_eval_dir[i]),get_raw_input(directions[i][j]))\n",
        "\n",
        "    ret_bleu.append(res_bleu)\n",
        "    ret_gleu.append(res_gleu)\n",
        "    #ret_wer.append(res_wer)\n",
        "\n",
        "print(np.mean(ret_bleu))\n",
        "print(np.mean(ret_gleu))\n",
        "#print(np.mean(ret_wer))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekdmfJo3DwjV"
      },
      "source": [
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU13u-zxAsaO"
      },
      "source": [
        "df_eval[\"GPT2-350K\"] = directions\n",
        "df_eval.to_csv(\"./gdrive/MyDrive/COMP0087/350k_GPT-2_Score.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixDQiUGmWJt9"
      },
      "source": [
        "#cccc\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEia0h9uQNMq"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "def wer(ref, hyp ,debug=False):\n",
        "    r = ref.split()\n",
        "    h = hyp.split()\n",
        "    #costs will holds the costs, like in the Levenshtein distance algorithm\n",
        "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
        "    # backtrace will hold the operations we've done.\n",
        "    # so we could later backtrace, like the WER algorithm requires us to.\n",
        "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
        "\n",
        "    OP_OK = 0\n",
        "    OP_SUB = 1\n",
        "    OP_INS = 2\n",
        "    OP_DEL = 3\n",
        "\n",
        "    DEL_PENALTY=1 # Tact\n",
        "    INS_PENALTY=1 # Tact\n",
        "    SUB_PENALTY=1 # Tact\n",
        "    # First column represents the case where we achieve zero\n",
        "    # hypothesis words by deleting all reference words.\n",
        "    for i in range(1, len(r)+1):\n",
        "        costs[i][0] = DEL_PENALTY*i\n",
        "        backtrace[i][0] = OP_DEL\n",
        "\n",
        "    # First row represents the case where we achieve the hypothesis\n",
        "    # by inserting all hypothesis words into a zero-length reference.\n",
        "    for j in range(1, len(h) + 1):\n",
        "        costs[0][j] = INS_PENALTY * j\n",
        "        backtrace[0][j] = OP_INS\n",
        "\n",
        "    # computation\n",
        "    for i in range(1, len(r)+1):\n",
        "        for j in range(1, len(h)+1):\n",
        "            if r[i-1] == h[j-1]:\n",
        "                costs[i][j] = costs[i-1][j-1]\n",
        "                backtrace[i][j] = OP_OK\n",
        "            else:\n",
        "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
        "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
        "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
        "\n",
        "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
        "                if costs[i][j] == substitutionCost:\n",
        "                    backtrace[i][j] = OP_SUB\n",
        "                elif costs[i][j] == insertionCost:\n",
        "                    backtrace[i][j] = OP_INS\n",
        "                else:\n",
        "                    backtrace[i][j] = OP_DEL\n",
        "\n",
        "    # back trace though the best route:\n",
        "    i = len(r)\n",
        "    j = len(h)\n",
        "    numSub = 0\n",
        "    numDel = 0\n",
        "    numIns = 0\n",
        "    numCor = 0\n",
        "    if debug:\n",
        "        print(\"OP\\tREF\\tHYP\")\n",
        "        lines = []\n",
        "    while i > 0 or j > 0:\n",
        "        if backtrace[i][j] == OP_OK:\n",
        "            numCor += 1\n",
        "            i-=1\n",
        "            j-=1\n",
        "            if debug:\n",
        "                lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
        "        elif backtrace[i][j] == OP_SUB:\n",
        "            numSub +=1\n",
        "            i-=1\n",
        "            j-=1\n",
        "            if debug:\n",
        "                lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
        "        elif backtrace[i][j] == OP_INS:\n",
        "            numIns += 1\n",
        "            j-=1\n",
        "            if debug:\n",
        "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
        "        elif backtrace[i][j] == OP_DEL:\n",
        "            numDel += 1\n",
        "            i-=1\n",
        "            if debug:\n",
        "                lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
        "    if debug:\n",
        "        lines = reversed(lines)\n",
        "        for line in lines:\n",
        "            print(line)\n",
        "        print(\"Ncor \" + str(numCor))\n",
        "        print(\"Nsub \" + str(numSub))\n",
        "        print(\"Ndel \" + str(numDel))\n",
        "        print(\"Nins \" + str(numIns))\n",
        "    return (numSub + numDel + numIns) / (float) (len(r))\n",
        "    wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
        "    return {'WER':wer_result, 'Cor':numCor, 'Sub':numSub, 'Ins':numIns, 'Del':numDel}\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umb_e5LwRLur"
      },
      "source": [
        "'''\n",
        "def wer(r, h):\n",
        "    \"\"\"\n",
        "    Calculation of WER with Levenshtein distance.\n",
        "\n",
        "    Works only for iterables up to 254 elements (uint8).\n",
        "    O(nm) time ans space complexity.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    r : list\n",
        "    h : list\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> wer(\"who is there\".split(), \"is there\".split())\n",
        "    1\n",
        "    >>> wer(\"who is there\".split(), \"\".split())\n",
        "    3\n",
        "    >>> wer(\"\".split(), \"who is there\".split())\n",
        "    3\n",
        "    \"\"\"\n",
        "    # initialisation\n",
        "    import numpy\n",
        "    d = numpy.zeros((len(r)+1)*(len(h)+1), dtype=numpy.uint8)\n",
        "    d = d.reshape((len(r)+1, len(h)+1))\n",
        "    for i in range(len(r)+1):\n",
        "        for j in range(len(h)+1):\n",
        "            if i == 0:\n",
        "                d[0][j] = j\n",
        "            elif j == 0:\n",
        "                d[i][0] = i\n",
        "\n",
        "    # computation\n",
        "    for i in range(1, len(r)+1):\n",
        "        for j in range(1, len(h)+1):\n",
        "            if r[i-1] == h[j-1]:\n",
        "                d[i][j] = d[i-1][j-1]\n",
        "            else:\n",
        "                substitution = d[i-1][j-1] + 1\n",
        "                insertion    = d[i][j-1] + 1\n",
        "                deletion     = d[i-1][j] + 1\n",
        "                d[i][j] = min(substitution, insertion, deletion)\n",
        "\n",
        "    return d[len(r)][len(h)]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import doctest\n",
        "    doctest.testmod()\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}