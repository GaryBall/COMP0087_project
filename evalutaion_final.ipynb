{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evalutaion_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fda850cb82bc48549a54c3ecdb767f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c2b06e7ef6ff4b58af9c03648d43f3b0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6ffb7cdaf5464d5f8974342fcd290827",
              "IPY_MODEL_08041175f26441a5b75833955f45991b"
            ]
          }
        },
        "c2b06e7ef6ff4b58af9c03648d43f3b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ffb7cdaf5464d5f8974342fcd290827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_276e93dd17fe48a8b68781943f133030",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1682,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1682,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ccb327cfa63c477ca58d664a3ae2fea5"
          }
        },
        "08041175f26441a5b75833955f45991b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ca33dd0c6e44d2bb58f097466a4b7a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.68k/1.68k [00:00&lt;00:00, 55.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_267cc56b5b6f4ef0931214d129ddc827"
          }
        },
        "276e93dd17fe48a8b68781943f133030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ccb327cfa63c477ca58d664a3ae2fea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ca33dd0c6e44d2bb58f097466a4b7a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "267cc56b5b6f4ef0931214d129ddc827": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6461871414dc42c3a34d6b5e765e1d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_800f8cd7fa3545c0acce5bd88bc157e1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9c93d708cbac403dbdde21cc31103032",
              "IPY_MODEL_f5b0e3dc9c1342488fcaf40928681f9d"
            ]
          }
        },
        "800f8cd7fa3545c0acce5bd88bc157e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c93d708cbac403dbdde21cc31103032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b40de920da124ba6849a899fb672e194",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 384515855,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 384515855,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b8d28c261824b0b91fd49169cd965c8"
          }
        },
        "f5b0e3dc9c1342488fcaf40928681f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_be9a73f68ed34b2e9471d450000d7eb7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 385M/385M [00:14&lt;00:00, 26.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c1afdb2aa45d4cc180f9d426d8146ae2"
          }
        },
        "b40de920da124ba6849a899fb672e194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b8d28c261824b0b91fd49169cd965c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be9a73f68ed34b2e9471d450000d7eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c1afdb2aa45d4cc180f9d426d8146ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0965d77dd2fb48f79a75ce89ba9478f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6a935431775b4f4bac600807111645a6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_580a44db562b481aaa4cf216c0204ac6",
              "IPY_MODEL_eaa52ae38bb94e0eb42e5035e0e102e9"
            ]
          }
        },
        "6a935431775b4f4bac600807111645a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "580a44db562b481aaa4cf216c0204ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ca571001c23d4f3b81485c24d21a6ef2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 850,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 850,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_058330f04cc543a18bfbfc147c614885"
          }
        },
        "eaa52ae38bb94e0eb42e5035e0e102e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_177a3febd2c54306a6fbd6ba12e6b648",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 850/850 [00:00&lt;00:00, 962B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17072019b61f406480c1754043f985b5"
          }
        },
        "ca571001c23d4f3b81485c24d21a6ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "058330f04cc543a18bfbfc147c614885": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "177a3febd2c54306a6fbd6ba12e6b648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17072019b61f406480c1754043f985b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f80d9e6da04410d9718bcd923cef8f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a79b8f0f8bce44a383c7340b2a9a1855",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_721c1229845a468ebad562c075176fa0",
              "IPY_MODEL_6365239082944b3894e0cfc7b8274e57"
            ]
          }
        },
        "a79b8f0f8bce44a383c7340b2a9a1855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "721c1229845a468ebad562c075176fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_abc749df653042fda20b0615c3a21f99",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 510445819,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 510445819,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4df3c7aad34043c68194e76709157a20"
          }
        },
        "6365239082944b3894e0cfc7b8274e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_52fd6b17ed404052890b575ebf0827dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 510M/510M [00:10&lt;00:00, 50.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4f1b7f6e295f4710a45b0b43bace6540"
          }
        },
        "abc749df653042fda20b0615c3a21f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4df3c7aad34043c68194e76709157a20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52fd6b17ed404052890b575ebf0827dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4f1b7f6e295f4710a45b0b43bace6540": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aac32707b11b4edbaac9a5f4c218c47a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_16dd9aad0ff544e3add106ba752fa38d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_018c3da47c8b4621be7fdb7624728e13",
              "IPY_MODEL_d705c8a48ac14834ae356425928a24e1"
            ]
          }
        },
        "16dd9aad0ff544e3add106ba752fa38d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "018c3da47c8b4621be7fdb7624728e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_25a92977f5c64e709f4f970ebad42efa",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898669,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898669,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38d4311e7efd4f8bb9bfe055d748728d"
          }
        },
        "d705c8a48ac14834ae356425928a24e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2a078fcfc6ec43409ff7410326b33ccb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:01&lt;00:00, 476kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15068145c056406bac9fffdfed70994a"
          }
        },
        "25a92977f5c64e709f4f970ebad42efa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38d4311e7efd4f8bb9bfe055d748728d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a078fcfc6ec43409ff7410326b33ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15068145c056406bac9fffdfed70994a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21713835b1164ae09d3c69c0e0e4eb87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b0b3d4dc0d42429ca869bd9718809c0d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8749f501f1c04f298fedd0dae03522e3",
              "IPY_MODEL_22fdfdc4046f4d8c8c2a308e1312d01a"
            ]
          }
        },
        "b0b3d4dc0d42429ca869bd9718809c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8749f501f1c04f298fedd0dae03522e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14a7b7e193d346329dfdbbfc1be44faa",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ccad7f4a213d4d9ebbbe806bc9b343e9"
          }
        },
        "22fdfdc4046f4d8c8c2a308e1312d01a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c6bc4f9f8bc6487690297c24ea7017bf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 494kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d3a52d09a34f4248a0d1108966bf9c55"
          }
        },
        "14a7b7e193d346329dfdbbfc1be44faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ccad7f4a213d4d9ebbbe806bc9b343e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6bc4f9f8bc6487690297c24ea7017bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d3a52d09a34f4248a0d1108966bf9c55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89445521f23b45b99b42c7fd2bf76ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2aea2b0b79934e8bb2acdb6c6916d69e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f2f585c63d064cc098d31eb4c9524db5",
              "IPY_MODEL_7c4911ad5f2344ec91c3e186ebbc92f3"
            ]
          }
        },
        "2aea2b0b79934e8bb2acdb6c6916d69e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2f585c63d064cc098d31eb4c9524db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a3da24dd051549fc9bacbf10611390fb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 298,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 298,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c111a8d90354352ab48eb73c950a633"
          }
        },
        "7c4911ad5f2344ec91c3e186ebbc92f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd5ef55cc0dc4612879eae44284cc646",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 298/298 [00:01&lt;00:00, 154B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d0d371225d514743b1266b712955dbf5"
          }
        },
        "a3da24dd051549fc9bacbf10611390fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c111a8d90354352ab48eb73c950a633": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd5ef55cc0dc4612879eae44284cc646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d0d371225d514743b1266b712955dbf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07be7d3d6a224b82bcfe0a1370ee22be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8ea4dd05ac7b4e35b524c64238d05880",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_133c9773aa6b47e0acd40ceb560c1986",
              "IPY_MODEL_2d67a7431ed4467a8977cc3186943ed8"
            ]
          }
        },
        "8ea4dd05ac7b4e35b524c64238d05880": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "133c9773aa6b47e0acd40ceb560c1986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0d749b91656e4049a353356efb295356",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8e64a141de9442e94a918275f05f0dd"
          }
        },
        "2d67a7431ed4467a8977cc3186943ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d8432daaedb844dc9b3f7cbc66f83178",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 595/595 [00:01&lt;00:00, 515B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db62dcda26e24dca900fedcaf234a3d2"
          }
        },
        "0d749b91656e4049a353356efb295356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8e64a141de9442e94a918275f05f0dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d8432daaedb844dc9b3f7cbc66f83178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db62dcda26e24dca900fedcaf234a3d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4a01714900141c4a69f67dd61182eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_19406569b2be421a9b8cdcd3756dc9d7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a23ef236b275404cbee1d710728aaede",
              "IPY_MODEL_3131e0897a0b427b91ba02a333078a72"
            ]
          }
        },
        "19406569b2be421a9b8cdcd3756dc9d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a23ef236b275404cbee1d710728aaede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0033d6adad40427c8a5dcea360c5bb4a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 780,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 780,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9f5ecac0f8b4003b8f67c28af6c36ad"
          }
        },
        "3131e0897a0b427b91ba02a333078a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_10f2474dd4fd454aac9194a7022e20a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 780/780 [00:00&lt;00:00, 1.86kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_359bccad9205477cb512d03c4017eae0"
          }
        },
        "0033d6adad40427c8a5dcea360c5bb4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9f5ecac0f8b4003b8f67c28af6c36ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10f2474dd4fd454aac9194a7022e20a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "359bccad9205477cb512d03c4017eae0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuHBMqJkcJyV"
      },
      "source": [
        "# Evaluation prearation\n",
        "\n",
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5HEQeFtdbz3",
        "outputId": "62c12fb3-ba14-431a-c057-1e713f015b41"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install boto3\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "!pip install jiwer==2.2.0\n",
        "!pip install -U nltk\n",
        "!pip install rouge/requirements.txt\n",
        "!pip install rouge-score\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .. \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.17.84)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.84 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.20.84)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.4.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.84->boto3) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.84->boto3) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.84->boto3) (1.15.0)\n",
            "fatal: destination path 'apex' already exists and is not an empty directory.\n",
            "Requirement already satisfied: jiwer==2.2.0 in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jiwer==2.2.0) (1.19.5)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (from jiwer==2.2.0) (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein->jiwer==2.2.0) (56.1.0)\n",
            "Requirement already up-to-date: nltk in /usr/local/lib/python3.7/dist-packages (3.6.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "\u001b[31mERROR: Invalid requirement: 'rouge/requirements.txt'\n",
            "Hint: It looks like a path. File 'rouge/requirements.txt' does not exist.\u001b[0m\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.6.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (7.1.2)\n",
            "/content/apex\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-52qkbt5a\n",
            "Created temporary directory: /tmp/pip-req-tracker-0pojv8_m\n",
            "Created requirements tracker '/tmp/pip-req-tracker-0pojv8_m'\n",
            "Created temporary directory: /tmp/pip-install-ou0y4ipu\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-51vtyh8q\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-0pojv8_m'\n",
            "    Running setup.py (path:/tmp/pip-req-build-51vtyh8q/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.8.1+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-51vtyh8q/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-51vtyh8q/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-51vtyh8q/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-51vtyh8q/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-51vtyh8q/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n",
            "    writing manifest file '/tmp/pip-req-build-51vtyh8q/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-51vtyh8q/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-51vtyh8q has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-0pojv8_m'\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-8oaejeko\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-8oaejeko\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-51vtyh8q/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-51vtyh8q/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-8oaejeko --python-tag cp37\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.8.1+cu101\n",
            "\n",
            "\n",
            "  /tmp/pip-req-build-51vtyh8q/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/pyprof\n",
            "  copying apex/pyprof/__init__.py -> build/lib/apex/pyprof\n",
            "  creating build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/__init__.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/reparameterization.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/weight_norm.py -> build/lib/apex/reparameterization\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/__init__.py -> build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/nvmarker.py -> build/lib/apex/pyprof/nvtx\n",
            "  creating build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/loss.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/dropout.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/softmax.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/usage.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/output.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/conv.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__main__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/base.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/utility.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/embedding.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/recurrentCell.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/prof.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pointwise.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/reduction.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/activation.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__init__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pooling.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/normalization.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/misc.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/linear.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/randomSample.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/data.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/convert.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/blas.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/optim.py -> build/lib/apex/pyprof/prof\n",
            "  creating build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/db.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/kernel.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/nvvp.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__main__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__init__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/parse.py -> build/lib/apex/pyprof/parse\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-8oaejeko/apex-0.1-cp37-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v2.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v3.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/pyprof/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/nvmarker.py'\n",
            "  adding 'apex/pyprof/parse/__init__.py'\n",
            "  adding 'apex/pyprof/parse/__main__.py'\n",
            "  adding 'apex/pyprof/parse/db.py'\n",
            "  adding 'apex/pyprof/parse/kernel.py'\n",
            "  adding 'apex/pyprof/parse/nvvp.py'\n",
            "  adding 'apex/pyprof/parse/parse.py'\n",
            "  adding 'apex/pyprof/prof/__init__.py'\n",
            "  adding 'apex/pyprof/prof/__main__.py'\n",
            "  adding 'apex/pyprof/prof/activation.py'\n",
            "  adding 'apex/pyprof/prof/base.py'\n",
            "  adding 'apex/pyprof/prof/blas.py'\n",
            "  adding 'apex/pyprof/prof/conv.py'\n",
            "  adding 'apex/pyprof/prof/convert.py'\n",
            "  adding 'apex/pyprof/prof/data.py'\n",
            "  adding 'apex/pyprof/prof/dropout.py'\n",
            "  adding 'apex/pyprof/prof/embedding.py'\n",
            "  adding 'apex/pyprof/prof/index_slice_join_mutate.py'\n",
            "  adding 'apex/pyprof/prof/linear.py'\n",
            "  adding 'apex/pyprof/prof/loss.py'\n",
            "  adding 'apex/pyprof/prof/misc.py'\n",
            "  adding 'apex/pyprof/prof/normalization.py'\n",
            "  adding 'apex/pyprof/prof/optim.py'\n",
            "  adding 'apex/pyprof/prof/output.py'\n",
            "  adding 'apex/pyprof/prof/pointwise.py'\n",
            "  adding 'apex/pyprof/prof/pooling.py'\n",
            "  adding 'apex/pyprof/prof/prof.py'\n",
            "  adding 'apex/pyprof/prof/randomSample.py'\n",
            "  adding 'apex/pyprof/prof/recurrentCell.py'\n",
            "  adding 'apex/pyprof/prof/reduction.py'\n",
            "  adding 'apex/pyprof/prof/softmax.py'\n",
            "  adding 'apex/pyprof/prof/usage.py'\n",
            "  adding 'apex/pyprof/prof/utility.py'\n",
            "  adding 'apex/reparameterization/__init__.py'\n",
            "  adding 'apex/reparameterization/reparameterization.py'\n",
            "  adding 'apex/reparameterization/weight_norm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp37-none-any.whl size=204691 sha256=2410fc2997c265f1a9b676133f65e12e4245a8a4f63cc054e0d3c8c493256f54\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-52qkbt5a/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "  Removing source in /tmp/pip-req-build-51vtyh8q\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Found existing installation: apex 0.1\n",
            "    Uninstalling apex-0.1:\n",
            "      Created temporary directory: /usr/local/lib/python3.7/dist-packages/~pex-0.1.dist-info\n",
            "      Removing file or directory /usr/local/lib/python3.7/dist-packages/apex-0.1.dist-info/\n",
            "      Created temporary directory: /usr/local/lib/python3.7/dist-packages/~pex\n",
            "      Removing file or directory /usr/local/lib/python3.7/dist-packages/apex/\n",
            "      Successfully uninstalled apex-0.1\n",
            "\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-0pojv8_m'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsmxZM0-x7fF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d6f64c-abca-44a5-eedb-f7239f97397d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3gLMKblgV2b",
        "outputId": "4f8c4348-d76a-4a6a-da17-1a93d90fbe27"
      },
      "source": [
        "# data preprocessing\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import json\n",
        "import h5py\n",
        "\n",
        "# control\n",
        "import argparse\n",
        "import logging\n",
        "from tqdm import trange\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# transformers\n",
        "from transformers import GPT2Config\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import BartForCausalLM, BartTokenizer,BartConfig"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz7e2PoybaiE"
      },
      "source": [
        "# fine-tuning model with bart\n",
        "# logging info\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import random\n",
        "import gc\n",
        "import boto3\n",
        "import shutil"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5MKxGg8bhj4"
      },
      "source": [
        "from tqdm import tqdm, trange\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "from transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import BartForCausalLM, BartTokenizer,BartConfig"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQSBz5mG77Ic"
      },
      "source": [
        "## data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEAelfO1BwUd"
      },
      "source": [
        "# load an independent test set that previously saved\n",
        "df_eval = pd.read_csv('./gdrive/MyDrive/COMP0087_v7/350k_GPT-2_Score.csv')\n",
        "df_eval_ner = df_eval[\"NER\"]  #NER\n",
        "df_eval_dir = df_eval[\"directions\"]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWI6m5-_64Ln",
        "outputId": "ca21d1eb-71d1-4f8a-ec3d-facaece7ca60"
      },
      "source": [
        "df_eval_dir"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     [\"Blend 1/2 quart dry milk, 14 ounces club sod...\n",
              "1     [\"Cook orzo in lightly salted water as package...\n",
              "2     [\"Mix corn, salt and milk together.\", \"Beat eg...\n",
              "3     [\"Mix cornstarch, sugar and salt.\", \"Add hot w...\n",
              "4     [\"Mix taco seasoning and refried beans to desi...\n",
              "                            ...                        \n",
              "95    [\"Beat egg substitute and add, in order, spice...\n",
              "96    [\"Pour all ingredients over chicken breasts in...\n",
              "97    [\"Put spices and sugar in basket of 8-cup perc...\n",
              "98    [\"Melt chocolate in boiling water.\", \"Cool.\", ...\n",
              "99    [\"Bring milk, shortening, cocoa, and sugar to ...\n",
              "Name: directions, Length: 100, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGWBEp9S8FSe"
      },
      "source": [
        "## generation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0UpZhdr7K48"
      },
      "source": [
        "def init_tokenizer(tokenizer_type = \"facebook/bart-base\"):\n",
        "  \n",
        "  tokenizer = BartTokenizer.from_pretrained(tokenizer_type)\n",
        "  special_tokens = {\n",
        "    \"additional_special_tokens\": [\n",
        "        \"<TITLE_START>\",\n",
        "        \"<TITLE_END>\",\n",
        "        \"<INSTR_START>\",\n",
        "        \"<NEXT_INSTR>\",\n",
        "        \"<INSTR_END>\",\n",
        "        \"<INGR_START>\",\n",
        "        \"<NEXT_INGR>\",\n",
        "        \"<INGR_END>\",\n",
        "        \"<RECIPE_START>\",\n",
        "        \"<RECIPE_END>\",\n",
        "        \"<INPUT_START>\",\n",
        "        \"<INPUT_END>\",\n",
        "        \"<NEXT_INPUT>\"\n",
        "      ]\n",
        "  }\n",
        "  tokenizer.add_special_tokens(special_tokens)\n",
        "  return tokenizer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edyp1__48Kdv"
      },
      "source": [
        "def set_seed(args):\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(model, length, context, tokenizer, num_samples=1, temperature=1, top_k=0, top_p=0.0, device='cpu'):\n",
        "    end_token = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
        "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            inputs = {'input_ids': generated}\n",
        "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
        "            # print(outputs)\n",
        "            next_token_logits = outputs[\"logits\"][0, -1, :] / temperature\n",
        "            \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
        "            #print(next_token)\n",
        "            if next_token.item() == end_token:\n",
        "                break\n",
        "    return generated"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Iv1hGHm8Gu1"
      },
      "source": [
        "def get_raw_input(NER):\n",
        "    test_str = NER\n",
        "    test_str = test_str.replace(\"[\",\"\")\n",
        "    test_str = test_str.replace(\"]\",\"\")\n",
        "    test_str = test_str.replace(\"\\\"\",\"\")\n",
        "\n",
        "    return test_str\n",
        "\n",
        "def get_instr(markdown):\n",
        "  markdown = markdown.split(\"\\n\")\n",
        "  if ' ## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index(' ## Instructions ##')\n",
        "    \n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "\n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "    \n",
        "    return output\n",
        "\n",
        "  elif '## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index('## Instructions ##')\n",
        "\n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "        \n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "\n",
        "    return output\n",
        "    \n",
        "  else:\n",
        "    return [\"failed to generate\"]\n",
        "\n",
        "\n",
        "def generate_recipe(raw_text):\n",
        "\n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "    context_tokens = tokenizer.encode(prepared_input)[0:-1]\n",
        "    out = sample_sequence(\n",
        "                model=model,\n",
        "                context=context_tokens,\n",
        "                tokenizer=tokenizer,\n",
        "                length=800,\n",
        "                device = 'cuda'\n",
        "            )\n",
        "    out = out[0, len(context_tokens):].tolist()\n",
        "    text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n",
        "    if \"<RECIPE_END>\" not in text:\n",
        "      # print(text)\n",
        "      print(\"Failed to generate, recipe's too long\")\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      return generate_recipe(raw_text)\n",
        "    else:\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      markdown = re.sub(\"<RECIPE_(START|END)>\", \"\", full_text)\n",
        "      recipe_n_title = markdown.split(\"<TITLE_START>\")\n",
        "      if len(recipe_n_title)<=1:\n",
        "        return generate_recipe(raw_text)\n",
        "      title = \"# \" + recipe_n_title[1].replace(\"<TITLE_END>\", \"\") + \" #\\n\"\n",
        "      markdown = recipe_n_title[0].replace(\"<INPUT_START>\", \"## Input ingredients ##\\n`\").replace(\"<INPUT_END>\", \"`\\n\")\n",
        "      markdown = markdown.replace(\"<NEXT_INPUT>\", \"`\\n`\").replace(\"<INGR_START>\", \"## Ingredients ##\\n* \").replace(\"<NEXT_INGR>\", \"\\n* \").replace(\"<INGR_END>\", \"\\n\")\n",
        "      #markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\\\"\").replace(\"<NEXT_INSTR>\", \"\\n\\\"\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\").replace(\"<NEXT_INSTR>\", \"\\n\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = re.sub(\"$ +#\", \"#\", markdown)\n",
        "      markdown = re.sub(\"( +`|` +)\", \"`\", markdown)\n",
        "  \n",
        "      return markdown"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUGnW9ug8JpC"
      },
      "source": [
        "## cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46H9vqmwHQOj"
      },
      "source": [
        "#func of cos_sim\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def get_cosine(vec1, vec2):\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "\n",
        "def text_to_vector(text):\n",
        "    word = re.compile(r'\\w+')\n",
        "    words = word.findall(text)\n",
        "    return Counter(words)\n",
        "\n",
        "\n",
        "def get_result(content_a, content_b):\n",
        "    text1 = content_a\n",
        "    text2 = content_b\n",
        "\n",
        "    vector1 = text_to_vector(text1)\n",
        "    vector2 = text_to_vector(text2)\n",
        "\n",
        "    cosine_result = get_cosine(vector1, vector2)\n",
        "    return cosine_result"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4rWzL5RHTxP"
      },
      "source": [
        "#cosine_similarity\n",
        "\n",
        "def COS_SIM(df_eval_dir,directions):\n",
        "  \"\"\"\n",
        "  df_eval_dir := gold_standard recipes\n",
        "  directions  := corresponding recipes \"\"\"\n",
        "\n",
        "  avg = 0\n",
        "\n",
        "  for i in range(len(directions)):\n",
        "    best = 0\n",
        "    for j in range(len(directions[i])):\n",
        "      cos = get_result(\" \".join(eval(df_eval_dir[i])),\n",
        "                      \" \".join(directions[i][j]))\n",
        "      best = max(best, cos)\n",
        "\n",
        "    avg += best\n",
        "\n",
        "  avg = avg/len(directions)\n",
        "  return avg"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1f4i3iiH7F_"
      },
      "source": [
        "## Bleu Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfxaEbKLHUU7"
      },
      "source": [
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "import nltk.translate.gleu_score as gleu\n",
        "import nltk.translate.meteor_score as meteor\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCb3bltrHUb4"
      },
      "source": [
        "# Helper Func.\n",
        "def list_to_words(recipe):\n",
        "  words = []\n",
        "  for i in recipe:\n",
        "    words += i.split()\n",
        "\n",
        "  return words\n",
        "# bleu score\n",
        "def bleu_score(recipe, refer):\n",
        "    hyp = list_to_words(eval(recipe))\n",
        "    refs = []\n",
        "    for i in refer:\n",
        "        refs.append(list_to_words(i))\n",
        "\n",
        "    smoothie = SmoothingFunction().method5\n",
        "    score_ref_a = bleu.sentence_bleu(refs, hyp, smoothing_function=smoothie, weights=(1, 0, 0, 0))\n",
        "    return score_ref_a\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpIvWnYUHYro"
      },
      "source": [
        "def bleu_cal(df_eval_dir, directions):\n",
        "  bleu_avg = []\n",
        "  for i in range(len(directions)):\n",
        "    bleu_avg.append(bleu_score(df_eval_dir[i], directions[i]))\n",
        "  return np.mean(bleu_avg)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFM43lh0H3VZ"
      },
      "source": [
        "## ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RApwjiOtHYtY"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def list_to_string(start_list):\n",
        "  string=''.join([str(item) for item in start_list])\n",
        "  return string\n",
        "\n",
        "def rouge_cal(df_eval_dir, directions, rouge_type = 'rougeL'):\n",
        "  rough_avg = []\n",
        "  scorer = rouge_scorer.RougeScorer([rouge_type], use_stemmer=True)\n",
        "\n",
        "  for i in range(len(df_eval_dir)):\n",
        "    for j in range(10):\n",
        "      scores= scorer.score(df_eval_dir[i], list_to_string(directions[i][j]))\n",
        "      rough_avg.append(list(scores.values())[0][0])\n",
        "  return np.mean(rough_avg)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfqa9Ldi8NI8"
      },
      "source": [
        "# model evaluation\n",
        "\n",
        "## BART1+GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIENuHZf8Mnn"
      },
      "source": [
        "# model = BartForCausalLM.from_pretrained(\"jky594176/BART1_GRU\")\n",
        "model = BartForCausalLM.from_pretrained(\"jky594176/recipe_BART1_GRU\")\n",
        "tokenizer = init_tokenizer(\"facebook/bart-base\")\n",
        "model = model.to('cuda')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skgTrNlX74Zn",
        "outputId": "86cca41a-91c5-4534-cd72-3880950d50cb"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "dir_bart1_gru =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "  print(\"step:\")\n",
        "  for j in range(replicated_size):\n",
        "    print(j, end = \", \")\n",
        "    md = generate_recipe(raw_text)\n",
        "    dir_bart1_gru[i].append(get_instr(md))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Generating the recipe:  0\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  1\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  2\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  3\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  4\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  5\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  6\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  7\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  8\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  9\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  10\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  11\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  12\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  13\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  14\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  15\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  16\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  17\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  18\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  19\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  20\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  21\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  22\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  23\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  24\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  25\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  26\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  27\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  28\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  29\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  30\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  31\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  32\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  33\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  34\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  35\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  36\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  37\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  38\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  39\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  40\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  41\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  42\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  43\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  44\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  45\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  46\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  47\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  48\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  49\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  50\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  51\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  52\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  53\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  54\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  55\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  56\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  57\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  58\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  59\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  60\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  61\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  62\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  63\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  64\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  65\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  66\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  67\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  68\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  69\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  70\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  71\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  72\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  73\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  74\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  75\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  76\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  77\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  78\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  79\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  80\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  81\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  82\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  83\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  84\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  85\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  86\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  87\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  88\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  89\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  90\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  91\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  92\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  93\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  94\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  95\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  96\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  97\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  98\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  99\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up2NvAQ_imbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea308e8-b831-438a-843a-dd3a90374945"
      },
      "source": [
        "print(\"Cos sim:\", COS_SIM(df_eval_dir, dir_bart1_gru))\n",
        "print(\"BLEU:\", bleu_cal(df_eval_dir, dir_bart1_gru))\n",
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, dir_bart1_gru,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, dir_bart1_gru,'rougeLsum'))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cos sim: 0.5643583126180023\n",
            "BLEU: 0.77208228998418\n",
            "rouge-1: 0.3565509156364043\n",
            "rouge-L: 0.2194203777934473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCVosSQ6GQm1"
      },
      "source": [
        "## BART1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9NcdS7IEphS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "fda850cb82bc48549a54c3ecdb767f95",
            "c2b06e7ef6ff4b58af9c03648d43f3b0",
            "6ffb7cdaf5464d5f8974342fcd290827",
            "08041175f26441a5b75833955f45991b",
            "276e93dd17fe48a8b68781943f133030",
            "ccb327cfa63c477ca58d664a3ae2fea5",
            "0ca33dd0c6e44d2bb58f097466a4b7a2",
            "267cc56b5b6f4ef0931214d129ddc827",
            "6461871414dc42c3a34d6b5e765e1d67",
            "800f8cd7fa3545c0acce5bd88bc157e1",
            "9c93d708cbac403dbdde21cc31103032",
            "f5b0e3dc9c1342488fcaf40928681f9d",
            "b40de920da124ba6849a899fb672e194",
            "8b8d28c261824b0b91fd49169cd965c8",
            "be9a73f68ed34b2e9471d450000d7eb7",
            "c1afdb2aa45d4cc180f9d426d8146ae2"
          ]
        },
        "outputId": "58eebcfd-4728-4f5d-f5e8-274c9f795958"
      },
      "source": [
        "model = BartForCausalLM.from_pretrained(\"jky594176/recipe_BART1\")\n",
        "model = model.to('cuda')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fda850cb82bc48549a54c3ecdb767f95",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1682.0, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6461871414dc42c3a34d6b5e765e1d67",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=384515855.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79tkk5uRGCpr",
        "outputId": "5a6a9780-f601-42bf-aa65-ee2d3888e962"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "dir_bart1 =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "  print(\"step:\")\n",
        "\n",
        "  for j in range(replicated_size):\n",
        "    print(j, end = \", \")\n",
        "    md = generate_recipe(raw_text)\n",
        "    dir_bart1[i].append(get_instr(md))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Generating the recipe:  0\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  1\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  2\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  3\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  4\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  5\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  6\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  7\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  8\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  9\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  10\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  11\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  12\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  13\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  14\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  15\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  16\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  17\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  18\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  19\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  20\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  21\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  22\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  23\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  24\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  25\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  26\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  27\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  28\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  29\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  30\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  31\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  32\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  33\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  34\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  35\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  36\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  37\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  38\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  39\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  40\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  41\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  42\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  43\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  44\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  45\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  46\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  47\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  48\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  49\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  50\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  51\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  52\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  53\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  54\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  55\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  56\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  57\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  58\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  59\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  60\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  61\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  62\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  63\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  64\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  65\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  66\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  67\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  68\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  69\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  70\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  71\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  72\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  73\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  74\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  75\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  76\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  77\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  78\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  79\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  80\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  81\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  82\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  83\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  84\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  85\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  86\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  87\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  88\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  89\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  90\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  91\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  92\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  93\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  94\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  95\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  96\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  97\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  98\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  99\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuUl9OaiGDhO",
        "outputId": "44cd80c3-3b6c-42e8-e0e4-a034c5281bda"
      },
      "source": [
        "print(\"Cos sim:\", COS_SIM(df_eval_dir, dir_bart1))\n",
        "print(\"BLEU:\", bleu_cal(df_eval_dir, dir_bart1))\n",
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, dir_bart1,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, dir_bart1,'rougeLsum'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cos sim: 0.5351948160757843\n",
            "BLEU: 0.760743964253477\n",
            "rouge-1: 0.3091233913976813\n",
            "rouge-L: 0.18639506343143267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aXGwoeWw7jw"
      },
      "source": [
        "## GPT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jtl_wx8xLipu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360,
          "referenced_widgets": [
            "0965d77dd2fb48f79a75ce89ba9478f8",
            "6a935431775b4f4bac600807111645a6",
            "580a44db562b481aaa4cf216c0204ac6",
            "eaa52ae38bb94e0eb42e5035e0e102e9",
            "ca571001c23d4f3b81485c24d21a6ef2",
            "058330f04cc543a18bfbfc147c614885",
            "177a3febd2c54306a6fbd6ba12e6b648",
            "17072019b61f406480c1754043f985b5",
            "2f80d9e6da04410d9718bcd923cef8f6",
            "a79b8f0f8bce44a383c7340b2a9a1855",
            "721c1229845a468ebad562c075176fa0",
            "6365239082944b3894e0cfc7b8274e57",
            "abc749df653042fda20b0615c3a21f99",
            "4df3c7aad34043c68194e76709157a20",
            "52fd6b17ed404052890b575ebf0827dd",
            "4f1b7f6e295f4710a45b0b43bace6540",
            "aac32707b11b4edbaac9a5f4c218c47a",
            "16dd9aad0ff544e3add106ba752fa38d",
            "018c3da47c8b4621be7fdb7624728e13",
            "d705c8a48ac14834ae356425928a24e1",
            "25a92977f5c64e709f4f970ebad42efa",
            "38d4311e7efd4f8bb9bfe055d748728d",
            "2a078fcfc6ec43409ff7410326b33ccb",
            "15068145c056406bac9fffdfed70994a",
            "21713835b1164ae09d3c69c0e0e4eb87",
            "b0b3d4dc0d42429ca869bd9718809c0d",
            "8749f501f1c04f298fedd0dae03522e3",
            "22fdfdc4046f4d8c8c2a308e1312d01a",
            "14a7b7e193d346329dfdbbfc1be44faa",
            "ccad7f4a213d4d9ebbbe806bc9b343e9",
            "c6bc4f9f8bc6487690297c24ea7017bf",
            "d3a52d09a34f4248a0d1108966bf9c55",
            "89445521f23b45b99b42c7fd2bf76ecb",
            "2aea2b0b79934e8bb2acdb6c6916d69e",
            "f2f585c63d064cc098d31eb4c9524db5",
            "7c4911ad5f2344ec91c3e186ebbc92f3",
            "a3da24dd051549fc9bacbf10611390fb",
            "8c111a8d90354352ab48eb73c950a633",
            "bd5ef55cc0dc4612879eae44284cc646",
            "d0d371225d514743b1266b712955dbf5",
            "07be7d3d6a224b82bcfe0a1370ee22be",
            "8ea4dd05ac7b4e35b524c64238d05880",
            "133c9773aa6b47e0acd40ceb560c1986",
            "2d67a7431ed4467a8977cc3186943ed8",
            "0d749b91656e4049a353356efb295356",
            "a8e64a141de9442e94a918275f05f0dd",
            "d8432daaedb844dc9b3f7cbc66f83178",
            "db62dcda26e24dca900fedcaf234a3d2",
            "a4a01714900141c4a69f67dd61182eb4",
            "19406569b2be421a9b8cdcd3756dc9d7",
            "a23ef236b275404cbee1d710728aaede",
            "3131e0897a0b427b91ba02a333078a72",
            "0033d6adad40427c8a5dcea360c5bb4a",
            "a9f5ecac0f8b4003b8f67c28af6c36ad",
            "10f2474dd4fd454aac9194a7022e20a2",
            "359bccad9205477cb512d03c4017eae0"
          ]
        },
        "outputId": "73e4458f-f664-4dd5-b0ef-871671fb8433"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, GPT2Tokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"jky594176/recipe_GPT2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"jky594176/recipe_GPT2\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0965d77dd2fb48f79a75ce89ba9478f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=850.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f80d9e6da04410d9718bcd923cef8f6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=510445819.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aac32707b11b4edbaac9a5f4c218c47a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898669.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21713835b1164ae09d3c69c0e0e4eb87",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89445521f23b45b99b42c7fd2bf76ecb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=298.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07be7d3d6a224b82bcfe0a1370ee22be",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=595.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4a01714900141c4a69f67dd61182eb4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=780.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onCSGcbxmHD4"
      },
      "source": [
        "model = model.to(\"cuda\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-R2TFY2MKCD",
        "outputId": "94093dc8-36e4-4042-ad04-0cfb3a2e8b55"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "dir_gpt2 =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "  print(\"step:\")\n",
        "\n",
        "  for j in range(replicated_size):\n",
        "    print(j, end = \", \")\n",
        "    md = generate_recipe(raw_text)\n",
        "    dir_gpt2[i].append(get_instr(md))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Generating the recipe:  0\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  1\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  2\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  3\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  4\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  5\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  6\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  7\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  8\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  9\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  10\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  11\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  12\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  13\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  14\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  15\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  16\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, 3, 4, Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, 9, \n",
            " Generating the recipe:  17\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  18\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  19\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  20\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  21\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  22\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  23\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, Failed to generate, recipe's too long\n",
            "6, 7, 8, 9, \n",
            " Generating the recipe:  24\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  25\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  26\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  27\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  28\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  29\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  30\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  31\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  32\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  33\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  34\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  35\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  36\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  37\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  38\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  39\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  40\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  41\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  42\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  43\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  44\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  45\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  46\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  47\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  48\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  49\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  50\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  51\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  52\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  53\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  54\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  55\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  56\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  57\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  58\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  59\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, 3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  60\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  61\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  62\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  63\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  64\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  65\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  66\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  67\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  68\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  69\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  70\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  71\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  72\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  73\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  74\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  75\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  76\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  77\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  78\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  79\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  80\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  81\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  82\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  83\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  84\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  85\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  86\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  87\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  88\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  89\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  90\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  91\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  92\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  93\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  94\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  95\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  96\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  97\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  98\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  99\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lef6Rnw9MUEQ",
        "outputId": "03c6a4d6-fd5f-446d-b4f5-4fffd13371a9"
      },
      "source": [
        "print(\"cos similarity:\", COS_SIM(df_eval_dir,dir_gpt2))\n",
        "print(\"BLEU: \",bleu_cal(df_eval_dir, dir_gpt2))\n",
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, dir_gpt2,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, dir_gpt2,'rougeLsum'))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cos similarity: 0.5389561959066712\n",
            "BLEU:  0.770623806605888\n",
            "rouge-1: 0.3006555958482678\n",
            "rouge-L: 0.17599998217410337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTe3xM2srJtT"
      },
      "source": [
        "## BART1+NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9nTKzkTrJBc"
      },
      "source": [
        "model = BartForCausalLM.from_pretrained(\"jky594176/recipe_BART1\")\n",
        "tokenizer = init_tokenizer(\"facebook/bart-base\")\n",
        "model = model.to('cuda')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "667BR_hNrJFX",
        "outputId": "be2b26e4-4c8d-4b89-8133-095b0ff9820f"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "dir_bart1_nn =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "  print(\"step:\")\n",
        "\n",
        "  for j in range(replicated_size):\n",
        "    print(j, end = \", \")\n",
        "    md = generate_recipe(raw_text)\n",
        "    dir_bart1_nn[i].append(get_instr(md))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Generating the recipe:  0\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  1\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  2\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  3\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  4\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  5\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  6\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  7\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  8\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  9\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  10\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  11\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  12\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  13\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  14\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  15\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  16\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  17\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  18\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  19\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  20\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  21\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  22\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  23\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  24\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  25\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  26\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  27\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  28\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  29\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  30\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  31\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "7, 8, 9, \n",
            " Generating the recipe:  32\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  33\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  34\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  35\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  36\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  37\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  38\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  39\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  40\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  41\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  42\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  43\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, Failed to generate, recipe's too long\n",
            "6, 7, 8, 9, \n",
            " Generating the recipe:  44\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  45\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  46\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  47\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  48\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  49\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  50\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  51\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  52\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  53\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  54\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  55\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  56\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  57\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  58\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  59\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  60\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  61\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  62\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  63\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  64\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  65\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  66\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  67\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  68\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  69\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  70\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  71\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  72\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  73\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  74\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  75\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  76\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  77\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  78\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  79\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  80\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "7, 8, 9, \n",
            " Generating the recipe:  81\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  82\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  83\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  84\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  85\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  86\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  87\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  88\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  89\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  90\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  91\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  92\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  93\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  94\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  95\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  96\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  97\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  98\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  99\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54Q0P_KmrJHt",
        "outputId": "b2ec757f-99b6-40a8-f5d5-0026a894072e"
      },
      "source": [
        "print(\"Cos sim:\", COS_SIM(df_eval_dir, dir_bart1_nn))\n",
        "print(\"BLEU:\", bleu_cal(df_eval_dir, dir_bart1_nn))\n",
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, dir_bart1_nn,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, dir_bart1_nn,'rougeL'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cos sim: 0.5342036210579831\n",
            "BLEU: 0.7706537892876422\n",
            "rouge-1: 0.31010366633561753\n",
            "rouge-L: 0.1850874663184411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzhM5AfQ7oNc"
      },
      "source": [
        "## BART2+GRU\n",
        "\n",
        "The generation helper functions for BART2 is not the same with BART1. So re-define them here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a68MnPnPrJLz"
      },
      "source": [
        "def set_seed(args):\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(model, length, raw_text, tokenizer, num_samples=1, temperature=1, top_k=0, top_p=0.0, device='cpu'):\n",
        "    end_token = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
        "    ing_token_id = tokenizer.convert_tokens_to_ids([\"<INPUT_END>\"])[0]\n",
        "    input_token = tokenizer.convert_tokens_to_ids([\"<NEXT_INPUT>\"])[0]\n",
        "\n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "    # print(prepared_input)\n",
        "\n",
        "    context_tokens = tokenizer.encode(prepared_input)[:-1]\n",
        "    context_len = len(context_tokens)\n",
        "    \n",
        "    encoder_tokens = torch.tensor(context_tokens[3:], dtype=torch.long, device=device)\n",
        "    encoder_tokens = encoder_tokens[encoder_tokens != input_token]\n",
        "\n",
        "    context = torch.tensor(context_tokens, dtype=torch.long, device=device)\n",
        "    # the encoder condition\n",
        "    raw_text_tokens = encoder_tokens\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    raw_text_tokens = raw_text_tokens.unsqueeze(0).repeat(num_samples, 1)\n",
        "    \n",
        "    # genereated token\n",
        "    start_token =  tokenizer.convert_tokens_to_ids([\"<RECIPE_START>\"])[0]\n",
        "    # generated = torch.tensor(start_token, dtype=torch.long, device=device).reshape(1).unsqueeze(0)\n",
        "    generated = context\n",
        "    # print(generated.shape)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            # inputs = {'input_ids': generated}\n",
        "            outputs = model(input_ids = raw_text_tokens, decoder_input_ids = generated)\n",
        "            # outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
        "            # print(outputs)\n",
        "            next_token_logits = outputs[\"logits\"][0, -1, :] / temperature\n",
        "            \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
        "            #print(next_token)\n",
        "            if next_token.item() == end_token:\n",
        "                break\n",
        "    return generated, context_len"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLs5uhD287LS"
      },
      "source": [
        "def get_raw_input(NER):\n",
        "    test_str = NER\n",
        "    test_str = test_str.replace(\"[\",\"\")\n",
        "    test_str = test_str.replace(\"]\",\"\")\n",
        "    test_str = test_str.replace(\"\\\"\",\"\")\n",
        "\n",
        "    return test_str\n",
        "\n",
        "def get_instr(markdown):\n",
        "  markdown = markdown.split(\"\\n\")\n",
        "  if ' ## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index(' ## Instructions ##')\n",
        "    \n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "\n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "    \n",
        "    return output\n",
        "\n",
        "  elif '## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index('## Instructions ##')\n",
        "\n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "        \n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "\n",
        "    return output\n",
        "    \n",
        "  else:\n",
        "    return [\"failed to generate\"]\n",
        "\n",
        "\n",
        "def generate_recipe(raw_text):\n",
        "  \n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "    out, context_len = sample_sequence(\n",
        "            model=model,\n",
        "            raw_text=raw_text,\n",
        "            tokenizer=tokenizer,\n",
        "            length=512,\n",
        "            device = \"cuda\"\n",
        "        )\n",
        "    out = out[0, context_len:].tolist()\n",
        "    text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n",
        "    if \"<RECIPE_END>\" not in text:\n",
        "      print(\"Failed to generate, recipe's too long\")\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      return generate_recipe(raw_text)\n",
        "    else:\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      markdown = re.sub(\"<RECIPE_(START|END)>\", \"\", full_text)\n",
        "      recipe_n_title = markdown.split(\"<TITLE_START>\")\n",
        "      if len(recipe_n_title)<=1:\n",
        "        return generate_recipe(raw_text)\n",
        "      title = \"# \" + recipe_n_title[1].replace(\"<TITLE_END>\", \"\") + \" #\\n\"\n",
        "      markdown = recipe_n_title[0].replace(\"<INPUT_START>\", \"## Input ingredients ##\\n`\").replace(\"<INPUT_END>\", \"`\\n\")\n",
        "      markdown = markdown.replace(\"<NEXT_INPUT>\", \"`\\n`\").replace(\"<INGR_START>\", \"## Ingredients ##\\n* \").replace(\"<NEXT_INGR>\", \"\\n* \").replace(\"<INGR_END>\", \"\\n\")\n",
        "      #markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\\\"\").replace(\"<NEXT_INSTR>\", \"\\n\\\"\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\").replace(\"<NEXT_INSTR>\", \"\\n\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = re.sub(\"$ +#\", \"#\", markdown)\n",
        "      markdown = re.sub(\"( +`|` +)\", \"`\", markdown)\n",
        "  \n",
        "      return markdown"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnkhqIbtMZOe"
      },
      "source": [
        "from transformers import BartForConditionalGeneration\n",
        "# model = BartForCausalLM.from_pretrained(\"jky594176/BART1_GRU\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"jky594176/BART2_GRU\")\n",
        "tokenizer = init_tokenizer(\"facebook/bart-base\")\n",
        "model = model.to('cuda')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqAYSgNr8-AO",
        "outputId": "1c03cc14-ce26-4dbe-924c-e6e5125de605"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "dir_bart2_gru =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "  print(\"step:\")\n",
        "\n",
        "  for j in range(replicated_size):\n",
        "    print(j, end = \", \")\n",
        "    md = generate_recipe(raw_text)\n",
        "    dir_bart2_gru[i].append(get_instr(md))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Generating the recipe:  0\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  1\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  2\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  3\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  4\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  5\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  6\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  7\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  8\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  9\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  10\n",
            "step:\n",
            "0, 1, 2, 3, 4, Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, 9, \n",
            " Generating the recipe:  11\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  12\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  13\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  14\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  15\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  16\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  17\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  18\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  19\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  20\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  21\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  22\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  23\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  24\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  25\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  26\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  27\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  28\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  29\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  30\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  31\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  32\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  33\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  34\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  35\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3EyQ9Ws9Fty"
      },
      "source": [
        "print(\"Cos sim:\", COS_SIM(df_eval_dir, dir_bart2_gru))\n",
        "print(\"BLEU:\", bleu_cal(df_eval_dir, dir_bart2_gru))\n",
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, dir_bart2_gru,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, dir_bart2_gru,'rougeL'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGWXvMMzFX2I"
      },
      "source": [
        "## BART2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63hq4lqiRWBt"
      },
      "source": [
        "def set_seed(args):\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(model, length, raw_text, tokenizer, num_samples=1, temperature=1, top_k=0, top_p=0.0, device='cpu'):\n",
        "    end_token = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "\n",
        "    context_tokens = tokenizer.encode(prepared_input)[0:-1]\n",
        "    raw_text_tokens = tokenizer.encode(raw_text)[0:-1]\n",
        "\n",
        "    context = torch.tensor(context_tokens, dtype=torch.long, device=device)\n",
        "    raw_text_tokens = torch.tensor(raw_text_tokens, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    raw_text_tokens = raw_text_tokens.unsqueeze(0).repeat(num_samples, 1)\n",
        "    # print(raw_text_tokens)\n",
        "    # print(context)\n",
        "    \n",
        "    # genereated token\n",
        "    start_token =  tokenizer.convert_tokens_to_ids([\"<RECIPE_START>\"])[0]\n",
        "    # generated = torch.tensor(start_token, dtype=torch.long, device=device).reshape(1).unsqueeze(0)\n",
        "    generated = context\n",
        "    # print(generated.shape)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            # inputs = {'input_ids': generated}\n",
        "            outputs = model(input_ids = raw_text_tokens, decoder_input_ids = generated)\n",
        "            # outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
        "            # print(outputs)\n",
        "            next_token_logits = outputs[\"logits\"][0, -1, :] / temperature\n",
        "            \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
        "            #print(next_token)\n",
        "            if next_token.item() == end_token:\n",
        "                break\n",
        "    return generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcABuelZRWrj"
      },
      "source": [
        "def get_raw_input(NER):\n",
        "    test_str = NER\n",
        "    test_str = test_str.replace(\"[\",\"\")\n",
        "    test_str = test_str.replace(\"]\",\"\")\n",
        "    test_str = test_str.replace(\"\\\"\",\"\")\n",
        "\n",
        "    return test_str\n",
        "\n",
        "def get_instr(markdown):\n",
        "  markdown = markdown.split(\"\\n\")\n",
        "  if ' ## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index(' ## Instructions ##')\n",
        "    \n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "\n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "    \n",
        "    return output\n",
        "\n",
        "  elif '## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index('## Instructions ##')\n",
        "\n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "        \n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "\n",
        "    return output\n",
        "    \n",
        "  else:\n",
        "    return [\"failed to generate\"]\n",
        "\n",
        "\n",
        "def generate_recipe(raw_text):\n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "    context_tokens = tokenizer.encode(prepared_input)[0:-1]\n",
        "    out = sample_sequence(\n",
        "                model=model,\n",
        "                raw_text=raw_text,\n",
        "                tokenizer=tokenizer,\n",
        "                length=512,\n",
        "                device = 'cuda'\n",
        "            )\n",
        "    out = out[0, len(context_tokens):].tolist()\n",
        "    text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n",
        "    if \"<RECIPE_END>\" not in text:\n",
        "      # print(text)\n",
        "      print(\"Failed to generate, recipe's too long\")\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      return generate_recipe(raw_text)\n",
        "    else:\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      markdown = re.sub(\"<RECIPE_(START|END)>\", \"\", full_text)\n",
        "      recipe_n_title = markdown.split(\"<TITLE_START>\")\n",
        "      if len(recipe_n_title)<=1:\n",
        "        return generate_recipe(raw_text)\n",
        "      title = \"# \" + recipe_n_title[1].replace(\"<TITLE_END>\", \"\") + \" #\\n\"\n",
        "      markdown = recipe_n_title[0].replace(\"<INPUT_START>\", \"## Input ingredients ##\\n`\").replace(\"<INPUT_END>\", \"`\\n\")\n",
        "      markdown = markdown.replace(\"<NEXT_INPUT>\", \"`\\n`\").replace(\"<INGR_START>\", \"## Ingredients ##\\n* \").replace(\"<NEXT_INGR>\", \"\\n* \").replace(\"<INGR_END>\", \"\\n\")\n",
        "      #markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\\\"\").replace(\"<NEXT_INSTR>\", \"\\n\\\"\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\").replace(\"<NEXT_INSTR>\", \"\\n\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = re.sub(\"$ +#\", \"#\", markdown)\n",
        "      markdown = re.sub(\"( +`|` +)\", \"`\", markdown)\n",
        "  \n",
        "      return markdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k6Eiuqk-QEe"
      },
      "source": [
        "from transformers import BartForConditionalGeneration\n",
        "model = BartForConditionalGeneration.from_pretrained(\"jky594176/recipe_bart2_v2\")\n",
        "model = model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC0s8yM_FW5O"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "dir_bart2 =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "  print(\"step:\")\n",
        "\n",
        "  for j in range(replicated_size):\n",
        "    print(j, end = \", \")\n",
        "    md = generate_recipe(raw_text)\n",
        "    dir_bart2[i].append(get_instr(md))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMM4SMJdFW7Z"
      },
      "source": [
        "print(\"Cos sim:\", COS_SIM(df_eval_dir, dir_bart2_gru))\n",
        "print(\"BLEU:\", bleu_cal(df_eval_dir, dir_bart2_gru))\n",
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, dir_bart2_gru,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, dir_bart2_gru,'rougeL'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyKOK5VJJPie"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}