{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evalutaion_final.ipynb（副本）",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fda850cb82bc48549a54c3ecdb767f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c2b06e7ef6ff4b58af9c03648d43f3b0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6ffb7cdaf5464d5f8974342fcd290827",
              "IPY_MODEL_08041175f26441a5b75833955f45991b"
            ]
          }
        },
        "c2b06e7ef6ff4b58af9c03648d43f3b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ffb7cdaf5464d5f8974342fcd290827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_276e93dd17fe48a8b68781943f133030",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1682,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1682,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ccb327cfa63c477ca58d664a3ae2fea5"
          }
        },
        "08041175f26441a5b75833955f45991b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ca33dd0c6e44d2bb58f097466a4b7a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.68k/1.68k [00:00&lt;00:00, 55.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_267cc56b5b6f4ef0931214d129ddc827"
          }
        },
        "276e93dd17fe48a8b68781943f133030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ccb327cfa63c477ca58d664a3ae2fea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ca33dd0c6e44d2bb58f097466a4b7a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "267cc56b5b6f4ef0931214d129ddc827": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6461871414dc42c3a34d6b5e765e1d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_800f8cd7fa3545c0acce5bd88bc157e1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9c93d708cbac403dbdde21cc31103032",
              "IPY_MODEL_f5b0e3dc9c1342488fcaf40928681f9d"
            ]
          }
        },
        "800f8cd7fa3545c0acce5bd88bc157e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c93d708cbac403dbdde21cc31103032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b40de920da124ba6849a899fb672e194",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 384515855,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 384515855,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b8d28c261824b0b91fd49169cd965c8"
          }
        },
        "f5b0e3dc9c1342488fcaf40928681f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_be9a73f68ed34b2e9471d450000d7eb7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 385M/385M [00:14&lt;00:00, 26.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c1afdb2aa45d4cc180f9d426d8146ae2"
          }
        },
        "b40de920da124ba6849a899fb672e194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b8d28c261824b0b91fd49169cd965c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be9a73f68ed34b2e9471d450000d7eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c1afdb2aa45d4cc180f9d426d8146ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0965d77dd2fb48f79a75ce89ba9478f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6a935431775b4f4bac600807111645a6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_580a44db562b481aaa4cf216c0204ac6",
              "IPY_MODEL_eaa52ae38bb94e0eb42e5035e0e102e9"
            ]
          }
        },
        "6a935431775b4f4bac600807111645a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "580a44db562b481aaa4cf216c0204ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ca571001c23d4f3b81485c24d21a6ef2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 850,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 850,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_058330f04cc543a18bfbfc147c614885"
          }
        },
        "eaa52ae38bb94e0eb42e5035e0e102e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_177a3febd2c54306a6fbd6ba12e6b648",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 850/850 [00:00&lt;00:00, 962B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17072019b61f406480c1754043f985b5"
          }
        },
        "ca571001c23d4f3b81485c24d21a6ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "058330f04cc543a18bfbfc147c614885": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "177a3febd2c54306a6fbd6ba12e6b648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17072019b61f406480c1754043f985b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f80d9e6da04410d9718bcd923cef8f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a79b8f0f8bce44a383c7340b2a9a1855",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_721c1229845a468ebad562c075176fa0",
              "IPY_MODEL_6365239082944b3894e0cfc7b8274e57"
            ]
          }
        },
        "a79b8f0f8bce44a383c7340b2a9a1855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "721c1229845a468ebad562c075176fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_abc749df653042fda20b0615c3a21f99",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 510445819,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 510445819,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4df3c7aad34043c68194e76709157a20"
          }
        },
        "6365239082944b3894e0cfc7b8274e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_52fd6b17ed404052890b575ebf0827dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 510M/510M [00:10&lt;00:00, 50.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4f1b7f6e295f4710a45b0b43bace6540"
          }
        },
        "abc749df653042fda20b0615c3a21f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4df3c7aad34043c68194e76709157a20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52fd6b17ed404052890b575ebf0827dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4f1b7f6e295f4710a45b0b43bace6540": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aac32707b11b4edbaac9a5f4c218c47a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_16dd9aad0ff544e3add106ba752fa38d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_018c3da47c8b4621be7fdb7624728e13",
              "IPY_MODEL_d705c8a48ac14834ae356425928a24e1"
            ]
          }
        },
        "16dd9aad0ff544e3add106ba752fa38d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "018c3da47c8b4621be7fdb7624728e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_25a92977f5c64e709f4f970ebad42efa",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898669,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898669,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38d4311e7efd4f8bb9bfe055d748728d"
          }
        },
        "d705c8a48ac14834ae356425928a24e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2a078fcfc6ec43409ff7410326b33ccb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:01&lt;00:00, 476kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15068145c056406bac9fffdfed70994a"
          }
        },
        "25a92977f5c64e709f4f970ebad42efa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38d4311e7efd4f8bb9bfe055d748728d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a078fcfc6ec43409ff7410326b33ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15068145c056406bac9fffdfed70994a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21713835b1164ae09d3c69c0e0e4eb87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b0b3d4dc0d42429ca869bd9718809c0d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8749f501f1c04f298fedd0dae03522e3",
              "IPY_MODEL_22fdfdc4046f4d8c8c2a308e1312d01a"
            ]
          }
        },
        "b0b3d4dc0d42429ca869bd9718809c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8749f501f1c04f298fedd0dae03522e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14a7b7e193d346329dfdbbfc1be44faa",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ccad7f4a213d4d9ebbbe806bc9b343e9"
          }
        },
        "22fdfdc4046f4d8c8c2a308e1312d01a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c6bc4f9f8bc6487690297c24ea7017bf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 494kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d3a52d09a34f4248a0d1108966bf9c55"
          }
        },
        "14a7b7e193d346329dfdbbfc1be44faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ccad7f4a213d4d9ebbbe806bc9b343e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6bc4f9f8bc6487690297c24ea7017bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d3a52d09a34f4248a0d1108966bf9c55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89445521f23b45b99b42c7fd2bf76ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2aea2b0b79934e8bb2acdb6c6916d69e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f2f585c63d064cc098d31eb4c9524db5",
              "IPY_MODEL_7c4911ad5f2344ec91c3e186ebbc92f3"
            ]
          }
        },
        "2aea2b0b79934e8bb2acdb6c6916d69e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2f585c63d064cc098d31eb4c9524db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a3da24dd051549fc9bacbf10611390fb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 298,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 298,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c111a8d90354352ab48eb73c950a633"
          }
        },
        "7c4911ad5f2344ec91c3e186ebbc92f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd5ef55cc0dc4612879eae44284cc646",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 298/298 [00:01&lt;00:00, 154B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d0d371225d514743b1266b712955dbf5"
          }
        },
        "a3da24dd051549fc9bacbf10611390fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c111a8d90354352ab48eb73c950a633": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd5ef55cc0dc4612879eae44284cc646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d0d371225d514743b1266b712955dbf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07be7d3d6a224b82bcfe0a1370ee22be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8ea4dd05ac7b4e35b524c64238d05880",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_133c9773aa6b47e0acd40ceb560c1986",
              "IPY_MODEL_2d67a7431ed4467a8977cc3186943ed8"
            ]
          }
        },
        "8ea4dd05ac7b4e35b524c64238d05880": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "133c9773aa6b47e0acd40ceb560c1986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0d749b91656e4049a353356efb295356",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 595,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 595,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8e64a141de9442e94a918275f05f0dd"
          }
        },
        "2d67a7431ed4467a8977cc3186943ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d8432daaedb844dc9b3f7cbc66f83178",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 595/595 [00:01&lt;00:00, 515B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db62dcda26e24dca900fedcaf234a3d2"
          }
        },
        "0d749b91656e4049a353356efb295356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8e64a141de9442e94a918275f05f0dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d8432daaedb844dc9b3f7cbc66f83178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db62dcda26e24dca900fedcaf234a3d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4a01714900141c4a69f67dd61182eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_19406569b2be421a9b8cdcd3756dc9d7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a23ef236b275404cbee1d710728aaede",
              "IPY_MODEL_3131e0897a0b427b91ba02a333078a72"
            ]
          }
        },
        "19406569b2be421a9b8cdcd3756dc9d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a23ef236b275404cbee1d710728aaede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0033d6adad40427c8a5dcea360c5bb4a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 780,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 780,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9f5ecac0f8b4003b8f67c28af6c36ad"
          }
        },
        "3131e0897a0b427b91ba02a333078a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_10f2474dd4fd454aac9194a7022e20a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 780/780 [00:00&lt;00:00, 1.86kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_359bccad9205477cb512d03c4017eae0"
          }
        },
        "0033d6adad40427c8a5dcea360c5bb4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9f5ecac0f8b4003b8f67c28af6c36ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10f2474dd4fd454aac9194a7022e20a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "359bccad9205477cb512d03c4017eae0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2cc17e384e5a43528457efd3ed0e3d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_df0fa90c399049cfba4f67034bce0022",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e753fc502cf04771b4ef55244ed40d99",
              "IPY_MODEL_946990d3080e4dfcb6cc7ede90c156a9"
            ]
          }
        },
        "df0fa90c399049cfba4f67034bce0022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e753fc502cf04771b4ef55244ed40d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_adac09af1de549039689371690913133",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a057ca22d35b43f0a283b9837b977e77"
          }
        },
        "946990d3080e4dfcb6cc7ede90c156a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b0ac98d02e124b68a6ba3a8e7fed6d14",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:02&lt;00:00, 358kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cbcd4bce30c3432c85889c841d4f6c81"
          }
        },
        "adac09af1de549039689371690913133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a057ca22d35b43f0a283b9837b977e77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0ac98d02e124b68a6ba3a8e7fed6d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cbcd4bce30c3432c85889c841d4f6c81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "717ed4c4d74f4183834191eb0c95d33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_62fdba02c39e478ba8428d41de1b4f6a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fa9c16bd0f4c4b92952b26f5a93ea5e7",
              "IPY_MODEL_9e0c3e0b09f54ab586174aa1769104e1"
            ]
          }
        },
        "62fdba02c39e478ba8428d41de1b4f6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa9c16bd0f4c4b92952b26f5a93ea5e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ca48c28218684789876b65393c81a0e2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2d7add17842742d4b06366dcdcf1ab14"
          }
        },
        "9e0c3e0b09f54ab586174aa1769104e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b6c390c07fdd425ba63bfe287bb11b65",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:01&lt;00:00, 287kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27519db3b8df46beae79b375ef12a62e"
          }
        },
        "ca48c28218684789876b65393c81a0e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2d7add17842742d4b06366dcdcf1ab14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6c390c07fdd425ba63bfe287bb11b65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "27519db3b8df46beae79b375ef12a62e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf439e1e02bc4d31a68a745b6eb1b7e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3d01ad95e1c84cca93d9de390d6e8a0e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_04cc86fdc0e34aa8a03cb8566cb11141",
              "IPY_MODEL_fa28bd853e5c4b1aa83c34aa8f7fda37"
            ]
          }
        },
        "3d01ad95e1c84cca93d9de390d6e8a0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04cc86fdc0e34aa8a03cb8566cb11141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3daec3f63b0e4181851694accfdd9ba5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_32368ce0af3a45ad92111090e2c9fa62"
          }
        },
        "fa28bd853e5c4b1aa83c34aa8f7fda37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d3913dc2686143d4809bf34d8a158e8b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 4.38MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a82545b9b2e415f9cd43cda705a867e"
          }
        },
        "3daec3f63b0e4181851694accfdd9ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "32368ce0af3a45ad92111090e2c9fa62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3913dc2686143d4809bf34d8a158e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a82545b9b2e415f9cd43cda705a867e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuHBMqJkcJyV"
      },
      "source": [
        "# Evaluation prearation\n",
        "\n",
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5HEQeFtdbz3",
        "outputId": "ab2dbf5b-6342-459e-e227-da814bbc7666"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install boto3\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "!pip install jiwer==2.2.0\n",
        "!pip install -U nltk\n",
        "!pip install rouge/requirements.txt\n",
        "!pip install rouge-score\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .. \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 57.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 55.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/20/4294e37c3c6936c905f1e9da958c776d7fee54a4512bdb7706d69c8720e6/boto3-1.17.84-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 8.5MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.84\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/22/72c81d754bbcb128cba2ad88670c3c320e4594e6ddd8cca6512c3967108c/botocore-1.20.84-py2.py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 42.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.84->boto3) (2.8.1)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/cd/1e2ec680ec7b09846dc6e605f5a7709dfb9d7128e51a026e7154e18a234e/urllib3-1.26.5-py2.py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 70.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.84->boto3) (1.15.0)\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, urllib3, botocore, s3transfer, boto3\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.17.84 botocore-1.20.84 jmespath-0.10.0 s3transfer-0.4.2 urllib3-1.26.5\n",
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8048, done.\u001b[K\n",
            "remote: Counting objects: 100% (135/135), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 8048 (delta 65), reused 93 (delta 41), pack-reused 7913\u001b[K\n",
            "Receiving objects: 100% (8048/8048), 14.11 MiB | 19.90 MiB/s, done.\n",
            "Resolving deltas: 100% (5464/5464), done.\n",
            "Collecting jiwer==2.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/cc/fb9d3132cba1f6d393b7d5a9398d9d4c8fc033bc54668cf87e9b197a6d7a/jiwer-2.2.0-py3-none-any.whl\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/dc/97f2b63ef0fa1fd78dcb7195aca577804f6b2b51e712516cc0e902a9a201/python-Levenshtein-0.12.2.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jiwer==2.2.0) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein->jiwer==2.2.0) (56.1.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149818 sha256=68c26ed160104c88a346d6ed97dc1e05338f23535115019546e6f62479c7277b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/26/73/4b48503bac73f01cf18e52cd250947049a7f339e940c5df8fc\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein, jiwer\n",
            "Successfully installed jiwer-2.2.0 python-Levenshtein-0.12.2\n",
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n",
            "\u001b[31mERROR: Invalid requirement: 'rouge/requirements.txt'\n",
            "Hint: It looks like a path. File 'rouge/requirements.txt' does not exist.\u001b[0m\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (7.1.2)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n",
            "/content/apex\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-m0976zmc\n",
            "Created temporary directory: /tmp/pip-req-tracker-fqnohnsf\n",
            "Created requirements tracker '/tmp/pip-req-tracker-fqnohnsf'\n",
            "Created temporary directory: /tmp/pip-install-bm4mj753\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-bq0in5h2\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-fqnohnsf'\n",
            "    Running setup.py (path:/tmp/pip-req-build-bq0in5h2/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.8.1+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-bq0in5h2/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-bq0in5h2/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-bq0in5h2/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-bq0in5h2/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-bq0in5h2/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n",
            "    writing manifest file '/tmp/pip-req-build-bq0in5h2/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-bq0in5h2/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-bq0in5h2 has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-fqnohnsf'\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-yql42bev\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-yql42bev\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-bq0in5h2/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-bq0in5h2/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-yql42bev --python-tag cp37\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.8.1+cu101\n",
            "\n",
            "\n",
            "  /tmp/pip-req-build-bq0in5h2/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/pyprof\n",
            "  copying apex/pyprof/__init__.py -> build/lib/apex/pyprof\n",
            "  creating build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/__init__.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/reparameterization.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/weight_norm.py -> build/lib/apex/reparameterization\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/__init__.py -> build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/nvmarker.py -> build/lib/apex/pyprof/nvtx\n",
            "  creating build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/loss.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/dropout.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/softmax.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/usage.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/output.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/conv.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__main__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/base.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/utility.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/embedding.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/recurrentCell.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/prof.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pointwise.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/reduction.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/activation.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__init__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pooling.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/normalization.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/misc.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/linear.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/randomSample.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/data.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/convert.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/blas.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/optim.py -> build/lib/apex/pyprof/prof\n",
            "  creating build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/db.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/kernel.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/nvvp.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__main__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__init__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/parse.py -> build/lib/apex/pyprof/parse\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-yql42bev/apex-0.1-cp37-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v2.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v3.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/pyprof/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/nvmarker.py'\n",
            "  adding 'apex/pyprof/parse/__init__.py'\n",
            "  adding 'apex/pyprof/parse/__main__.py'\n",
            "  adding 'apex/pyprof/parse/db.py'\n",
            "  adding 'apex/pyprof/parse/kernel.py'\n",
            "  adding 'apex/pyprof/parse/nvvp.py'\n",
            "  adding 'apex/pyprof/parse/parse.py'\n",
            "  adding 'apex/pyprof/prof/__init__.py'\n",
            "  adding 'apex/pyprof/prof/__main__.py'\n",
            "  adding 'apex/pyprof/prof/activation.py'\n",
            "  adding 'apex/pyprof/prof/base.py'\n",
            "  adding 'apex/pyprof/prof/blas.py'\n",
            "  adding 'apex/pyprof/prof/conv.py'\n",
            "  adding 'apex/pyprof/prof/convert.py'\n",
            "  adding 'apex/pyprof/prof/data.py'\n",
            "  adding 'apex/pyprof/prof/dropout.py'\n",
            "  adding 'apex/pyprof/prof/embedding.py'\n",
            "  adding 'apex/pyprof/prof/index_slice_join_mutate.py'\n",
            "  adding 'apex/pyprof/prof/linear.py'\n",
            "  adding 'apex/pyprof/prof/loss.py'\n",
            "  adding 'apex/pyprof/prof/misc.py'\n",
            "  adding 'apex/pyprof/prof/normalization.py'\n",
            "  adding 'apex/pyprof/prof/optim.py'\n",
            "  adding 'apex/pyprof/prof/output.py'\n",
            "  adding 'apex/pyprof/prof/pointwise.py'\n",
            "  adding 'apex/pyprof/prof/pooling.py'\n",
            "  adding 'apex/pyprof/prof/prof.py'\n",
            "  adding 'apex/pyprof/prof/randomSample.py'\n",
            "  adding 'apex/pyprof/prof/recurrentCell.py'\n",
            "  adding 'apex/pyprof/prof/reduction.py'\n",
            "  adding 'apex/pyprof/prof/softmax.py'\n",
            "  adding 'apex/pyprof/prof/usage.py'\n",
            "  adding 'apex/pyprof/prof/utility.py'\n",
            "  adding 'apex/reparameterization/__init__.py'\n",
            "  adding 'apex/reparameterization/reparameterization.py'\n",
            "  adding 'apex/reparameterization/weight_norm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp37-none-any.whl size=204691 sha256=12850ae70d20470494ba1bcf7c01e3de2c3f79ad31ebbdca875a67c4ddca12c2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m0976zmc/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "  Removing source in /tmp/pip-req-build-bq0in5h2\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-fqnohnsf'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsmxZM0-x7fF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228510d3-3028-4861-efda-39bc2f6852e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3gLMKblgV2b",
        "outputId": "1b7a7639-dd12-495d-9939-15b3f29791b7"
      },
      "source": [
        "# data preprocessing\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import json\n",
        "import h5py\n",
        "\n",
        "# control\n",
        "import argparse\n",
        "import logging\n",
        "from tqdm import trange\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# transformers\n",
        "from transformers import GPT2Config\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import BartForCausalLM, BartTokenizer,BartConfig"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz7e2PoybaiE"
      },
      "source": [
        "# fine-tuning model with bart\n",
        "# logging info\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import random\n",
        "import gc\n",
        "import boto3\n",
        "import shutil"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5MKxGg8bhj4"
      },
      "source": [
        "from tqdm import tqdm, trange\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "from transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import BartForCausalLM, BartTokenizer,BartConfig"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQSBz5mG77Ic"
      },
      "source": [
        "## data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEAelfO1BwUd"
      },
      "source": [
        "# load an independent test set that previously saved\n",
        "df_eval = pd.read_csv('./gdrive/MyDrive/COMP0087_v7/350k_GPT-2_Score.csv')\n",
        "df_eval_ner = df_eval[\"NER\"]  #NER\n",
        "df_eval_dir = df_eval[\"directions\"]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWI6m5-_64Ln",
        "outputId": "36d1aead-2c56-4a75-eb56-1464a29fedae"
      },
      "source": [
        "df_eval_dir"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     [\"Blend 1/2 quart dry milk, 14 ounces club sod...\n",
              "1     [\"Cook orzo in lightly salted water as package...\n",
              "2     [\"Mix corn, salt and milk together.\", \"Beat eg...\n",
              "3     [\"Mix cornstarch, sugar and salt.\", \"Add hot w...\n",
              "4     [\"Mix taco seasoning and refried beans to desi...\n",
              "                            ...                        \n",
              "95    [\"Beat egg substitute and add, in order, spice...\n",
              "96    [\"Pour all ingredients over chicken breasts in...\n",
              "97    [\"Put spices and sugar in basket of 8-cup perc...\n",
              "98    [\"Melt chocolate in boiling water.\", \"Cool.\", ...\n",
              "99    [\"Bring milk, shortening, cocoa, and sugar to ...\n",
              "Name: directions, Length: 100, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGWBEp9S8FSe"
      },
      "source": [
        "## generation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0UpZhdr7K48"
      },
      "source": [
        "def init_tokenizer(tokenizer_type = \"facebook/bart-base\"):\n",
        "  \n",
        "  tokenizer = BartTokenizer.from_pretrained(tokenizer_type)\n",
        "  special_tokens = {\n",
        "    \"additional_special_tokens\": [\n",
        "        \"<TITLE_START>\",\n",
        "        \"<TITLE_END>\",\n",
        "        \"<INSTR_START>\",\n",
        "        \"<NEXT_INSTR>\",\n",
        "        \"<INSTR_END>\",\n",
        "        \"<INGR_START>\",\n",
        "        \"<NEXT_INGR>\",\n",
        "        \"<INGR_END>\",\n",
        "        \"<RECIPE_START>\",\n",
        "        \"<RECIPE_END>\",\n",
        "        \"<INPUT_START>\",\n",
        "        \"<INPUT_END>\",\n",
        "        \"<NEXT_INPUT>\"\n",
        "      ]\n",
        "  }\n",
        "  tokenizer.add_special_tokens(special_tokens)\n",
        "  return tokenizer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edyp1__48Kdv"
      },
      "source": [
        "def set_seed(args):\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(model, length, context, tokenizer, num_samples=1, temperature=1, top_k=0, top_p=0.0, device='cpu'):\n",
        "    end_token = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
        "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            inputs = {'input_ids': generated}\n",
        "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
        "            # print(outputs)\n",
        "            next_token_logits = outputs[\"logits\"][0, -1, :] / temperature\n",
        "            \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
        "            #print(next_token)\n",
        "            if next_token.item() == end_token:\n",
        "                break\n",
        "    return generated"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Iv1hGHm8Gu1"
      },
      "source": [
        "def get_raw_input(NER):\n",
        "    test_str = NER\n",
        "    test_str = test_str.replace(\"[\",\"\")\n",
        "    test_str = test_str.replace(\"]\",\"\")\n",
        "    test_str = test_str.replace(\"\\\"\",\"\")\n",
        "\n",
        "    return test_str\n",
        "\n",
        "def get_instr(markdown):\n",
        "  markdown = markdown.split(\"\\n\")\n",
        "  if ' ## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index(' ## Instructions ##')\n",
        "    \n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "\n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "    \n",
        "    return output\n",
        "\n",
        "  elif '## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index('## Instructions ##')\n",
        "\n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "        \n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "\n",
        "    return output\n",
        "    \n",
        "  else:\n",
        "    return [\"failed to generate\"]\n",
        "\n",
        "\n",
        "def generate_recipe(raw_text):\n",
        "\n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "    context_tokens = tokenizer.encode(prepared_input)[0:-1]\n",
        "    out = sample_sequence(\n",
        "                model=model,\n",
        "                context=context_tokens,\n",
        "                tokenizer=tokenizer,\n",
        "                length=800,\n",
        "                device = 'cuda'\n",
        "            )\n",
        "    out = out[0, len(context_tokens):].tolist()\n",
        "    text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n",
        "    if \"<RECIPE_END>\" not in text:\n",
        "      # print(text)\n",
        "      print(\"Failed to generate, recipe's too long\")\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      return generate_recipe(raw_text)\n",
        "    else:\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      markdown = re.sub(\"<RECIPE_(START|END)>\", \"\", full_text)\n",
        "      recipe_n_title = markdown.split(\"<TITLE_START>\")\n",
        "      if len(recipe_n_title)<=1:\n",
        "        return generate_recipe(raw_text)\n",
        "      title = \"# \" + recipe_n_title[1].replace(\"<TITLE_END>\", \"\") + \" #\\n\"\n",
        "      markdown = recipe_n_title[0].replace(\"<INPUT_START>\", \"## Input ingredients ##\\n`\").replace(\"<INPUT_END>\", \"`\\n\")\n",
        "      markdown = markdown.replace(\"<NEXT_INPUT>\", \"`\\n`\").replace(\"<INGR_START>\", \"## Ingredients ##\\n* \").replace(\"<NEXT_INGR>\", \"\\n* \").replace(\"<INGR_END>\", \"\\n\")\n",
        "      #markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\\\"\").replace(\"<NEXT_INSTR>\", \"\\n\\\"\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\").replace(\"<NEXT_INSTR>\", \"\\n\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = re.sub(\"$ +#\", \"#\", markdown)\n",
        "      markdown = re.sub(\"( +`|` +)\", \"`\", markdown)\n",
        "  \n",
        "      return markdown"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUGnW9ug8JpC"
      },
      "source": [
        "## cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46H9vqmwHQOj"
      },
      "source": [
        "#func of cos_sim\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def get_cosine(vec1, vec2):\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "\n",
        "def text_to_vector(text):\n",
        "    word = re.compile(r'\\w+')\n",
        "    words = word.findall(text)\n",
        "    return Counter(words)\n",
        "\n",
        "\n",
        "def get_result(content_a, content_b):\n",
        "    text1 = content_a\n",
        "    text2 = content_b\n",
        "\n",
        "    vector1 = text_to_vector(text1)\n",
        "    vector2 = text_to_vector(text2)\n",
        "\n",
        "    cosine_result = get_cosine(vector1, vector2)\n",
        "    return cosine_result"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4rWzL5RHTxP"
      },
      "source": [
        "#cosine_similarity\n",
        "\n",
        "def COS_SIM(df_eval_dir,directions):\n",
        "  \"\"\"\n",
        "  df_eval_dir := gold_standard recipes\n",
        "  directions  := corresponding recipes \"\"\"\n",
        "\n",
        "  avg = 0\n",
        "\n",
        "  for i in range(len(directions)):\n",
        "    best = 0\n",
        "    for j in range(len(directions[i])):\n",
        "      cos = get_result(\" \".join(eval(df_eval_dir[i])),\n",
        "                      \" \".join(directions[i][j]))\n",
        "      best = max(best, cos)\n",
        "\n",
        "    avg += best\n",
        "\n",
        "  avg = avg/len(directions)\n",
        "  return avg"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1f4i3iiH7F_"
      },
      "source": [
        "## Bleu Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfxaEbKLHUU7"
      },
      "source": [
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "import nltk.translate.gleu_score as gleu\n",
        "import nltk.translate.meteor_score as meteor\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCb3bltrHUb4"
      },
      "source": [
        "# Helper Func.\n",
        "def list_to_words(recipe):\n",
        "  words = []\n",
        "  for i in recipe:\n",
        "    words += i.split()\n",
        "\n",
        "  return words\n",
        "# bleu score\n",
        "def bleu_score(recipe, refer):\n",
        "    hyp = list_to_words(eval(recipe))\n",
        "    refs = []\n",
        "    for i in refer:\n",
        "        refs.append(list_to_words(i))\n",
        "\n",
        "    smoothie = SmoothingFunction().method5\n",
        "    score_ref_a = bleu.sentence_bleu(refs, hyp, smoothing_function=smoothie, weights=(1, 0, 0, 0))\n",
        "    return score_ref_a\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpIvWnYUHYro"
      },
      "source": [
        "def bleu_cal(df_eval_dir, directions):\n",
        "  bleu_avg = []\n",
        "  for i in range(len(directions)):\n",
        "    bleu_avg.append(bleu_score(df_eval_dir[i], directions[i]))\n",
        "  return np.mean(bleu_avg)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFM43lh0H3VZ"
      },
      "source": [
        "## ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RApwjiOtHYtY"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def list_to_string(start_list):\n",
        "  string=''.join([str(item) for item in start_list])\n",
        "  return string\n",
        "\n",
        "def rouge_cal(df_eval_dir, directions, rouge_type = 'rougeL'):\n",
        "  rough_avg = []\n",
        "  scorer = rouge_scorer.RougeScorer([rouge_type], use_stemmer=True)\n",
        "\n",
        "  for i in range(len(df_eval_dir)):\n",
        "    for j in range(10):\n",
        "      scores= scorer.score(df_eval_dir[i], list_to_string(directions[i][j]))\n",
        "      rough_avg.append(list(scores.values())[0][0])\n",
        "  return np.mean(rough_avg)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfqa9Ldi8NI8"
      },
      "source": [
        "# model evaluation\n",
        "\n",
        "## BART1+GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIENuHZf8Mnn"
      },
      "source": [
        "# model = BartForCausalLM.from_pretrained(\"jky594176/BART1_GRU\")\n",
        "model = BartForCausalLM.from_pretrained(\"jky594176/recipe_BART1_GRU\")\n",
        "tokenizer = init_tokenizer(\"facebook/bart-base\")\n",
        "model = model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skgTrNlX74Zn",
        "outputId": "86cca41a-91c5-4534-cd72-3880950d50cb"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "dir_bart1_gru =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "  print(\"step:\")\n",
        "  for j in range(replicated_size):\n",
        "    print(j, end = \", \")\n",
        "    md = generate_recipe(raw_text)\n",
        "    dir_bart1_gru[i].append(get_instr(md))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Generating the recipe:  0\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  1\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  2\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  3\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  4\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  5\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  6\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  7\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  8\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  9\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  10\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  11\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  12\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  13\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  14\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  15\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  16\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  17\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  18\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  19\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  20\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  21\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  22\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  23\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  24\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  25\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  26\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  27\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  28\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  29\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  30\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  31\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  32\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  33\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  34\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  35\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  36\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  37\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  38\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  39\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  40\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  41\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  42\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  43\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  44\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  45\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  46\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  47\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  48\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  49\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  50\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  51\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  52\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  53\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  54\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  55\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  56\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  57\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  58\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  59\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  60\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  61\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  62\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  63\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  64\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  65\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  66\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  67\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  68\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  69\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  70\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  71\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  72\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  73\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  74\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  75\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  76\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  77\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  78\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  79\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  80\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  81\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  82\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  83\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  84\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  85\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  86\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  87\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  88\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  89\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  90\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  91\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  92\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  93\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  94\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  95\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  96\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  97\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  98\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  99\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up2NvAQ_imbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea308e8-b831-438a-843a-dd3a90374945"
      },
      "source": [
        "print(\"Cos sim:\", COS_SIM(df_eval_dir, dir_bart1_gru))\n",
        "print(\"BLEU:\", bleu_cal(df_eval_dir, dir_bart1_gru))\n",
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, dir_bart1_gru,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, dir_bart1_gru,'rougeLsum'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cos sim: 0.5643583126180023\n",
            "BLEU: 0.77208228998418\n",
            "rouge-1: 0.3565509156364043\n",
            "rouge-L: 0.2194203777934473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCVosSQ6GQm1"
      },
      "source": [
        "## BART1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9NcdS7IEphS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "fda850cb82bc48549a54c3ecdb767f95",
            "c2b06e7ef6ff4b58af9c03648d43f3b0",
            "6ffb7cdaf5464d5f8974342fcd290827",
            "08041175f26441a5b75833955f45991b",
            "276e93dd17fe48a8b68781943f133030",
            "ccb327cfa63c477ca58d664a3ae2fea5",
            "0ca33dd0c6e44d2bb58f097466a4b7a2",
            "267cc56b5b6f4ef0931214d129ddc827",
            "6461871414dc42c3a34d6b5e765e1d67",
            "800f8cd7fa3545c0acce5bd88bc157e1",
            "9c93d708cbac403dbdde21cc31103032",
            "f5b0e3dc9c1342488fcaf40928681f9d",
            "b40de920da124ba6849a899fb672e194",
            "8b8d28c261824b0b91fd49169cd965c8",
            "be9a73f68ed34b2e9471d450000d7eb7",
            "c1afdb2aa45d4cc180f9d426d8146ae2"
          ]
        },
        "outputId": "58eebcfd-4728-4f5d-f5e8-274c9f795958"
      },
      "source": [
        "model = BartForCausalLM.from_pretrained(\"jky594176/recipe_BART1\")\n",
        "model = model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fda850cb82bc48549a54c3ecdb767f95",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1682.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6461871414dc42c3a34d6b5e765e1d67",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=384515855.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79tkk5uRGCpr",
        "outputId": "5a6a9780-f601-42bf-aa65-ee2d3888e962"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "dir_bart1 =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "  print(\"step:\")\n",
        "\n",
        "  for j in range(replicated_size):\n",
        "    print(j, end = \", \")\n",
        "    md = generate_recipe(raw_text)\n",
        "    dir_bart1[i].append(get_instr(md))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Generating the recipe:  0\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  1\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  2\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  3\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  4\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  5\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  6\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  7\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  8\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  9\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  10\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  11\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  12\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  13\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  14\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  15\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  16\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  17\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  18\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  19\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  20\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  21\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  22\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  23\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  24\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  25\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  26\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  27\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  28\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  29\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  30\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  31\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  32\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  33\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  34\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  35\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  36\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  37\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  38\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  39\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  40\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  41\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  42\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  43\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  44\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  45\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  46\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  47\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  48\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  49\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  50\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  51\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  52\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  53\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  54\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  55\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  56\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  57\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  58\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  59\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  60\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  61\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  62\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  63\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  64\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  65\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  66\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  67\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  68\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  69\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  70\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  71\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  72\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  73\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  74\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  75\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  76\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  77\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  78\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  79\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  80\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  81\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  82\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  83\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  84\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  85\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  86\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  87\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  88\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  89\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  90\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  91\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  92\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  93\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  94\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  95\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  96\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  97\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  98\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  99\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuUl9OaiGDhO",
        "outputId": "44cd80c3-3b6c-42e8-e0e4-a034c5281bda"
      },
      "source": [
        "print(\"Cos sim:\", COS_SIM(df_eval_dir, dir_bart1))\n",
        "print(\"BLEU:\", bleu_cal(df_eval_dir, dir_bart1))\n",
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, dir_bart1,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, dir_bart1,'rougeLsum'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cos sim: 0.5351948160757843\n",
            "BLEU: 0.760743964253477\n",
            "rouge-1: 0.3091233913976813\n",
            "rouge-L: 0.18639506343143267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aXGwoeWw7jw"
      },
      "source": [
        "## GPT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jtl_wx8xLipu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360,
          "referenced_widgets": [
            "0965d77dd2fb48f79a75ce89ba9478f8",
            "6a935431775b4f4bac600807111645a6",
            "580a44db562b481aaa4cf216c0204ac6",
            "eaa52ae38bb94e0eb42e5035e0e102e9",
            "ca571001c23d4f3b81485c24d21a6ef2",
            "058330f04cc543a18bfbfc147c614885",
            "177a3febd2c54306a6fbd6ba12e6b648",
            "17072019b61f406480c1754043f985b5",
            "2f80d9e6da04410d9718bcd923cef8f6",
            "a79b8f0f8bce44a383c7340b2a9a1855",
            "721c1229845a468ebad562c075176fa0",
            "6365239082944b3894e0cfc7b8274e57",
            "abc749df653042fda20b0615c3a21f99",
            "4df3c7aad34043c68194e76709157a20",
            "52fd6b17ed404052890b575ebf0827dd",
            "4f1b7f6e295f4710a45b0b43bace6540",
            "aac32707b11b4edbaac9a5f4c218c47a",
            "16dd9aad0ff544e3add106ba752fa38d",
            "018c3da47c8b4621be7fdb7624728e13",
            "d705c8a48ac14834ae356425928a24e1",
            "25a92977f5c64e709f4f970ebad42efa",
            "38d4311e7efd4f8bb9bfe055d748728d",
            "2a078fcfc6ec43409ff7410326b33ccb",
            "15068145c056406bac9fffdfed70994a",
            "21713835b1164ae09d3c69c0e0e4eb87",
            "b0b3d4dc0d42429ca869bd9718809c0d",
            "8749f501f1c04f298fedd0dae03522e3",
            "22fdfdc4046f4d8c8c2a308e1312d01a",
            "14a7b7e193d346329dfdbbfc1be44faa",
            "ccad7f4a213d4d9ebbbe806bc9b343e9",
            "c6bc4f9f8bc6487690297c24ea7017bf",
            "d3a52d09a34f4248a0d1108966bf9c55",
            "89445521f23b45b99b42c7fd2bf76ecb",
            "2aea2b0b79934e8bb2acdb6c6916d69e",
            "f2f585c63d064cc098d31eb4c9524db5",
            "7c4911ad5f2344ec91c3e186ebbc92f3",
            "a3da24dd051549fc9bacbf10611390fb",
            "8c111a8d90354352ab48eb73c950a633",
            "bd5ef55cc0dc4612879eae44284cc646",
            "d0d371225d514743b1266b712955dbf5",
            "07be7d3d6a224b82bcfe0a1370ee22be",
            "8ea4dd05ac7b4e35b524c64238d05880",
            "133c9773aa6b47e0acd40ceb560c1986",
            "2d67a7431ed4467a8977cc3186943ed8",
            "0d749b91656e4049a353356efb295356",
            "a8e64a141de9442e94a918275f05f0dd",
            "d8432daaedb844dc9b3f7cbc66f83178",
            "db62dcda26e24dca900fedcaf234a3d2",
            "a4a01714900141c4a69f67dd61182eb4",
            "19406569b2be421a9b8cdcd3756dc9d7",
            "a23ef236b275404cbee1d710728aaede",
            "3131e0897a0b427b91ba02a333078a72",
            "0033d6adad40427c8a5dcea360c5bb4a",
            "a9f5ecac0f8b4003b8f67c28af6c36ad",
            "10f2474dd4fd454aac9194a7022e20a2",
            "359bccad9205477cb512d03c4017eae0"
          ]
        },
        "outputId": "73e4458f-f664-4dd5-b0ef-871671fb8433"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, GPT2Tokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"jky594176/recipe_GPT2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"jky594176/recipe_GPT2\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0965d77dd2fb48f79a75ce89ba9478f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=850.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f80d9e6da04410d9718bcd923cef8f6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=510445819.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aac32707b11b4edbaac9a5f4c218c47a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898669.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21713835b1164ae09d3c69c0e0e4eb87",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89445521f23b45b99b42c7fd2bf76ecb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=298.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07be7d3d6a224b82bcfe0a1370ee22be",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=595.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4a01714900141c4a69f67dd61182eb4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=780.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onCSGcbxmHD4"
      },
      "source": [
        "model = model.to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-R2TFY2MKCD",
        "outputId": "94093dc8-36e4-4042-ad04-0cfb3a2e8b55"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "dir_gpt2 =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "  print(\"step:\")\n",
        "\n",
        "  for j in range(replicated_size):\n",
        "    print(j, end = \", \")\n",
        "    md = generate_recipe(raw_text)\n",
        "    dir_gpt2[i].append(get_instr(md))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Generating the recipe:  0\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  1\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  2\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  3\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  4\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  5\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  6\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  7\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  8\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  9\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  10\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  11\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  12\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  13\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  14\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  15\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  16\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, 3, 4, Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, 9, \n",
            " Generating the recipe:  17\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  18\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  19\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  20\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  21\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  22\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  23\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, Failed to generate, recipe's too long\n",
            "6, 7, 8, 9, \n",
            " Generating the recipe:  24\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  25\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  26\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  27\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  28\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  29\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  30\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  31\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  32\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  33\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  34\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  35\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  36\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  37\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  38\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  39\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  40\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  41\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  42\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  43\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  44\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  45\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  46\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  47\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  48\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  49\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  50\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  51\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  52\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  53\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  54\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  55\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  56\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  57\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  58\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  59\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, 3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  60\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  61\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  62\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  63\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  64\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  65\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  66\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  67\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  68\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  69\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  70\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  71\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  72\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  73\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  74\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  75\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  76\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  77\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  78\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  79\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  80\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  81\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  82\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  83\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  84\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  85\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  86\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  87\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  88\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  89\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  90\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  91\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  92\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  93\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  94\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  95\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  96\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  97\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  98\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  99\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lef6Rnw9MUEQ",
        "outputId": "03c6a4d6-fd5f-446d-b4f5-4fffd13371a9"
      },
      "source": [
        "print(\"cos similarity:\", COS_SIM(df_eval_dir,dir_gpt2))\n",
        "print(\"BLEU: \",bleu_cal(df_eval_dir, dir_gpt2))\n",
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, dir_gpt2,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, dir_gpt2,'rougeLsum'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cos similarity: 0.5389561959066712\n",
            "BLEU:  0.770623806605888\n",
            "rouge-1: 0.3006555958482678\n",
            "rouge-L: 0.17599998217410337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTe3xM2srJtT"
      },
      "source": [
        "## BART1+NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9nTKzkTrJBc"
      },
      "source": [
        "model = BartForCausalLM.from_pretrained(\"jky594176/recipe_BART1\")\n",
        "tokenizer = init_tokenizer(\"facebook/bart-base\")\n",
        "model = model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "667BR_hNrJFX",
        "outputId": "be2b26e4-4c8d-4b89-8133-095b0ff9820f"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "dir_bart1_nn =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "  print(\"step:\")\n",
        "\n",
        "  for j in range(replicated_size):\n",
        "    print(j, end = \", \")\n",
        "    md = generate_recipe(raw_text)\n",
        "    dir_bart1_nn[i].append(get_instr(md))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Generating the recipe:  0\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  1\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  2\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  3\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  4\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  5\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  6\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  7\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  8\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  9\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  10\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  11\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  12\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  13\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  14\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  15\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  16\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  17\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  18\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  19\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  20\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  21\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  22\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  23\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  24\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  25\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  26\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  27\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  28\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  29\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  30\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  31\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "7, 8, 9, \n",
            " Generating the recipe:  32\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  33\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  34\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  35\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  36\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  37\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  38\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  39\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  40\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  41\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  42\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  43\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, Failed to generate, recipe's too long\n",
            "6, 7, 8, 9, \n",
            " Generating the recipe:  44\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  45\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  46\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  47\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  48\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  49\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  50\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  51\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  52\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  53\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  54\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  55\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  56\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  57\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  58\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  59\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  60\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  61\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  62\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  63\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  64\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  65\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  66\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  67\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  68\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  69\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  70\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  71\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  72\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  73\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  74\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  75\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  76\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  77\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  78\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  79\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  80\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "7, 8, 9, \n",
            " Generating the recipe:  81\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  82\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  83\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  84\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  85\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  86\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  87\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  88\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  89\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  90\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  91\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  92\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  93\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  94\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  95\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  96\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  97\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  98\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  99\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54Q0P_KmrJHt",
        "outputId": "b2ec757f-99b6-40a8-f5d5-0026a894072e"
      },
      "source": [
        "print(\"Cos sim:\", COS_SIM(df_eval_dir, dir_bart1_nn))\n",
        "print(\"BLEU:\", bleu_cal(df_eval_dir, dir_bart1_nn))\n",
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, dir_bart1_nn,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, dir_bart1_nn,'rougeL'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cos sim: 0.5342036210579831\n",
            "BLEU: 0.7706537892876422\n",
            "rouge-1: 0.31010366633561753\n",
            "rouge-L: 0.1850874663184411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzhM5AfQ7oNc"
      },
      "source": [
        "## BART2+GRU\n",
        "\n",
        "The generation helper functions for BART2 is not the same with BART1. So re-define them here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a68MnPnPrJLz"
      },
      "source": [
        "def set_seed(args):\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(model, length, raw_text, tokenizer, num_samples=1, temperature=1, top_k=0, top_p=0.0, device='cpu'):\n",
        "    end_token = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
        "    ing_token_id = tokenizer.convert_tokens_to_ids([\"<INPUT_END>\"])[0]\n",
        "    input_token = tokenizer.convert_tokens_to_ids([\"<NEXT_INPUT>\"])[0]\n",
        "\n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "    # print(prepared_input)\n",
        "\n",
        "    context_tokens = tokenizer.encode(prepared_input)[:-1]\n",
        "    context_len = len(context_tokens)\n",
        "    \n",
        "    encoder_tokens = torch.tensor(context_tokens[3:], dtype=torch.long, device=device)\n",
        "    encoder_tokens = encoder_tokens[encoder_tokens != input_token]\n",
        "\n",
        "    context = torch.tensor(context_tokens, dtype=torch.long, device=device)\n",
        "    # the encoder condition\n",
        "    raw_text_tokens = encoder_tokens\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    raw_text_tokens = raw_text_tokens.unsqueeze(0).repeat(num_samples, 1)\n",
        "    \n",
        "    # genereated token\n",
        "    start_token =  tokenizer.convert_tokens_to_ids([\"<RECIPE_START>\"])[0]\n",
        "    # generated = torch.tensor(start_token, dtype=torch.long, device=device).reshape(1).unsqueeze(0)\n",
        "    generated = context\n",
        "    # print(generated.shape)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            # inputs = {'input_ids': generated}\n",
        "            outputs = model(input_ids = raw_text_tokens, decoder_input_ids = generated)\n",
        "            # outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
        "            # print(outputs)\n",
        "            next_token_logits = outputs[\"logits\"][0, -1, :] / temperature\n",
        "            \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
        "            #print(next_token)\n",
        "            if next_token.item() == end_token:\n",
        "                break\n",
        "    return generated, context_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLs5uhD287LS"
      },
      "source": [
        "def get_raw_input(NER):\n",
        "    test_str = NER\n",
        "    test_str = test_str.replace(\"[\",\"\")\n",
        "    test_str = test_str.replace(\"]\",\"\")\n",
        "    test_str = test_str.replace(\"\\\"\",\"\")\n",
        "\n",
        "    return test_str\n",
        "\n",
        "def get_instr(markdown):\n",
        "  markdown = markdown.split(\"\\n\")\n",
        "  if ' ## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index(' ## Instructions ##')\n",
        "    \n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "\n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "    \n",
        "    return output\n",
        "\n",
        "  elif '## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index('## Instructions ##')\n",
        "\n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "        \n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "\n",
        "    return output\n",
        "    \n",
        "  else:\n",
        "    return [\"failed to generate\"]\n",
        "\n",
        "\n",
        "def generate_recipe(raw_text):\n",
        "  \n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "    out, context_len = sample_sequence(\n",
        "            model=model,\n",
        "            raw_text=raw_text,\n",
        "            tokenizer=tokenizer,\n",
        "            length=512,\n",
        "            device = \"cuda\"\n",
        "        )\n",
        "    out = out[0, context_len:].tolist()\n",
        "    text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n",
        "    if \"<RECIPE_END>\" not in text:\n",
        "      print(\"Failed to generate, recipe's too long\")\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      return generate_recipe(raw_text)\n",
        "    else:\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      markdown = re.sub(\"<RECIPE_(START|END)>\", \"\", full_text)\n",
        "      recipe_n_title = markdown.split(\"<TITLE_START>\")\n",
        "      if len(recipe_n_title)<=1:\n",
        "        return generate_recipe(raw_text)\n",
        "      title = \"# \" + recipe_n_title[1].replace(\"<TITLE_END>\", \"\") + \" #\\n\"\n",
        "      markdown = recipe_n_title[0].replace(\"<INPUT_START>\", \"## Input ingredients ##\\n`\").replace(\"<INPUT_END>\", \"`\\n\")\n",
        "      markdown = markdown.replace(\"<NEXT_INPUT>\", \"`\\n`\").replace(\"<INGR_START>\", \"## Ingredients ##\\n* \").replace(\"<NEXT_INGR>\", \"\\n* \").replace(\"<INGR_END>\", \"\\n\")\n",
        "      #markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\\\"\").replace(\"<NEXT_INSTR>\", \"\\n\\\"\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\").replace(\"<NEXT_INSTR>\", \"\\n\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = re.sub(\"$ +#\", \"#\", markdown)\n",
        "      markdown = re.sub(\"( +`|` +)\", \"`\", markdown)\n",
        "  \n",
        "      return markdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnkhqIbtMZOe"
      },
      "source": [
        "from transformers import BartForConditionalGeneration\n",
        "# model = BartForCausalLM.from_pretrained(\"jky594176/BART1_GRU\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"jky594176/BART2_GRU\")\n",
        "tokenizer = init_tokenizer(\"facebook/bart-base\")\n",
        "model = model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqAYSgNr8-AO",
        "outputId": "1c03cc14-ce26-4dbe-924c-e6e5125de605"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "dir_bart2_gru =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "  print(\"step:\")\n",
        "\n",
        "  for j in range(replicated_size):\n",
        "    print(j, end = \", \")\n",
        "    md = generate_recipe(raw_text)\n",
        "    dir_bart2_gru[i].append(get_instr(md))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Generating the recipe:  0\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  1\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  2\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  3\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  4\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  5\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  6\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  7\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  8\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  9\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  10\n",
            "step:\n",
            "0, 1, 2, 3, 4, Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, 9, \n",
            " Generating the recipe:  11\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  12\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  13\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  14\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  15\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  16\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  17\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  18\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  19\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  20\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  21\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  22\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  23\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  24\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  25\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  26\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  27\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  28\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  29\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  30\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  31\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  32\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  33\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  34\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  35\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  36\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  37\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  38\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  39\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  40\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  41\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  42\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  43\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  44\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  45\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  46\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  47\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  48\n",
            "step:\n",
            "0, 1, 2, 3, 4, Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, 9, \n",
            " Generating the recipe:  49\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  50\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  51\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  52\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  53\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  54\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  55\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  56\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  57\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  58\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  59\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  60\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  61\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  62\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  63\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  64\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  65\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  66\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  67\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  68\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  69\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  70\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  71\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  72\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  73\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  74\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  75\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  76\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  77\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  78\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  79\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  80\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  81\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  82\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  83\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  84\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  85\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  86\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  87\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  88\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  89\n",
            "step:\n",
            "0, 1, 2, 3, 4, Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, 9, \n",
            " Generating the recipe:  90\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  91\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  92\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  93\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  94\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  95\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  96\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  97\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  98\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  99\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3EyQ9Ws9Fty",
        "outputId": "350ca060-89fc-4e0f-b735-e14ae44b8b3b"
      },
      "source": [
        "print(\"Cos sim:\", COS_SIM(df_eval_dir, dir_bart2_gru))\n",
        "print(\"BLEU:\", bleu_cal(df_eval_dir, dir_bart2_gru))\n",
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, dir_bart2_gru,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, dir_bart2_gru,'rougeL'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cos sim: 0.5464908674250816\n",
            "BLEU: 0.7863652763423015\n",
            "rouge-1: 0.3461590359974851\n",
            "rouge-L: 0.21184638727300262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGWXvMMzFX2I"
      },
      "source": [
        "## BART2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63hq4lqiRWBt"
      },
      "source": [
        "def set_seed(args):\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(model, length, raw_text, tokenizer, num_samples=1, temperature=1, top_k=0, top_p=0.0, device='cpu'):\n",
        "    end_token = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "\n",
        "    context_tokens = tokenizer.encode(prepared_input)[0:-1]\n",
        "    raw_text_tokens = tokenizer.encode(raw_text)[0:-1]\n",
        "\n",
        "    context = torch.tensor(context_tokens, dtype=torch.long, device=device)\n",
        "    raw_text_tokens = torch.tensor(raw_text_tokens, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    raw_text_tokens = raw_text_tokens.unsqueeze(0).repeat(num_samples, 1)\n",
        "    # print(raw_text_tokens)\n",
        "    # print(context)\n",
        "    \n",
        "    # genereated token\n",
        "    start_token =  tokenizer.convert_tokens_to_ids([\"<RECIPE_START>\"])[0]\n",
        "    # generated = torch.tensor(start_token, dtype=torch.long, device=device).reshape(1).unsqueeze(0)\n",
        "    generated = context\n",
        "    # print(generated.shape)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            # inputs = {'input_ids': generated}\n",
        "            outputs = model(input_ids = raw_text_tokens, decoder_input_ids = generated)\n",
        "            # outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
        "            # print(outputs)\n",
        "            next_token_logits = outputs[\"logits\"][0, -1, :] / temperature\n",
        "            \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
        "            #print(next_token)\n",
        "            if next_token.item() == end_token:\n",
        "                break\n",
        "    return generated"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcABuelZRWrj"
      },
      "source": [
        "def get_raw_input(NER):\n",
        "    test_str = NER\n",
        "    test_str = test_str.replace(\"[\",\"\")\n",
        "    test_str = test_str.replace(\"]\",\"\")\n",
        "    test_str = test_str.replace(\"\\\"\",\"\")\n",
        "\n",
        "    return test_str\n",
        "\n",
        "def get_instr(markdown):\n",
        "  markdown = markdown.split(\"\\n\")\n",
        "  if ' ## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index(' ## Instructions ##')\n",
        "    \n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "\n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "    \n",
        "    return output\n",
        "\n",
        "  elif '## Instructions ##' in markdown:\n",
        "    instr_index = markdown.index('## Instructions ##')\n",
        "\n",
        "    output = [i for i in markdown[instr_index+1:]]\n",
        "\n",
        "    if \" \" in output:\n",
        "        output.remove(\" \")\n",
        "        \n",
        "    if \"\" in output:\n",
        "        output.remove(\"\")    \n",
        "\n",
        "    return output\n",
        "    \n",
        "  else:\n",
        "    return [\"failed to generate\"]\n",
        "\n",
        "\n",
        "def generate_recipe(raw_text):\n",
        "    prepared_input = ' <RECIPE_START> <INPUT_START> ' + raw_text.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n",
        "    context_tokens = tokenizer.encode(prepared_input)[0:-1]\n",
        "    out = sample_sequence(\n",
        "                model=model,\n",
        "                raw_text=raw_text,\n",
        "                tokenizer=tokenizer,\n",
        "                length=512,\n",
        "                device = 'cuda'\n",
        "            )\n",
        "    out = out[0, len(context_tokens):].tolist()\n",
        "    text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n",
        "    if \"<RECIPE_END>\" not in text:\n",
        "      # print(text)\n",
        "      print(\"Failed to generate, recipe's too long\")\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      return generate_recipe(raw_text)\n",
        "    else:\n",
        "      full_text = prepared_input + text\n",
        "      # print(full_text)\n",
        "      markdown = re.sub(\"<RECIPE_(START|END)>\", \"\", full_text)\n",
        "      recipe_n_title = markdown.split(\"<TITLE_START>\")\n",
        "      if len(recipe_n_title)<=1:\n",
        "        return generate_recipe(raw_text)\n",
        "      title = \"# \" + recipe_n_title[1].replace(\"<TITLE_END>\", \"\") + \" #\\n\"\n",
        "      markdown = recipe_n_title[0].replace(\"<INPUT_START>\", \"## Input ingredients ##\\n`\").replace(\"<INPUT_END>\", \"`\\n\")\n",
        "      markdown = markdown.replace(\"<NEXT_INPUT>\", \"`\\n`\").replace(\"<INGR_START>\", \"## Ingredients ##\\n* \").replace(\"<NEXT_INGR>\", \"\\n* \").replace(\"<INGR_END>\", \"\\n\")\n",
        "      #markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\\\"\").replace(\"<NEXT_INSTR>\", \"\\n\\\"\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = markdown.replace(\"<INSTR_START>\", \"## Instructions ##\\n\").replace(\"<NEXT_INSTR>\", \"\\n\").replace(\"<INSTR_END>\", \"\\n\")\n",
        "      markdown = re.sub(\"$ +#\", \"#\", markdown)\n",
        "      markdown = re.sub(\"( +`|` +)\", \"`\", markdown)\n",
        "  \n",
        "      return markdown"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k6Eiuqk-QEe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "2cc17e384e5a43528457efd3ed0e3d86",
            "df0fa90c399049cfba4f67034bce0022",
            "e753fc502cf04771b4ef55244ed40d99",
            "946990d3080e4dfcb6cc7ede90c156a9",
            "adac09af1de549039689371690913133",
            "a057ca22d35b43f0a283b9837b977e77",
            "b0ac98d02e124b68a6ba3a8e7fed6d14",
            "cbcd4bce30c3432c85889c841d4f6c81",
            "717ed4c4d74f4183834191eb0c95d33d",
            "62fdba02c39e478ba8428d41de1b4f6a",
            "fa9c16bd0f4c4b92952b26f5a93ea5e7",
            "9e0c3e0b09f54ab586174aa1769104e1",
            "ca48c28218684789876b65393c81a0e2",
            "2d7add17842742d4b06366dcdcf1ab14",
            "b6c390c07fdd425ba63bfe287bb11b65",
            "27519db3b8df46beae79b375ef12a62e",
            "bf439e1e02bc4d31a68a745b6eb1b7e6",
            "3d01ad95e1c84cca93d9de390d6e8a0e",
            "04cc86fdc0e34aa8a03cb8566cb11141",
            "fa28bd853e5c4b1aa83c34aa8f7fda37",
            "3daec3f63b0e4181851694accfdd9ba5",
            "32368ce0af3a45ad92111090e2c9fa62",
            "d3913dc2686143d4809bf34d8a158e8b",
            "9a82545b9b2e415f9cd43cda705a867e"
          ]
        },
        "outputId": "07146d41-563a-4065-8a33-e4c07281cb4a"
      },
      "source": [
        "from transformers import BartForConditionalGeneration\n",
        "model = BartForConditionalGeneration.from_pretrained(\"jky594176/recipe_bart2_v3\")\n",
        "model = model.to('cuda')\n",
        "tokenizer = init_tokenizer(\"facebook/bart-base\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2cc17e384e5a43528457efd3ed0e3d86",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "717ed4c4d74f4183834191eb0c95d33d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf439e1e02bc4d31a68a745b6eb1b7e6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC0s8yM_FW5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396d049f-15e8-46f0-d257-9d3df6397487"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "# generate recipes derived from gold standard ones\n",
        "test_size = 100\n",
        "replicated_size = 10\n",
        "dir_bart2 =  [[] for i in range(test_size)]\n",
        "\n",
        "for i in range(test_size):\n",
        "  raw_text = get_raw_input(df_eval_ner[i])\n",
        "  print(\"\\n Generating the recipe: \",i)\n",
        "  print(\"step:\")\n",
        "\n",
        "  for j in range(replicated_size):\n",
        "    print(j, end = \", \")\n",
        "    md = generate_recipe(raw_text)\n",
        "    dir_bart2[i].append(get_instr(md))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Generating the recipe:  0\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  1\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  2\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, 9, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  3\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "4, 5, Failed to generate, recipe's too long\n",
            "6, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "7, 8, 9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  4\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  5\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, Failed to generate, recipe's too long\n",
            "6, 7, 8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  6\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  7\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "3, 4, 5, Failed to generate, recipe's too long\n",
            "6, 7, 8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  8\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, 3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "7, 8, 9, \n",
            " Generating the recipe:  9\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  10\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  11\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "7, 8, 9, \n",
            " Generating the recipe:  12\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  13\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "3, 4, Failed to generate, recipe's too long\n",
            "5, 6, 7, Failed to generate, recipe's too long\n",
            "8, Failed to generate, recipe's too long\n",
            "9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  14\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  15\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  16\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  17\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "2, 3, 4, Failed to generate, recipe's too long\n",
            "5, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "6, 7, 8, 9, \n",
            " Generating the recipe:  18\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "1, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, Failed to generate, recipe's too long\n",
            "7, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "8, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "9, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  19\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  20\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, Failed to generate, recipe's too long\n",
            "4, 5, Failed to generate, recipe's too long\n",
            "6, 7, 8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  21\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  22\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  23\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, Failed to generate, recipe's too long\n",
            "3, 4, Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  24\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  25\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  26\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  27\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  28\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "7, 8, 9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  29\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  30\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, Failed to generate, recipe's too long\n",
            "2, Failed to generate, recipe's too long\n",
            "3, Failed to generate, recipe's too long\n",
            "4, 5, 6, Failed to generate, recipe's too long\n",
            "7, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "8, 9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  31\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  32\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "6, Failed to generate, recipe's too long\n",
            "7, 8, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  33\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "3, Failed to generate, recipe's too long\n",
            "4, Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  34\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, Failed to generate, recipe's too long\n",
            "4, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "5, Failed to generate, recipe's too long\n",
            "6, 7, 8, 9, \n",
            " Generating the recipe:  35\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, Failed to generate, recipe's too long\n",
            "6, 7, 8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  36\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "7, 8, 9, \n",
            " Generating the recipe:  37\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "7, 8, 9, \n",
            " Generating the recipe:  38\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  39\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  40\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, 3, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "4, 5, 6, Failed to generate, recipe's too long\n",
            "7, 8, 9, \n",
            " Generating the recipe:  41\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  42\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "2, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "3, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "4, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "5, Failed to generate, recipe's too long\n",
            "6, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "7, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "8, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "9, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  43\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, Failed to generate, recipe's too long\n",
            "6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  44\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  45\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  46\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, Failed to generate, recipe's too long\n",
            "7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  47\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  48\n",
            "step:\n",
            "0, 1, 2, 3, 4, Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, 9, \n",
            " Generating the recipe:  49\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  50\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  51\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "3, Failed to generate, recipe's too long\n",
            "4, 5, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "6, Failed to generate, recipe's too long\n",
            "7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  52\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, Failed to generate, recipe's too long\n",
            "6, 7, 8, 9, \n",
            " Generating the recipe:  53\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  54\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "1, 2, 3, 4, Failed to generate, recipe's too long\n",
            "5, 6, Failed to generate, recipe's too long\n",
            "7, 8, Failed to generate, recipe's too long\n",
            "9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  55\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "7, 8, 9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  56\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  57\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, Failed to generate, recipe's too long\n",
            "6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  58\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  59\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  60\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "2, 3, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "4, 5, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "6, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "7, Failed to generate, recipe's too long\n",
            "8, 9, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  61\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  62\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, Failed to generate, recipe's too long\n",
            "3, 4, 5, Failed to generate, recipe's too long\n",
            "6, 7, 8, 9, \n",
            " Generating the recipe:  63\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  64\n",
            "step:\n",
            "0, 1, 2, 3, 4, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, 9, \n",
            " Generating the recipe:  65\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  66\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "7, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  67\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "3, 4, 5, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "6, 7, 8, 9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  68\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "1, Failed to generate, recipe's too long\n",
            "2, 3, 4, Failed to generate, recipe's too long\n",
            "5, Failed to generate, recipe's too long\n",
            "6, 7, 8, Failed to generate, recipe's too long\n",
            "9, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  69\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, Failed to generate, recipe's too long\n",
            "3, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "4, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "5, Failed to generate, recipe's too long\n",
            "6, Failed to generate, recipe's too long\n",
            "7, Failed to generate, recipe's too long\n",
            "8, 9, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  70\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  71\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, Failed to generate, recipe's too long\n",
            "6, 7, 8, 9, \n",
            " Generating the recipe:  72\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  73\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "2, 3, 4, Failed to generate, recipe's too long\n",
            "5, Failed to generate, recipe's too long\n",
            "6, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "7, Failed to generate, recipe's too long\n",
            "8, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  74\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  75\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  76\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, 3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "7, 8, 9, \n",
            " Generating the recipe:  77\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, Failed to generate, recipe's too long\n",
            "7, 8, 9, \n",
            " Generating the recipe:  78\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "2, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "3, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "4, 5, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "6, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "7, Failed to generate, recipe's too long\n",
            "8, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "9, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  79\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  80\n",
            "step:\n",
            "0, 1, 2, 3, 4, Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, 9, \n",
            " Generating the recipe:  81\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "3, 4, Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, 9, \n",
            " Generating the recipe:  82\n",
            "step:\n",
            "0, 1, 2, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "3, 4, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "5, 6, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "7, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  83\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  84\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "4, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "5, 6, Failed to generate, recipe's too long\n",
            "7, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "8, 9, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  85\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  86\n",
            "step:\n",
            "0, 1, 2, 3, Failed to generate, recipe's too long\n",
            "4, 5, Failed to generate, recipe's too long\n",
            "6, 7, 8, 9, \n",
            " Generating the recipe:  87\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  88\n",
            "step:\n",
            "0, 1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  89\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "1, 2, 3, 4, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  90\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "1, 2, 3, 4, Failed to generate, recipe's too long\n",
            "5, 6, 7, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "8, 9, \n",
            " Generating the recipe:  91\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  92\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, 2, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "8, 9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  93\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
            " Generating the recipe:  94\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "1, 2, Failed to generate, recipe's too long\n",
            "3, Failed to generate, recipe's too long\n",
            "4, 5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  95\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, Failed to generate, recipe's too long\n",
            "\n",
            " Generating the recipe:  96\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, Failed to generate, recipe's too long\n",
            "6, Failed to generate, recipe's too long\n",
            "7, 8, 9, \n",
            " Generating the recipe:  97\n",
            "step:\n",
            "0, 1, 2, 3, 4, 5, 6, 7, 8, Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  98\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "1, 2, Failed to generate, recipe's too long\n",
            "3, 4, Failed to generate, recipe's too long\n",
            "5, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "6, 7, 8, Failed to generate, recipe's too long\n",
            "Failed to generate, recipe's too long\n",
            "9, \n",
            " Generating the recipe:  99\n",
            "step:\n",
            "0, Failed to generate, recipe's too long\n",
            "1, Failed to generate, recipe's too long\n",
            "2, 3, 4, 5, 6, 7, Failed to generate, recipe's too long\n",
            "8, 9, Failed to generate, recipe's too long\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMM4SMJdFW7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a48a9f-ece1-482b-f832-b24316cf8c5b"
      },
      "source": [
        "print(\"Cos sim:\", COS_SIM(df_eval_dir, dir_bart2))\n",
        "print(\"BLEU:\", bleu_cal(df_eval_dir, dir_bart2))\n",
        "print(\"rouge-1:\", rouge_cal(df_eval_dir, dir_bart2,'rouge1'))\n",
        "print(\"rouge-L:\", rouge_cal(df_eval_dir, dir_bart2,'rougeL'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cos sim: 0.5247337591017832\n",
            "BLEU: 0.7075080899185022\n",
            "rouge-1: 0.27962831125499793\n",
            "rouge-L: 0.19444832497840317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyKOK5VJJPie"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}